{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h1 style=\"color:#FF7C00  \">  Seq2Seq in Tensorflow </h1>\n",
    "<h4>  </h4> \n",
    "<h3 style=\"color:lightblue\"> Keywords: </h3>  keras pad_sequences() + \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'       \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FF7C00  \">  <u> Example 1 </u> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = [\n",
    "    \"hello\", \"goodbye\", \"thank you\", \"please\", \"yes\", \"no\", \"I love you\",\n",
    "    \"how are you\", \"good morning\", \"good night\", \"water\", \"food\", \"sun\",\n",
    "    \"moon\", \"star\", \"book\", \"computer\", \"phone\", \"flower\", \"tree\", \n",
    "    \"house\", \"car\", \"bus\", \"train\", \"sky\", \"cloud\", \"rain\", \"snow\",\n",
    "    \"bird\", \"cat\", \"dog\", \"fish\", \"mountain\", \"valley\", \"ocean\", \"sea\",\n",
    "    \"river\", \"forest\", \"desert\", \"city\", \"village\", \"country\", \"king\",\n",
    "    \"queen\", \"prince\", \"princess\", \"happy\", \"sad\", \"angry\", \"excited\",\n",
    "    \"bored\", \"tired\", \"hungry\", \"thirsty\", \"hot\", \"cold\", \"big\", \"small\",\n",
    "    \"fast\", \"slow\", \"up\", \"down\", \"left\", \"right\", \"day\", \"night\", \"light\",\n",
    "    \"dark\", \"young\", \"old\", \"man\", \"woman\", \"boy\", \"girl\", \"friend\", \"enemy\",\n",
    "    \"song\", \"dance\", \"jump\", \"run\", \"walk\", \"stop\", \"go\", \"come\", \"push\",\n",
    "    \"pull\", \"open\", \"close\", \"hard\", \"soft\", \"short\", \"tall\", \"wide\", \"narrow\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_sentences = [\n",
    "    \"ciao\", \"addio\", \"grazie\", \"per favore\", \"sì\", \"no\", \"ti amo\",\n",
    "    \"come stai\", \"buongiorno\", \"buonanotte\", \"acqua\", \"cibo\", \"sole\",\n",
    "    \"luna\", \"stella\", \"libro\", \"computer\", \"telefono\", \"fiore\", \"albero\",\n",
    "    \"casa\", \"auto\", \"autobus\", \"treno\", \"cielo\", \"nuvola\", \"pioggia\", \"neve\",\n",
    "    \"uccello\", \"gatto\", \"cane\", \"pesce\", \"montagna\", \"valle\", \"oceano\", \"mare\",\n",
    "    \"fiume\", \"foresta\", \"deserto\", \"città\", \"villaggio\", \"paese\", \"re\",\n",
    "    \"regina\", \"principe\", \"principessa\", \"felice\", \"triste\", \"arrabbiato\", \"eccitato\",\n",
    "    \"annoito\", \"stanco\", \"affamato\", \"assetato\", \"caldo\", \"freddo\", \"grande\", \"piccolo\",\n",
    "    \"veloce\", \"lento\", \"su\", \"giù\", \"sinistra\", \"destra\", \"giorno\", \"notte\", \"luce\",\n",
    "    \"scuro\", \"giovane\", \"vecchio\", \"uomo\", \"donna\", \"ragazzo\", \"ragazza\", \"amico\", \"nemico\",\n",
    "    \"canzone\", \"danza\", \"salta\", \"corri\", \"cammina\", \"ferma\", \"vai\", \"vieni\", \"spingi\",\n",
    "    \"tira\", \"apri\", \"chiudi\", \"duro\", \"morbido\", \"corto\", \"alto\", \"largo\", \"stretto\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#FF7C00  \">  Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_eng = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer_eng.fit_on_texts(english_sentences)\n",
    "english_seq = tokenizer_eng.texts_to_sequences(english_sentences)\n",
    "\n",
    "tokenizer_frn = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer_frn.fit_on_texts(italian_sentences)\n",
    "french_seq = tokenizer_frn.texts_to_sequences(italian_sentences)\n",
    "\n",
    "max_len_eng = max([len(seq) for seq in english_seq])\n",
    "max_len_frn = max([len(seq) for seq in french_seq])\n",
    "\n",
    "english_seq = keras.preprocessing.sequence.pad_sequences(english_seq, maxlen=max_len_eng, padding='post')\n",
    "french_seq = keras.preprocessing.sequence.pad_sequences(french_seq, maxlen=max_len_frn, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#FF7C00  \">  Seq2Seq Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters\n",
    "embedding_dim = 50\n",
    "lstm_units = 128\n",
    "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
    "vocab_size_frn = len(tokenizer_frn.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Encoder\n",
    "encoder_input = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(vocab_size_eng, embedding_dim)(encoder_input)\n",
    "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
    "encoder_output, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "####### Decoder\n",
    "decoder_input = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(vocab_size_frn, embedding_dim)(decoder_input)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_frn, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = keras.Model([encoder_input, decoder_input], decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model = keras.Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(lstm_units,))\n",
    "decoder_state_input_c = Input(shape=(lstm_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedding_inference = Embedding(vocab_size_frn, embedding_dim)(decoder_input)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_inference, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = keras.Model([decoder_input] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Splitting data for training and testing\n",
    "eng_train, eng_val, frn_train, frn_val = train_test_split(english_seq, french_seq, test_size=0.2)\n",
    "\n",
    "model.fit([eng_train, frn_train[:, :-1]], frn_train[:, 1:], \n",
    "            validation_data=([eng_val, frn_val[:, :-1]], frn_val[:, 1:]),\n",
    "            batch_size=2, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence):\n",
    "    # Tokenize and pad the input sentence\n",
    "    input_seq = tokenizer_eng.texts_to_sequences([input_sentence])\n",
    "    input_seq = keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_len_eng, padding='post')\n",
    "\n",
    "    # Get the encoder states\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Start token for the decoder, using the first word in our dictionary as the start point\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = 1  # Let's use the first word index as a starting point\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Get the token with the highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        # Check if the index is in the dictionary\n",
    "        if sampled_token_index in tokenizer_frn.index_word:\n",
    "            sampled_char = tokenizer_frn.index_word[sampled_token_index]\n",
    "            decoded_sentence += ' ' + sampled_char\n",
    "        else:\n",
    "            break  # Exit if the index isn't in the dictionary\n",
    "\n",
    "        # Exit loop if max length is reached\n",
    "        if len(decoded_sentence.split()) > max_len_frn:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target_seq and states for the next loop iteration\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"hello\"\n",
    "predicted_translation = translate(input_sentence)\n",
    "predicted_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h3 style=\"color:#FF7C00  \"> Note: </h3>\n",
    "</div>\n",
    "Clearly, the prediction is wrong! Seq2Seq models typically require large datasets to produce accurate translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FF7C00  \">  <u> Example 2 </u> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = []\n",
    "italian_sentences = []\n",
    "\n",
    "with open(\"./data/eng-ita.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        eng, ita = line.strip().split(\"\\t\")\n",
    "        english_sentences.append(eng)\n",
    "        italian_sentences.append(ita)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Use only 4% of the data to avoid memory allocation problems. \"\"\"\n",
    "\n",
    "num_samples = int(0.04 * len(english_sentences))  \n",
    "# Randomly sample indices\n",
    "sampled_indices = random.sample(range(len(english_sentences)), num_samples)\n",
    "\n",
    "## Use indices to sample from lists\n",
    "english_sentences_sampled = [english_sentences[i] for i in sampled_indices]\n",
    "italian_sentences_sampled = [italian_sentences[i] for i in sampled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization using TensorFlow's Keras API\n",
    "tokenizer_eng = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(english_sentences_sampled)\n",
    "english_sequences = tokenizer_eng.texts_to_sequences(english_sentences_sampled)\n",
    "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
    "max_len_eng = max([len(seq) for seq in english_sequences])\n",
    "\n",
    "tokenizer_frn = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer_frn.fit_on_texts(italian_sentences_sampled)\n",
    "italian_sequences = tokenizer_frn.texts_to_sequences(italian_sentences_sampled)\n",
    "vocab_size_frn = len(tokenizer_frn.word_index) + 1\n",
    "max_len_frn = max([len(seq) for seq in italian_sequences])\n",
    "\n",
    "# Padding sequences\n",
    "english_sequences = tf.keras.preprocessing.sequence.pad_sequences(english_sequences, maxlen=max_len_eng, padding='post')\n",
    "italian_sequences = tf.keras.preprocessing.sequence.pad_sequences(italian_sequences, maxlen=max_len_frn, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([english_sequences, italian_sequences[:,:-1]], keras.utils.to_categorical(italian_sequences[:,1:], num_classes=vocab_size_frn), \n",
    "                                                                                    batch_size=64, \n",
    "                                                                                    epochs=100, \n",
    "                                                                                    validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
