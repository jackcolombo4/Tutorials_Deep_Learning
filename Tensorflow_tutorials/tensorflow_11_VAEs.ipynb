{"cells":[{"cell_type":"markdown","metadata":{},"source":["<div style=\"line-height:1.2;\">\n","\n","<h1 style=\"color:#FF7C00; margin-bottom: 0.3em;\">Variational Autoencoders in TensorFlow </h1>\n","\n","<h4 style=\"margin-top: 0.3em; margin-bottom: 1em;\"> Examples of VAEs with Bayesian approach with Prior.</h4>\n","\n","<div style=\"line-height:1.4; margin-bottom: 0.5em;\">\n","    <h3 style=\"color: lightblue; display: inline; margin-right: 0.5em;\">Keywords:</h3> \n","    tf reduce_mean + tfp distributions + tf.keras.layers + tf.GradientTape()\n","</div>\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H06NKEsUriS3"},"outputs":[],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7015,"status":"ok","timestamp":1693128193743,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"mws8-hQSrh7m"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","from tensorflow.keras.datasets import mnist"]},{"cell_type":"markdown","metadata":{},"source":["<h3 style=\"color:#FF7C00  \"> Recap: </h3>\n","<div style=\"margin-top: -20px;\">\n","Variational Autoencoders (are a type of generative model for unsupervised learning. <br>\n","VAE is a probabilistic approach to autoencoders, allowing dimensionality reduction and latent space exploration. <br>\n","=> Goal: assign the highest likelihood to our data set. <br>\n","</div>"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1693128193744,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"0vy829gq9jji"},"outputs":[],"source":["tfd = tfp.distributions     #tf.contrib.distributions deprecated!"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1693128193745,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"S2BWpeZcrc-t"},"outputs":[],"source":["def make_encoder(data, code_size):\n","    # Flatten the input data\n","    x = tf.layers.flatten(data)\n","\n","    # Apply a fully connected layer with 200 units and ReLU activation\n","    x = tf.layers.dense(x, 200, tf.nn.relu)\n","    # Apply another fully connected layer with 200 units and ReLU activation\n","    x = tf.layers.dense(x, 200, tf.nn.relu)\n","    \n","    # Generate the mean (loc) of the latent code\n","    loc = tf.layers.dense(x, code_size)\n","    # Generate the scale (standard deviation) of the latent code using softplus\n","    scale = tf.layers.dense(x, code_size, tf.nn.softplus)\n","    \n","    # Return a Multivariate normal distribution with the calculated mean and scale\n","    return tfd.MultivariateNormalDiag(loc, scale)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1693128193745,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"mzxP3OWL_IP9"},"outputs":[],"source":["def make_encoder(data, code_size):\n","    # Flatten the input 'data' using a Flatten layer\n","    x = tf.keras.layers.Flatten()(data)\n","    \n","    # Add a Dense layer with 200 units and ReLU activation function\n","    x = tf.keras.layers.Dense(200, activation=tf.nn.relu)(x)\n","    # Add another Dense layer with 200 units and ReLU activation function\n","    x = tf.keras.layers.Dense(200, activation=tf.nn.relu)(x)\n","\n","    # Create a Dense layer with 'code_size' units for loc (mean of the distribution)\n","    loc = tf.keras.layers.Dense(code_size)(x)\n","    # Create a Dense layer with 'code_size' units and a softplus activation function for scale \n","    # (standard deviation of the distribution)\n","    scale = tf.keras.layers.Dense(code_size, activation=tf.nn.softplus)(x)\n","\n","    # Return a Multivariate Normal Diagonal distribution defined by loc and scale\n","    return tfp.distributions.MultivariateNormalDiag(loc, scale)\n"]},{"cell_type":"markdown","metadata":{"id":"CHQNXnGKul8l"},"source":["In the context of a VAE, the Prior distribution represents the assumptions about the latent space before any data is observed. <br>\n","It is used to regularize the learning process by providing a source of information for the latent space.    \n","The Prior is fixed and defines what distribution of codes we would expect.     \n","=> Usually => Normal distribution with zero mean and unit variance.        \n","Using a prior means encouraging the learned latent space to follow the assumed prior distribution, promoting meaningful and smooth representations.\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1693128193745,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"_rJrp1LSsbwG"},"outputs":[],"source":["def make_prior(code_size):\n","    \"\"\" Create the prior distribution for the latent code. \"\"\"\n","    # Set the mean of the prior distribution to zero\n","    loc = tf.zeros(code_size)\n","    # Set the scale (standard deviation) of the prior distribution to one\n","    scale = tf.ones(code_size)\n","    # Create a multivariate normal distribution representing the prior\n","    return tfd.MultivariateNormalDiag(loc, scale)"]},{"cell_type":"markdown","metadata":{"id":"yDg0vVrp1zSj"},"source":["<h2 style=\"color:#FF7C00  \">  Decoder </h2>\n","The decoder maps goal is to reconstruct images, the given code to a distribution of images that are plausible for the code."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1693128193746,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"59SRArcuscDS"},"outputs":[],"source":["def make_decoder(code, data_shape):\n","    \"\"\" Decode to a Bernoulli distribution is constructed to model binary data generation from logits created from dense layers. \"\"\"\n","    # Set the input to the latent code\n","    x = code\n","    ## Create 2 layers to introduce non-linearity and complexity to transform the latent code \n","    # (Fully connected layer with 200 units and ReLU activation)\n","    x = tf.layers.dense(x, 200, tf.nn.relu)\n","    x = tf.layers.dense(x, 200, tf.nn.relu)\n","\n","    # Add another fully connected layer to Generate logits (pre-activation values) for the output data\n","    logit = tf.layers.dense(x, np.prod(data_shape))\n","    # Reshape the logits to match the desired output data shape\n","    logit = tf.reshape(logit, [-1] + data_shape)\n","\n","    \"\"\" The Bernoulli distribution for binary data with the calculated logits (model with binary data). \n","        - 2 means that the distribution independence across rows and columns (width and height),\n","        allowing to evaluate the probability of an image under the distribution, not just individual pixels.\n","    \"\"\"\n","    indip = tfd.Independent(tfd.Bernoulli(logit), 2)\n","    return indip"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1693128193746,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"hpS_29EL_kJp"},"outputs":[],"source":["def make_decoder(code, data_shape):\n","    x = code\n","    x = tf.keras.layers.Dense(200, activation=tf.nn.relu)(x)\n","    x = tf.keras.layers.Dense(200, activation=tf.nn.relu)(x)\n","\n","    logit = tf.keras.layers.Dense(np.prod(data_shape))(x)\n","    logit = tf.reshape(logit, [-1] + data_shape)\n","\n","    bernoulli_distribution = tfp.distributions.Bernoulli(logits=logit)\n","    independent_distribution = tfp.distributions.Independent(bernoulli_distribution, 2)\n","\n","    return independent_distribution"]},{"cell_type":"markdown","metadata":{"id":"7VUE3P_DscG0"},"source":["<div style=\"line-height:0.3\">\n","<h2 style=\"color:#FF7C00  \"> Loss </h2>\n","</div>\n","\n","- Evidence lower bound (ELBO), an approximation to the data likelihood\n","- ELBO combines the likelihood and the KL divergence, and maximizing it helps learn meaningful latent representations "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Calculate Loss.\n","N.B. \n","The eager_execution need to be disabled to make this code running without errors, \n","but it makes difficult to implement the right code for next cells!\n","\"\"\"\n","tf.compat.v1.disable_eager_execution()\n","\n","# Define a placeholder for input data with shape [batch_size, 28, 28]\n","#data = tf.placeholder(tf.float32, [None, 28, 28])\n","data = tf.compat.v1.placeholder(tf.float32, [None, 28, 28])\n","\n","# Create the prior distribution for the latent code with a size of 2\n","prior = make_prior(code_size=2)\n","# Create the posterior distribution by encoding the input data into the latent space\n","posterior = make_encoder(data, code_size=2)\n","\n","# Sample a latent code from the posterior distribution\n","code = posterior.sample()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":427,"status":"ok","timestamp":1693128194138,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"HpZ2Puuisigm"},"outputs":[],"source":["# Calculate the log-likelihood of the data given the generated code\n","likelihood = make_decoder(code, [28, 28]).log_prob(data)\n","\n","# Calculate the KL divergence (deviation) between the posterior and prior distributions\n","divergence = tfd.kl_divergence(posterior, prior)\n","\n","# Calculate the Evidence Lower Bound (ELBO) which is a measure of how well the model fits the data\n","elbo = tf.reduce_mean(likelihood - divergence)"]},{"cell_type":"markdown","metadata":{"id":"IIUw9ae6silg"},"source":["<h2 style=\"color:#FF7C00  \"> Optimizer </h2>"]},{"cell_type":"markdown","metadata":{},"source":["**Recap tf.GradientTape():** <br>\n","Record Operations: Inside a  context, TensorFlow \"records\" the operations that are performed on tensors. <br> \n","Any operation performed within this context is tracked, so that their gradients can be computed later. <br>\n","\n","Compute Gradients: TensorFlow computes gradients through the computational graph formed by the recorded operations, <br> \n","using the backpropagation algorithm. <br>"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":970,"status":"ok","timestamp":1693128195103,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"X64Il_mXBGHV","outputId":"3652a87e-8650-490b-da51-507860eb30f2"},"outputs":[{"data":{"text/plain":["<tf.Variable 'iteration:0' shape=() dtype=int64>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["### Get trainable variables for the prior, posterior, and decoder to compute the gradient descent\n","prior_trainable_vars = prior.trainable_variables\n","posterior_trainable_vars = posterior.trainable_variables\n","decoder_trainable_vars = make_decoder(code, [28, 28]).trainable_variables\n","\n","# Combine all trainable variables\n","all_trainable_vars = prior_trainable_vars + posterior_trainable_vars + decoder_trainable_vars\n","\n","# Create an optimizer\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","\n","## Use a GradientTape to compute gradients\n","with tf.GradientTape() as tape:\n","    loss = -elbo                # Negative ELBO since we want to maximize ELBO\n","\n","# Compute gradients\n","gradients = tape.gradient(loss, all_trainable_vars)\n","\n","# Apply gradients using the optimizer\n","optimizer.apply_gradients(zip(gradients, all_trainable_vars))"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1693128195105,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"MUIsOo1AsiqX"},"outputs":[],"source":["samples = make_decoder(prior.sample(10), [28, 28]).mean()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1693128195107,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"cOHNdFrAsiyJ"},"outputs":[],"source":["def plot_codes(codes):\n","    with tf.compat.v1.Session() as sess:\n","        codes_values = sess.run(codes)\n","\n","    plt.scatter(codes_values[:, 0], codes_values[:, 1], c='b', marker='o')\n","    plt.xlabel('Code Dimension 1')\n","    plt.ylabel('Code Dimension 2')\n","    plt.title('Latent Space Visualization')\n","    plt.show()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1693128195109,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"3Ih-yWJ0HAIm","outputId":"f16f68c5-138b-4ffa-db56-a65fb90a7c0c"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\" Check eager mode \"\"\"\n","tf.executing_eagerly()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1693128195111,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"QJQX2lN9IGTB","outputId":"b2fd8a81-07d5-4ee0-cb39-386477d99940"},"outputs":[{"name":"stdout","output_type":"stream","text":["skipping since tf.enable_eager_execution must be called at program startup to avoid the specific ValueError\n"]}],"source":["%%script echo Skipping, tf.enable_eager_execution must be called at program startup to avoid the specific ValueError\n","tf.compat.v1.enable_eager_execution()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1693128195113,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"Jk5bOQkoIIeU","outputId":"38fe3d22-a184-4fee-80f0-9c84b62b8386"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\" Check again eager mode \"\"\"\n","tf.compat.v1.executing_eagerly()"]},{"cell_type":"markdown","metadata":{"id":"kAZ7STepJeiI"},"source":["The following snippet it is not working due to the error: <br>\n","RuntimeError: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function. <br>\n","The eager mode was disabled before! <br>"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1693128195114,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"onHLOlFM8XZT","outputId":"f12a7ec2-3633-4207-f491-27f7b7cf372d"},"outputs":[{"name":"stdout","output_type":"stream","text":["skipping due to error\n"]}],"source":["%%script echo skipping due to error\n","# Define the number of training steps and the batch size\n","num_steps = 10000\n","batch_size = 32\n","\n","# Load the MNIST dataset\n","(train_images, _), (test_images, _) = mnist.load_data()\n","\n","# Preprocess the data\n","train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\"float32\") / 255\n","test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype(\"float32\") / 255\n","\n","# Binarize the images\n","train_images[train_images >= 0.5] = 1.0\n","train_images[train_images < 0.5] = 0.0\n","test_images[test_images >= 0.5] = 1.0\n","test_images[test_images < 0.5] = 0.0\n","\n","# Use tf.data to batch and shuffle the data\n","train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(batch_size)\n","\n","\"\"\" Training loop \"\"\"\n","for step in range(1, num_steps + 1):\n","    # Sample a batch of data\n","    for batch_x in train_dataset:\n","        # Open a GradientTape\n","        with tf.GradientTape() as tape:\n","            # Encode the data to the latent space and sample a code\n","            posterior = make_encoder(batch_x, code_size=2)\n","            code = posterior.sample()\n","\n","            # Decode the code and calculate the log-likelihood of the data\n","            likelihood = make_decoder(code, [28, 28]).log_prob(batch_x)\n","            \n","            # Calculate the KL divergence between the posterior and prior distributions\n","            divergence = tfd.kl_divergence(posterior, prior)\n","            # Calculate the negative ELBO as the loss\n","            loss = -tf.reduce_mean(likelihood - divergence)\n","\n","        # Compute the gradients with respect to the loss\n","        gradients = tape.gradient(loss, tape.watched_variables())\n","\n","        # Apply the gradients using the optimizer\n","        optimizer.apply_gradients(zip(gradients, tape.watched_variables()))\n","\n","    # Print loss and generate samples every 1000 steps\n","    if step % 1000 == 0:\n","        print(f\"Step {step}, Loss: {loss}\")\n","\n","        # Generate samples\n","        samples = make_decoder(prior.sample(10), [28, 28]).mean()\n","\n","        ##### Plot the samples\n","        for i in range(10):\n","            plt.subplot(2, 5, i + 1)\n","            plt.imshow(samples[i, :, :, 0], cmap=\"gray\")\n","            plt.axis(\"off\")\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WS6xeUU7DSGj"},"source":["<div style=\"line-height:0.5\">\n","<h3 style=\"color:#FF7C00  \"> Note: </h3>\n","</div>\n","\n","The Following code works instead ... => NB huge RAM consuption! <br> \n","Therefore the number of steps and the batch size is minimal (num_steps = 100 + batch_size = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Define the number of training steps and the batch size\n","num_steps = 100 #10000\n","batch_size = 5 #32\n","\n","# Load the MNIST dataset\n","(train_images, _), (test_images, _) = mnist.load_data()\n","\n","## Preprocess the data\n","train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\"float32\") / 255\n","test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype(\"float32\") / 255\n","\n","#### Binarize the images\n","train_images[train_images >= 0.5] = 1.0\n","train_images[train_images < 0.5] = 0.0\n","test_images[test_images >= 0.5] = 1.0\n","test_images[test_images < 0.5] = 0.0\n","\n","# Use tf.data to batch and shuffle the data\n","train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(batch_size).repeat()\n","\n","# Create an iterator for the dataset\n","train_iterator = tf.compat.v1.data.make_one_shot_iterator(train_dataset)\n","\n","# Define the prior\n","prior = tfp.distributions.Normal(loc=0.0, scale=1.0)\n","\n","## Define the decoder\n","def make_decoder(code, output_shape):\n","    return tfp.distributions.Bernoulli(logits=code)\n","\n","# Define the optimizer\n","optimizer = tf.keras.optimizers.Adam()\n","\n","# Sample a batch of data outside the loop, \n","# to avoid to call \"Iterator.get_next()\" inside the training loop, which can lead to resource exhaustion.\n","batch_x = train_iterator.get_next()"]},{"cell_type":"markdown","metadata":{},"source":["<h3 style=\"color:#FF7C00  \"> => Train </h3>"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61667,"status":"ok","timestamp":1693128256755,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"3gSN7Vq9EDLM","outputId":"0935ddc0-1d06-4449-9fa9-37eeb4f27830"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py:445: UserWarning: An unusually high number of `Iterator.get_next()` calls was detected. This often indicates that `Iterator.get_next()` is being called inside a training loop, which will cause gradual slowdown and eventual resource exhaustion. If this is the case, restructure your code to call `next_element = iterator.get_next()` once outside the loop, and use `next_element` as the input to some computation that is invoked inside the loop.\n","  warnings.warn(GET_NEXT_CALL_WARNING_MESSAGE)\n"]}],"source":["\"\"\" Training loop \"\"\"\n","for step in range(1, num_steps + 1):\n","    # Sample a batch of data\n","    #batch_x = train_iterator.get_next() #removed from here\n","\n","    # Open a GradientTape\n","    with tf.GradientTape() as tape:\n","        ## Encode the data to the latent space and sample a code\n","        posterior = make_encoder(batch_x, code_size=2)\n","        code = posterior.sample()\n","\n","        ## Decode the code and calculate the log-likelihood of the data\n","        decoder = make_decoder(code, [28, 28])\n","        likelihood = decoder.log_prob(batch_x)\n","\n","        ####### Sample from the posterior distribution to estimate the KL-divergence\n","        num_samples = 10\n","        samples = posterior.sample(num_samples)\n","        log_probs_posterior = posterior.log_prob(samples)\n","        log_probs_prior = prior.log_prob(samples)\n","        kl_estimate = tf.reduce_mean(log_probs_posterior - log_probs_prior)\n","\n","        # Calculate the negative ELBO as the loss\n","        loss = -tf.reduce_mean(likelihood - kl_estimate)\n","\n","    # Get all trainable variables\n","    trainable_vars = prior.trainable_variables + posterior.trainable_variables + decoder.trainable_variables\n","\n","    # Compute the gradients with respect to the loss\n","    gradients = tape.gradient(loss, trainable_vars)\n","\n","    # Apply the gradients using the optimizer\n","    optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","    # Print loss and generate samples every 1000 steps\n","    if step % 1000 == 0:\n","        print(f\"Step {step}, Loss: {loss}\")\n","\n","        # Generate samples\n","        samples = decoder.sample(10)\n","\n","        # Plot the samples\n","        for i in range(10):\n","            plt.subplot(2, 5, i + 1)\n","            plt.imshow(samples[i, :, :, 0], cmap=\"gray\")\n","            plt.axis(\"off\")\n","        plt.show()\n","\n","    # Sample a new batch of data for the next step\n","    batch_x = train_iterator.get_next()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP+44zRLpr3+CXdwVCs8R71","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
