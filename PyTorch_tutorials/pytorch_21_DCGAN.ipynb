{"cells":[{"cell_type":"markdown","metadata":{},"source":["<div style=\"line-height:0.5\">\n","<div style=\"line-height:1\">\n","<h1 style=\"color:#BF66F2 \">  Deep Convolutional Generative Adversarial Network <br> in PyTorch 1 </h1>\n","<div style=\"line-height:0.2\">\n","<h4> Implementation of a DCGANs model, trained on the MNIST dataset. </h4>\n","<span style=\"display: inline-block;\">\n","    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3>\n","    import another notebook + nn.ConvTranspose2d() + nn.LeakyReLU() + transforms.Normalize() + torchvision.utils.make_grid()\n","</span>\n","</div> \n","</div> "]},{"cell_type":"markdown","metadata":{"id":"seG5uChKNfLA"},"source":["<h2 style=\"color:#BF66F2 \"> Recap: DCGANs </h2>\n","<div style=\"margin-top: -29px;\">\n","GAN architecture specifically designed for generating high-quality images. <br>\n","DCGANs use convolutional neural networks (CNNs) as both the generator and discriminator models => well-suited for image synthesis tasks.\n","</div>\n","\n","<h3> Differences between DCGANs and standard GANs: </h3>\n","<div style=\"margin-top: -20px;\">\n","\n","\n","+ Architecture: <br> \n","DCGANs use convolutional layers in both the generator and discriminator networks, allowing them to capture spatial features in the images. <br> In contrast, standard GANs typically use fully connected layers, which are less effective for image generation tasks.\n","\n","+ Stabilized Training: DCGANs introduce some stability-enhancing techniques in training, such as using batch normalization in both the generator and discriminator <br> \n","and using specific activation functions like Leaky ReLU. \n","<br> These techniques help prevent issues like mode collapse and vanishing gradients.\n","\n","+ Preprocessing: DCGANs require minimal preprocessing of the data, often just scaling the pixel values to a range between -1 and 1. <br>\n","In contrast, some standard GANs may require more complex data preprocessing steps.\n","\n","+ Image Generation Quality: DCGANs are known for producing higher-quality images with more realistic details compared to standard GANs. <br> This is due to the use of convolutional layers, which are better at capturing local image patterns and structures."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1690105636478,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"i5NWykR8Bq_Q","outputId":"04296caf-35b1-46cf-953a-b694b9ce2c30"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 4\n","drwxr-xr-x 1 root root 4096 Jul 20 13:28 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"]}],"source":["#%%script echo Uncomment if not on Colab\n","%ls -lt /content"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1690105644654,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"e6rJNQqzCUgv","outputId":"da23dd81-7e3f-4cd4-a2dc-72aef47e52a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Skipping:Just for Colab, used to import other module (another ipynb notebook)\n"]}],"source":["%%script echo Skipping:Just for Colab, used to import other module (another ipynb notebook)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%run '/content/drive/MyDrive/my_folder/torch_21_DCGANs_model.ipynb'"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10184,"status":"ok","timestamp":1690105667788,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"9eGYceMHGyq3"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":768,"status":"ok","timestamp":1690105679654,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"lKGlQySQJckx","outputId":"d6f4e0a6-f638-4a81-db82-00bd1d6e8291"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":405,"status":"ok","timestamp":1690105683526,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"kow1W7ifBrQ3"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    \"\"\" Discriminator Network model.\n","\n","    Args:\n","        - channels_img: Number of image channels\n","        - features_d: Feature depth \n","    \"\"\"    \n","    def __init__(self, channels_img, features_d):\n","        super(Discriminator, self).__init__()\n","        # Create generator model as a sequential module\n","        self.disc = nn.Sequential(\n","            # First deconv block, takes noise, upsample (input: N x channels_img x 64 x 64)\n","            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            ### Additional deconv blocks \n","            # _block(in_channels, out_channels, kernel_size, stride, padding)\n","            self._block(features_d, features_d * 2, 4, 2, 1),\n","            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n","            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n","            # Output deconv layer after all \"_block img\" output is 4x4 (Conv2d below makes into 1x1)\n","            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n","            \n","            nn.Sigmoid(),\n","        )\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        \"\"\" Add another Convolutional block with Batchnorm and LeakyReLU. \"\"\"\n","        return nn.Sequential(\n","            nn.Conv2d(\n","                in_channels,\n","                out_channels,\n","                kernel_size,\n","                stride,\n","                padding,\n","                bias=False,\n","            ),\n","            # nn.BatchNorm2d(out_channels),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","    def forward(self, x):\n","        \"\"\" Perform forward pass.\"\"\"\n","        return self.disc(x)\n","\n","\n","class Generator(nn.Module):\n","    \"\"\" Generator Network model.\n","\n","    Attributes:\n","        - channels_noise: Noise channels\n","        - channels_img: Image channels\n","        - features_g: Feature depth\n","    \"\"\"    \n","    def __init__(self, channels_noise, channels_img, features_g):\n","        super(Generator, self).__init__()\n","        self.net = nn.Sequential(\n","            #### Additional deconv blocks (Input: N x channels_noise x 1 x 1)\n","            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n","            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n","            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n","            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n","            # Output layer (N x channels_img x 64 x 64)\n","            nn.ConvTranspose2d(\n","                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n","            ),\n","            # Tanh activation for image pixels \n","            nn.Tanh(),\n","        )\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(\n","                in_channels,\n","                out_channels,\n","                kernel_size,\n","                stride,\n","                padding,\n","                bias=False,\n","            ),\n","            # nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x):\n","        \"\"\" Perform forward pass.\"\"\"\n","        return self.net(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def initialize_weights(model):\n","    \"\"\" Initialize weights of the given model \"\"\"\n","    for m in model.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n","            nn.init.normal_(m.weight.data, 0.0, 0.02)\n","\n","\n","def test():\n","    N, in_channels, H, W = 8, 3, 64, 64\n","    noise_dim = 100\n","    x = torch.randn((N, in_channels, H, W))\n","    disc = Discriminator(in_channels, 8)\n","    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n","    gen = Generator(noise_dim, in_channels, 8)\n","    z = torch.randn((N, noise_dim, 1, 1))\n","    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n","    print(\"Success, tests passed!\")"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690105685018,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"kuBzqAiWAbk3"},"outputs":[],"source":["\"\"\" Hyperparameters.\"\"\"\n","LEARNING_RATE = 2e-4      #0.0002\n","BATCH_SIZE = 128\n","IMAGE_SIZE = 64\n","CHANNELS_IMG = 1\n","NOISE_DIM = 100\n","NUM_EPOCHS = 5\n","FEATURES_DISC = 64\n","FEATURES_GEN = 64"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690105686799,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"UVOwucrGAbpL"},"outputs":[],"source":["\"\"\" Define a sequence of image transformations using Compose() that are applied to the training images:\n","    1. Resize the images to the specified IMAGE_SIZE\n","    2. Convert the input image to a PyTorch tensor\n","    3. Normalize the pixel values of the images to the range [-1, 1],\\\\\n","        The mean and standard deviation are set to [0.5, 0.5, ..., 0.5] for each channel.\n","\"\"\"\n","transforms = transforms.Compose(\n","    [\n","        transforms.Resize(IMAGE_SIZE),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n","        ),\n","    ]\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1928,"status":"ok","timestamp":1690105690593,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"zJoR7tVkAbsv","outputId":"1ba6e55d-ff3e-4b68-c855-3376fcebe77a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 290796178.58it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00<00:00, 20065544.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 134231811.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00<00:00, 5642929.14it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["dataset = datasets.MNIST( root=\"dataset/\", train=True, transform=transforms, download=True)   #channels_img=1"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":8322,"status":"ok","timestamp":1690105701473,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"qXiMlBbnAbwN"},"outputs":[],"source":["dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n","disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1131,"status":"ok","timestamp":1690105721849,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"G1LGEHrnIJaD"},"outputs":[],"source":["initialize_weights(gen)\n","initialize_weights(disc)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":476,"status":"ok","timestamp":1690105724642,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"UUyxaySdAbzV"},"outputs":[],"source":["### Optimizers and Loss\n","opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n","opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n","criterion = nn.BCELoss()"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690105724644,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"7WAI5VuIJLLL"},"outputs":[],"source":["\"\"\" Create a fixed_noise Tensor used as input to the generator during training\n","1. Generate random nums from a standard normal distribution with mean 0 and standard deviation 1.\\\\\n","    The tensor has 4 dimensions: batch size (32 samples), number of noise dimensions (100), height, and width.\n","2. Move the tensor to the specified device\n","\"\"\"\n","\n","fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690105726067,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"gaNwc6yMIXYF"},"outputs":[],"source":["\"\"\" Writes entries directly to event files in the log_dir to be consumed by TensorBoard. \"\"\"\n","writer_real = SummaryWriter(f\"logs/real\")\n","writer_fake = SummaryWriter(f\"logs/fake\")"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690105728524,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"OKUx1DoNAuGp","outputId":"84135679-608b-4611-e84d-45d17e98b9f3"},"outputs":[{"data":{"text/plain":["Discriminator(\n","  (disc): Sequential(\n","    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.2)\n","    (2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): LeakyReLU(negative_slope=0.2)\n","    )\n","    (3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): LeakyReLU(negative_slope=0.2)\n","    )\n","    (4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): LeakyReLU(negative_slope=0.2)\n","    )\n","    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n","    (6): Sigmoid()\n","  )\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["step = 0\n","gen.train()\n","disc.train()"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507137,"status":"ok","timestamp":1690106236793,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"PLK6y3c4Ab2g","outputId":"85c50a27-00a4-49a2-df78-f1bec8643e4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [0/5] Batch 0/469                   Loss D: 0.6908, loss G: 0.7850\n","Epoch [0/5] Batch 100/469                   Loss D: 0.0004, loss G: 7.8235\n","Epoch [0/5] Batch 200/469                   Loss D: 0.0003, loss G: 8.9830\n","Epoch [0/5] Batch 300/469                   Loss D: 0.0001, loss G: 9.2160\n","Epoch [0/5] Batch 400/469                   Loss D: 0.0001, loss G: 10.2485\n","Epoch [1/5] Batch 0/469                   Loss D: 0.0000, loss G: 10.7900\n","Epoch [1/5] Batch 100/469                   Loss D: 0.0000, loss G: 10.9549\n","Epoch [1/5] Batch 200/469                   Loss D: 0.0000, loss G: 11.0135\n","Epoch [1/5] Batch 300/469                   Loss D: 0.0000, loss G: 11.7651\n","Epoch [1/5] Batch 400/469                   Loss D: 0.0000, loss G: 12.0667\n","Epoch [2/5] Batch 0/469                   Loss D: 0.0000, loss G: 12.1739\n","Epoch [2/5] Batch 100/469                   Loss D: 0.0000, loss G: 12.4651\n","Epoch [2/5] Batch 200/469                   Loss D: 0.0000, loss G: 12.6769\n","Epoch [2/5] Batch 300/469                   Loss D: 0.0000, loss G: 12.4745\n","Epoch [2/5] Batch 400/469                   Loss D: 0.0000, loss G: 12.8363\n","Epoch [3/5] Batch 0/469                   Loss D: 0.0000, loss G: 12.9820\n","Epoch [3/5] Batch 100/469                   Loss D: 0.0000, loss G: 11.9098\n","Epoch [3/5] Batch 200/469                   Loss D: 0.0000, loss G: 13.2017\n","Epoch [3/5] Batch 300/469                   Loss D: 0.0000, loss G: 13.4215\n","Epoch [3/5] Batch 400/469                   Loss D: 0.0000, loss G: 13.5994\n","Epoch [4/5] Batch 0/469                   Loss D: 0.0000, loss G: 13.6942\n","Epoch [4/5] Batch 100/469                   Loss D: 0.0000, loss G: 13.5712\n","Epoch [4/5] Batch 200/469                   Loss D: 0.0000, loss G: 13.1527\n","Epoch [4/5] Batch 300/469                   Loss D: 0.0000, loss G: 13.8052\n","Epoch [4/5] Batch 400/469                   Loss D: 0.0000, loss G: 13.9922\n"]}],"source":["\"\"\" Training. \n","N.B.\n","Check torch_20_GAN file for further info.\n","\"\"\"\n","for epoch in range(NUM_EPOCHS):\n","    # Target labels not needed! <3 unsupervised\n","    for batch_idx, (real, _) in enumerate(dataloader):\n","        real = real.to(device)\n","        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n","        fake = gen(noise)\n","\n","        ############## Train Discriminator\n","        disc_real = disc(real).reshape(-1)\n","        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n","        disc_fake = disc(fake.detach()).reshape(-1)\n","        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n","        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n","        ### Gradients to zero, backpropagate and update Generator's parameters\n","        disc.zero_grad()\n","        loss_disc.backward()\n","        opt_disc.step()\n","\n","        ##################### Train Generator\n","        output = disc(fake).reshape(-1)\n","        loss_gen = criterion(output, torch.ones_like(output))\n","        ### Gradients to zero, backpropagate and update Generator's parameters\n","        gen.zero_grad()\n","        loss_gen.backward()\n","        opt_gen.step()\n","\n","        # Print losses + print to Tensorboard\n","        if batch_idx % 100 == 0:\n","            print(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n","                    Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\")\n","\n","            with torch.no_grad():\n","                fake = gen(fixed_noise)\n","                # take out (up to) 32 examples\n","                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n","                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n","                ## Log generated fake and real images to TensorBoard\n","                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n","                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n","\n","            step += 1"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOTZZn3oTEzp3TEXiKQaKWL","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
