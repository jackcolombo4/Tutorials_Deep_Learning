{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:1.2;\">\n",
    "\n",
    "<h1 style=\"color:#BF66F2; margin-bottom: 0.3em;\"> RNN + CNN model in PyTorch 1 </h1>\n",
    "\n",
    "<h4 style=\"margin-top: 0.3em; margin-bottom: 1em;\"> Image captioning with an Encoder and a Decoder. Focus on the Compose() function. </h4>\n",
    "\n",
    "<div style=\"line-height:1.4; margin-bottom: 0.5em;\">\n",
    "    <h3 style=\"color: lightblue; display: inline; margin-right: 0.5em;\">Keywords:</h3> SummaryWriter TensorBoard + Dropout + AdaptiveAvgPool2d + filterwarnings + DataLoader drop_last=True\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top: 5px;\">\n",
    "<div style=\"line-height:1.2\">\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: red; display: inline;\">Notes:</h3> \n",
    "    The dataset of images and captions was drastically reduced to make the example reproducible and avoid uploading a huge data folder.\n",
    "</span>\n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#BF66F2 \">  RNN + CNN model in PyTorch 1 </h1>\n",
    "<div style=\"margin-top: -30px;\">\n",
    "<h4> Image captioning with an Encoder and a Decoder. Focus on Compose() function </h4> \n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top: -18px;\">\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3>\n",
    "    SummaryWriter TensorBoard + Dropout + AdaptiveAvgPool2d + filterwarnings + DataLoader drop_last=True\n",
    "</span>\n",
    "<br>\n",
    "<div style=\"margin-top: -1px;\">\n",
    "<div style=\"line-height:1.2\">\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: red; display: inline;\">Notes:</h3> \n",
    "    The dataset of images and captions was drastically reduced to make the example reproducible and avoid uploading a huge data folder.\n",
    "</span>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  #to ignore CUDA warnings when GPU is not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy  \n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import statistics\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#BF66F2\"> Recap: SummaryWriter </h3>\n",
    "<div style=\"margin-top: -17px;\">\n",
    "SummaryWriter is a class in the PyTorch library that provides a way to write TensorBoard event files. <br>\n",
    "It creates a writer object that writes data to a directory specified by the user. <br>\n",
    "During training, the train() function in your code writes the training loss to the SummaryWriter object using the writer.add_scalar() method. <br>\n",
    "The global step value is also recorded.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\" Convolutional neural network (CNN) for encoding image features (inherits from nn.Module).\n",
    "\n",
    "    Parameters:\n",
    "        - The size of the output embedding [int]\n",
    "        - Bool to decide if finetune the CNN [bool]\n",
    "    \n",
    "    Attributes:\n",
    "        - Flag to finetune the CNN [bool].\n",
    "        - Inception V3 network [nn.Module].\n",
    "        - Rectified linear unit activation function [nn.Module].\n",
    "        - Dropout laye [nn.Module].\n",
    "    \n",
    "    Methods:\n",
    "        - __init__(self, embed_size, train_CNN=False)\n",
    "        - forward(images)\n",
    "    \n",
    "    Details:\n",
    "        - Create and inception loading the pre-trained Inception V3 network from the torchvision.models module\n",
    "        - Replace the final fully connected layer of the Inception V3 network with a linear layer of the specified output size.\n",
    "        - Create relu.\n",
    "        - Create dropout layer -> dropout probability of 0.5.\n",
    "    Notes:\n",
    "        'aux_logits' param for inception_v3 cannot be set to False!\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        \"\"\" Constructor to initialize the class \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        #self.inception = models.inception_v3(pretrained=True)\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        #self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)  \n",
    "        self.inception.fc = nn.Identity() \n",
    "        self.relu = nn.ReLU()\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1)) \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\" Computes the forward pass of the CNN on a batch of images.\n",
    "        \n",
    "        Parameters:\n",
    "            - Tensor of images\n",
    "        \n",
    "        Details: \n",
    "            - Pass the input images through the Inception V3 network to produce a tensor of shape (batch_size, embed_size).\n",
    "            - Apply the ReLU activation function and dropout regularization to the tensor of encoded image features and return the resulting tensor.\n",
    "        \n",
    "        Returns:\n",
    "            - Tensor of encoded image features.\n",
    "        \"\"\"\n",
    "        #features = self.inception(images)\n",
    "    \n",
    "        features = self.inception(images) # Extract features from InceptionOutputs\n",
    "        #features = features.view(features.size(0), -1)  # Flatten the features tensor\n",
    "        print(type(features))\n",
    "        features = self.dropout(self.relu(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\" Recurrent neural network (RNN) for decoding image features into captions.\n",
    "\n",
    "    Parameters:\n",
    "        - Size of the input word embeddings [int].\n",
    "        - Size of the hidden state of the LSTM [int].\n",
    "        - Size of the vocabulary [int].\n",
    "        - Number of LSTM layers [int].\n",
    "\n",
    "    Attributes:\n",
    "        - Embedding layer for mapping words to vectors [nn.Module].\n",
    "        - LSTM layer for processing the input sequence [nn.Module].\n",
    "        - Linear layer for mapping from hidden states to output logits [nn.Module].\n",
    "        - Dropout layer [nn.Module].\n",
    "\n",
    "    Methods:\n",
    "        - __init__(self, embed_size, hidden_size, vocab_size, num_layers)\n",
    "        - forward(features, captions)\n",
    "    \n",
    "    Details:\n",
    "        - Create an embedding layer for mapping words to vectors.\n",
    "        - Create an LSTM layer for processing the input sequence.\n",
    "        - Create a linear layer for mapping from hidden states to output logits.\n",
    "        - Create a dropout layer, with a dropout probability of 0.5.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        \"\"\" Constructor to initialize the class \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)  \n",
    "        self.dropout = nn.Dropout(0.5)  \n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\" Compute the Forward pass of the RNN on a batch of image features and captions.         \n",
    "        \n",
    "        Parameters:\n",
    "            - Tensor of features\n",
    "            - Tensor of caption sequences\n",
    "        \n",
    "        Details: \n",
    "            - Map the input caption sequence to a tensor of word embeddings using the embedding layer, and apply dropout regularization.\\\\\n",
    "            The embeddings tensor has a shape of (seq_length, embed_size), where seq_length is the length of the caption sequence. \n",
    "            - Concatenate the features tensor with the embeddings tensor along the batch dimension.\\\\\n",
    "                -The result in a tensor with a shape of:\n",
    "                    - with dim = 0 (#along the first dimension) is (seq_length+1, embed_size)\n",
    "                    - with dim = 0 (#along the second dimension) is (batch_size, seq_length+1, embed_size) \n",
    "                \n",
    "            - Pass the concatenated tensor through the LSTM layer to produce a tensor of hidden states.\n",
    "            - Map the hidden states to a tensor of output logits using the linear layer.\n",
    "        \n",
    "        Returns:\n",
    "            - Tensor of encoded image features, the output logits.\n",
    "        \"\"\"\n",
    "        captions = captions[:, :-1]\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        print(\"Features size in forward pass method:\", features.size())\n",
    "        print(\"Embeddings size in forward pass method:\", embeddings.size())\n",
    "        #embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=1) #along the second dimension\n",
    "        # Concatenate along batch dimension\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)  \n",
    "        \n",
    "        ### Transform input tensor to match expected input size of LSTM layer\n",
    "        embeddings = self.linear(embeddings)  \n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    \"\"\" Neural network that combines an image encoder with a caption decoder.\n",
    "    \n",
    "    Parameters:\n",
    "        - The size of the input word embeddings [int]\n",
    "        - The size of the hidden state of the LSTM [int]\n",
    "        - The size of the vocabulary [int]\n",
    "        - The number of LSTM layers [int]\n",
    "\n",
    "    Attributes:\n",
    "        - encoderCNN (EncoderCNN)\n",
    "        - decoderRNN (DecoderRNN)\n",
    "\n",
    "    Methods:\n",
    "        - __init__(self, embed_size, hidden_size, vocab_size, num_layers)\n",
    "        - forward(self, images, captions)\n",
    "        - caption_image(self, image, vocabulary, max_length=50)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        \"\"\" Constructor to initialize the class \"\"\"\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        #self.encoderCNN = nn.Sequential(*list(inception.children())[:-2], nn.AdaptiveAvgPool2d((1,1)), nn.Identity())        \n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        \"\"\" Compute the forward pass of the CNNtoRNN network on a batch of images and captions.\n",
    "        \n",
    "        Parameters:\n",
    "            - images (torch.Tensor): A tensor of shape (batch_size, channels, height, width) representing a batch of images\n",
    "            - captions (torch.Tensor): A tensor of shape (batch_size, max_seq_length) representing a batch of caption sequences\n",
    "        \n",
    "        Details:\n",
    "            - Create Encoder for extracting image features\n",
    "            - Create caption Decoder for generating captions from image features\n",
    "        \n",
    "        Returns:\n",
    "            - Tensor of shape (batch_size, max_seq_length, vocab_size) representing the output logits for each word in the caption sequence.\n",
    "        \"\"\"\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        \"\"\"Generate a caption for a given image using the trained CNNtoRNN model.\n",
    "        \n",
    "        Parameters:\n",
    "            - image (torch.Tensor): A tensor of shape (channels, height, width) representing an input image\n",
    "            - vocabulary (torchtext.vocab.Vocab): A vocabulary object that maps between words and word indices\n",
    "            - max_length (int): The maximum length of the generated caption\n",
    "        \n",
    "        Details: \n",
    "            - Disable gradient computation during inference to reduce memory usage and speed up computation\n",
    "            - Initialize an empty list to store the generated word indices\n",
    "            - Extract image features from the input image using the image encoder, and add an extra dimension to the tensor for the batch size\n",
    "            - Initialize the hidden state tensor to None\n",
    "            - Iterate over the maximum length of the generated caption\n",
    "                - Pass the input tensor and the hidden state tensor through the LSTM layer of the caption decoder to produce a new hidden state tensor\n",
    "                - Map the hidden state tensor to a tensor of output logits using the linear layer of the caption decoder\n",
    "                Remove the first dimension from the hiddens tensor (1, batch_size, hidden_size), with squeeze(1)\n",
    "                    to pass the tensor to the linear layer of the caption decoder, which expects a tensor of shape (batch_size, vocab_size).\n",
    "                - Select the word index with the highest logit value as the predicted word\n",
    "                - Append the predicted word index to the list of generated word indices\n",
    "                - Map the predicted word index to a tensor of word embeddings using the embedding layer of the caption decoder,\n",
    "                    and add an extra dimension to the tensor for the batch size\n",
    "                - If the predicted word is the end-of-sentence token, break out of the loop\n",
    "        \n",
    "        Returns:\n",
    "            - List of strings representing the generated caption.\n",
    "        \"\"\"\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = self.encoderCNN(image)\n",
    "            states = None\n",
    "            x = features.unsqueeze(0)\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(model, device, dataset):\n",
    "    \"\"\" Print some example image captions generated by the CNNtoRNN model.\n",
    "    \n",
    "    Parameters:\n",
    "        - Trained CNNtoRNN model to use for generating captions\n",
    "        - Device to use for running the model (\"cpu\" or \"cuda\").\n",
    "        - Dataset used to train the model.\n",
    "\n",
    "    Details:    \n",
    "        - Create the composition of image transformation functions that will be applied to images.\\\\\n",
    "            The Compose function is used to create a sequence of transformations that will be applied to an input image in order.\\\\\n",
    "            `transforms.Compose` is a function from the `torchvision.transforms` module that creates a\\\\\n",
    "            composition of multiple image transformations. \n",
    "            - 3 transformation sequence => 3 transformations: \n",
    "                - resizing the image to a fixed size of (299,299 pixels), \n",
    "                - converting the image to a PyTorch tensor, \n",
    "                - normalizing the pixel values of the image to have zero mean and unit variance.\\\\\n",
    "                    (typical preprocessing step to help the model to converge more quickly and improve its overall performance).\\\\\n",
    "            => The resulting tensor has shape (3, 299, 299). --> (color channels (RGB), height, width) \n",
    "\n",
    "        - Add an extra dimension (of size 1) at the beginning of the tensor, with the 'unsqueeze(0)',\\\\\n",
    "            without having to modify the model's input format. \\\\\n",
    "            In fact PyTorch expect, expect the input to be in the form of batches of data, to make the model more stable\\\\\n",
    "            and use parallelism and vectorization to process multiple data points at once.\\\\ \n",
    "            This allows using the same training pipeline and data loading code that you would use for larger datasets,\\\\\n",
    "            without having to make modifications to handle the case of a single data point.\n",
    "\n",
    "        - Print the caption generated by a model for an image, using the \"caption_image\" method,\\\\\n",
    "            which takes an image (`test_img5`) and a vocabulary (`dataset.vocab`) as input.\\\\\n",
    "            The `join` method is used to concatenate the words in the caption into a single string.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    test_img1 = transform(Image.open(\"./data/images_torch_07/flickr8k/test_examples/dog.jpg\").convert(\"RGB\")).unsqueeze(0)\n",
    "    test_img2 = transform(Image.open(\"./data/images_torch_07/flickr8k/test_examples/child.jpg\").convert(\"RGB\")).unsqueeze(0)\n",
    "    test_img3 = transform(Image.open(\"./data/images_torch_07/flickr8k/test_examples/bus.png\").convert(\"RGB\")).unsqueeze(0)\n",
    "    test_img4 = transform(Image.open(\"./data/images_torch_07/flickr8k/test_examples/boat.png\").convert(\"RGB\")).unsqueeze(0)\n",
    "    test_img5 = transform(Image.open(\"./data/images_torch_07/flickr8k/test_examples/horse.png\").convert(\"RGB\")).unsqueeze(0)\n",
    "    \n",
    "    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
    "    print(\"Example 1 OUTPUT: \" + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab)))\n",
    "    print()\n",
    "    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
    "    print(\"Example 2 OUTPUT: \"+ \" \".join(model.caption_image(test_img2.to(device), dataset.vocab)))\n",
    "    print()\n",
    "    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
    "    print(\"Example 3 OUTPUT: \"+ \" \".join(model.caption_image(test_img3.to(device), dataset.vocab)))\n",
    "    print()\n",
    "    print(\"Example 4 CORRECT: A small boat in the ocean\")\n",
    "    print(\"Example 4 OUTPUT: \"+ \" \".join(model.caption_image(test_img4.to(device), dataset.vocab)))\n",
    "    print()\n",
    "    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n",
    "    print(\"Example 5 OUTPUT: \"+ \" \".join(model.caption_image(test_img5.to(device), dataset.vocab)))\n",
    "    \n",
    "    # Set train mode\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    \"\"\" Save the current state of the CNNtoRNN model and optimizer to a checkpoint file. \"\"\"\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    \"\"\" Load the state of the trained CNNtoRNN model and optimizer from a checkpoint dict. \\\n",
    "    Set the current training step to the step saved in the dict.\n",
    "\n",
    "    Parameters:\n",
    "        - Dictionary containing the saved state of the model, optimizer, and training step \n",
    "        - CNNtoRNN model to load the state into.\n",
    "        - Optimizer to load the state into.\n",
    "    \n",
    "    Returns:\n",
    "        - Training step saved in the checkpoint file [int].\n",
    "    \"\"\"\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#BF66F2\"> Recap: Convert text -> numerical values </h3>\n",
    "<div style=\"margin-top: -17px;\">\n",
    "It is necessary to:\n",
    "\n",
    "1. Vocabulary mapping each word to a index\n",
    "2. Setup a Pytorch dataset to load the data\n",
    "3. Setup padding of every batch (all examples should be of same seq_len and setup dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping since already downloaded\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipping since already downloaded\n",
    "#### Download with\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy_eng = spacy.load(\"en\")                   #load a model using a shortcut it is an obsolete way!\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\" Class for building and using a vocabulary for NPL.\n",
    "    \n",
    "    Attributes:\n",
    "        - itos dict to maps integer indices to string tokens;\n",
    "        - stoi dict to maps string tokens to integer indices;\n",
    "        - The minimum frequency required for a word to be included in the vocabulary [int].\n",
    "\n",
    "    Methods:\n",
    "        - __len__(): Returns the number of tokens in the vocabulary.\n",
    "        - tokenizer_eng(text): Tokenizes an English text string using Spacy. \n",
    "        - build_vocabulary(sentence_list): Builds the vocabulary from a list of sentences.\n",
    "        - numericalize(text): Converts a text string to a list of integer indices corresponding to the tokens in the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, freq_threshold):\n",
    "        \"\"\" Constructor. \"\"\"\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return the number of tokens in the vocabulary. \"\"\"\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        \"\"\" Tokenize an English text string using Spacy.\n",
    "\n",
    "        Parameters:\n",
    "            The text to be tokenized [str].\n",
    "\n",
    "        Returns:\n",
    "            List of string tokens.\n",
    "        \"\"\"\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        \"\"\" Set up the vocabulary from a list of sentences.\n",
    "        \n",
    "        Parameters:\n",
    "            List of sentences to build the vocabulary from.\n",
    "\n",
    "        Details: \n",
    "            - Iterate over each sentence in the list\n",
    "            - Tokenize the sentence into words using Spacy\n",
    "\n",
    "        \"\"\"\n",
    "        frequencies = {}\n",
    "        idx = 4        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # If the word is not in the frequency dictionary, add it and set the count to 1\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                # If the word is already in the frequency dictionary, increment its count\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                # If the word count reaches the frequency threshold, add the word to the vocabulary\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        \"\"\" Convert a text string to a list of integer indices corresponding to the tokens in the vocabulary.\n",
    "\n",
    "        Details: \n",
    "            The text string is tokenized into a list of string tokens using the tokenizer_eng method.\\\\\n",
    "            For each token in the tokenized text list, checks if it is in the vocabulary's stoi dictionary (the token is a known word in the vocabulary).\\\\\n",
    "            If it is, the corresponding integer index is added to a list. \\\\\n",
    "            If it is not, the <UNK> token's integer index (index 3 in the vocabulary's stoi dictionary) is added to the list instead.\n",
    "            \n",
    "        Parameters:\n",
    "            Text to be numericalized [str]\n",
    "\n",
    "        Returns:\n",
    "            List of integer indices corresponding to the tokens in the vocabulary\n",
    "        \"\"\"\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    \"\"\" PyTorch Dataset class for loading image-caption pairs from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        - Root directory containing the image files [str]\n",
    "        - Path to the CSV file containing the image-caption pairs [str]\n",
    "        - Transformation to apply to the images (callable, optional)\n",
    "        - Minimum frequency required for a word to be included in the vocabulary [int]\n",
    "\n",
    "    Attributes:\n",
    "        - Root directory containing the image files [str]\n",
    "        - Pandas DataFrame containing the image-caption pairs\n",
    "        - Pandas Series containing the image filenames\n",
    "        - Pandas Series containing the captions\n",
    "        - Vocabulary object used to numericalize the captions\n",
    "\n",
    "    Methods:\n",
    "        - __len__(): Returns the number of image-caption pairs in the dataset\n",
    "        - __getitem__(index): Loads an image-caption pair from the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        ## Get dataset of images\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.transform = transform\n",
    "        ## Get img, caption columns\n",
    "        self.imgs = self.df[\"image\"]\n",
    "        self.captions = self.df[\"caption\"]\n",
    "\n",
    "        ## Initialize vocabulary and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return the number of image-caption pairs in thedataset. \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Load an image-caption pair from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "            The index of the image-caption pair to load [int].\n",
    "\n",
    "        Returns:\n",
    "            The image and its corresponding numericalized caption.\n",
    "        \"\"\"\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    \"\"\" Collate the image and caption data returned by the FlickrDataset class into batches\\\\\n",
    "    that can be fed into a neural network for training. \n",
    "    \n",
    "    Args:\n",
    "        batch (list): A list of tuples containing image and caption data.\n",
    "\n",
    "    Details: \n",
    "        - Creates a list imgs containing the image data from the input batch, extracting the image data\\\\\n",
    "            For each item it takes the first element and unsqueezes it along the first dimension,\\\\\n",
    "            to create a tensor of shape (1, C, H, W), where C, H, and W are the number of channels, height, and width\\\\\n",
    "            imgs = contains len(batch) tensors, each of shape (1, C, H, W).\n",
    "        - Concatenate the image data into a single tensor, along the first dimension.\\\\\n",
    "            imgs = Tensor has shape (batch_size, C, H, W)\n",
    "        - Extract the caption data from the input batch for each item\n",
    "        - Pad the caption data sequence so that all captions have the same length (the length of the longest caption in the batch)\\\\\n",
    "            tensors targets so that all captions have the same length.\\\\\n",
    "            The pad_sequence() function is used to perform the padding.\\\\\n",
    "            By default, pad_sequence() pads sequences to have the same length along the second dimension (batch_first=False).\\\\\n",
    "            targets shape = (max_seq_len, batch_size)\n",
    "\n",
    "    Returns:\n",
    "        Collated image data and caption data.\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_idx):\n",
    "        \"\"\" Initializes a MyCollate object with a padding index.\n",
    "        Args:\n",
    "            pad_idx (int): The index of the padding token in the vocabulary.\n",
    "        \"\"\"\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, targets\n",
    "\n",
    "\n",
    "def get_loader(root_folder, annotation_file, transform, batch_size=32, num_workers=8, shuffle=True, pin_memory=True):\n",
    "    \"\"\" Returns a DataLoader for loading image-caption pairs, to be able to load data in parallel using multiple worker threads.\n",
    "\n",
    "    Parameters:\n",
    "        - Path to the root folder containing the image data [str].\n",
    "        - Path to the file containing the image captions [str].\n",
    "        - Function used to transform the images.\n",
    "        - Batch size for loading data [int].\n",
    "        - Number of worker threads to use for loading data. [int].\n",
    "        - Option to shuffle the data during loading [bool].\n",
    "        - Option to pin the data in memory during loading [bool].\n",
    "\n",
    "    Details: \n",
    "        - Create a FlickrDataset object\n",
    "        - Get the index of the padding <PAD> token in the vocabulary of the FlickrDataset object\\\\\n",
    "        The vocab attribute of the FlickrDataset object contains a torchtext.vocab.Vocab object that maps words to integer indices.\\\\ \n",
    "        The stoi attribute of the Vocab object is a dictionary that maps words to their corresponding integer indices.\\\\\n",
    "        - Create a DataLoader object to load the image-caption pairs in batches. \n",
    "\n",
    "    Notes: \n",
    "        - #***\n",
    "            - Adding drop_last = True is fundamental when dealing with InceptionOutputs!\\\\\n",
    "            To avoid the infamous TypeError, raised when activations functions receive during the training also an \"InceptionOutputs\" object after a batch of Tensors.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader object and the FlickrDataset object.\n",
    "    \"\"\"\n",
    "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=True,     #***\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),)\n",
    "\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\" Encapsulate training process into a method, to train a CNNtoRNN model on the Flickr8k dataset.\n",
    "    \n",
    "    Details:\n",
    "        - Create a sequence of transformations:\n",
    "            4 transformations == 4 sequences: \n",
    "            - resizing to fixed size of (356, 356) pixels\n",
    "            - cropping the images to a random size of 299x299 pixels.\n",
    "            - converting the image to a PyTorch tensor\n",
    "            - normalizing pixel values to have zero mean and unit variance. \n",
    "                Using (0.5, 0.5, 0.5) for both mean and variance => output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "                This means that the pixel values in each channel will be shifted by -0.5 and then scaled by 1/0.5=2,\\\\\n",
    "                which will result in pixel values with zero mean and unit variance. \n",
    "\n",
    "        - Get custom get_loader \n",
    "        - Call the nn.Module model.train() to set the mode of the model object to \"train\".\\\\\n",
    "        - For all epochs:\n",
    "            - Save checkpoint if requested.\n",
    "            - Initializes a for loop that iterates over the batches of data in the train_loader object.\\\\\n",
    "                - \"enumerate\" returns an iterator that generates tuples containing the batch index and the batch data;\\\\\n",
    "                - \"tqdm\" is used to display a progress bar during training;\\\\\n",
    "                - total number of batches in the train_loader;\\\\\n",
    "                - leave=False tells the progress bar to remain visible after training ends. \n",
    "            - Generate output => predicted captions. \n",
    "                caption[:-1] slice to exclude the last word in each caption, since this word is used as the ground truth\\\\ \n",
    "                for the model's prediction.\\\\\n",
    "                In fact, during training, the model is typically trained to predict the next word in the sequence\\\\ \n",
    "                given all the previous words.\\\\\n",
    "                Thus, by removing the last word from the ground truth caption, the model is forced to predict the last word itself\\\\ \n",
    "                and is trained to generate the entire caption from scratch.\\\\\n",
    "                This ensure that the model is not simply memorizing the ground truth captions,\\\\\n",
    "                but is instead learning to generate captions that accurately describe the contents of the input image.\n",
    "\n",
    "            - Calculate the loss between the predicted captions and the ground truth captions using CrossEntropyLoss.\n",
    "                Reshape to flatten the predicted and ground truth captions into 2D arrays with shape:\\\\\n",
    "                (batch_size * seq_len, vocab_size) and (batch_size * seq_len,), respectively.\n",
    "            - Record the training loss to the SummaryWriter object\\\\\n",
    "                \"add_scalar\" saves a scalar value (the training loss at each training iteration) to the event file. \n",
    "            - Perform backpropagation and gradient descent steps of the optimization process, resetting the gradients\\\\ \n",
    "                of all model parameters to zero. \n",
    "            - Update the model parameters using the computed gradients and the optimizer's update rule, doing a step.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_loader, dataset = get_loader(\n",
    "        root_folder=\"./data/images_torch_07/flickr8k/some_images\",\n",
    "        annotation_file=\"./data/images_torch_07/flickr8k/some_captions.txt\",\n",
    "        transform=transform,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # Check if run of GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    load_model, save_model, train_CNN  = False, False, False\n",
    "\n",
    "    ###### Hyperparameters\n",
    "    embed_size = 2048\n",
    "    hidden_size = 2048\n",
    "    vocab_size = len(dataset.vocab)\n",
    "    num_layers = 1\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 2\n",
    "\n",
    "    # Write on Tensorboard \n",
    "    writer = SummaryWriter(\"runs/flickr\")\n",
    "    step = 0\n",
    "    \n",
    "    ### Initialize model, loss, optimizer\n",
    "    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    ##### Finetune the CNN\n",
    "    for name, param in model.encoderCNN.inception.named_parameters():\n",
    "        if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = train_CNN\n",
    "\n",
    "    if load_model:\n",
    "        step = load_checkpoint(torch.load(\"./checkpoints/my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "    # Set train mode\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print_examples(model, device, dataset)\n",
    "        if save_model:\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"step\": step,\n",
    "            }\n",
    "            save_checkpoint(checkpoint)\n",
    "\n",
    "        for idx, (imgs, captions) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Pass complete captions tensor, excluding the last element\n",
    "            outputs = model(imgs, captions[:, :-1])  \n",
    "            # Calculate Loss adjust the target captions accordingly            \n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions[:, 1:].reshape(-1))  \n",
    "\n",
    "            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #loss.backward(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#BF66F2\"> Main </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),])\n",
    "\n",
    "# Create Loader avoiding the UserWarning: This DataLoader will create 8 worker processes in total. \n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    loader, dataset = get_loader(\"./data/images_torch_07/flickr8k/some_images/\", \"./data/images_torch_07/flickr8k/some_captions.txt\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 CORRECT: Dog on a beach by the ocean\n",
      "<class 'torch.Tensor'>\n",
      "Example 1 OUTPUT: <UNK> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS>\n",
      "\n",
      "Example 2 CORRECT: Child holding red frisbee outdoors\n",
      "<class 'torch.Tensor'>\n",
      "Example 2 OUTPUT: <EOS>\n",
      "\n",
      "Example 3 CORRECT: Bus driving by parked cars\n",
      "<class 'torch.Tensor'>\n",
      "Example 3 OUTPUT: people a <EOS>\n",
      "\n",
      "Example 4 CORRECT: A small boat in the ocean\n",
      "<class 'torch.Tensor'>\n",
      "Example 4 OUTPUT: fire man a people a man girl girl . a . water . <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS>\n",
      "\n",
      "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
      "<class 'torch.Tensor'>\n",
      "Example 5 OUTPUT: fire man a people a man girl girl . a . water . <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 CORRECT: Dog on a beach by the ocean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Example 1 OUTPUT: <UNK> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS>\n",
      "\n",
      "Example 2 CORRECT: Child holding red frisbee outdoors\n",
      "<class 'torch.Tensor'>\n",
      "Example 2 OUTPUT: <EOS>\n",
      "\n",
      "Example 3 CORRECT: Bus driving by parked cars\n",
      "<class 'torch.Tensor'>\n",
      "Example 3 OUTPUT: people a <EOS>\n",
      "\n",
      "Example 4 CORRECT: A small boat in the ocean\n",
      "<class 'torch.Tensor'>\n",
      "Example 4 OUTPUT: fire man a people a man girl girl . a . water . <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS>\n",
      "\n",
      "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
      "<class 'torch.Tensor'>\n",
      "Example 5 OUTPUT: fire man a people a man girl girl . a . water . <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  \r"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    train()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Tensorboard call\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipping Tensorboard call \n",
    "# Activate Tensorboard on localhost 6006\n",
    "!python -m tensorboard.main --logdir=logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
