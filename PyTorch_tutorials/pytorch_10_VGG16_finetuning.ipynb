{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "  <h1 style=\"color:#BF66F2 \">  Fine-tuning with pretrained models in PyTorch </h1>\n",
    "  <span style=\"display: inline-block;\">\n",
    "      <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3>\n",
    "    transforms.Compose + WeightedRandomSampler + Loss + Optimizers  \n",
    "    </span>\n",
    "\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"line-height:1.8\">\n",
    "  <h3 style=\"color:red; margin-bottom: 0;\"> Notes: </h3> <!-- Add margin-bottom: 0; here -->\n",
    "  <div style=\"line-height:0.6\">\n",
    "    <div style=\"line-height:1.5\">\n",
    "      Faster training since most parameters are frozen + better generalization due to features learned from a large dataset. <br>\n",
    "      VGG16 alternatives:   <br>\n",
    "      - ResNet - A very common alternative, especially ResNet-18 or ResNet-50. Works well for transfer learning. <br>\n",
    "      - Inception - Also suitable for transfer learning, though sometimes slightly worse than ResNet. <br>\n",
    "      - MobileNet - A more efficient architecture, good if you have memory or latency constraints. <br>\n",
    "\n",
    "In this case Custom the dataset consists of images of dogs images ---> Start with an imbalanced dataset and fix! <br>  WeightedRandomSampler => samples elements from [0,..,len(weights)-1] with given probabilities (a sequence of weights). <br>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  #to ignore CUDA warnings when GPU is not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as Func\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adabound import AdaBound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ImageFolder class expects the root directory to contain subdirectories for each class, <br> and each subdirectory should contain images belonging to that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = datasets.ImageFolder(root=\"./dataset/dogs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imb_get_loader(root_dir, batch_size):\n",
    "    \"\"\" DataLoader object for loading images from the given root directory. \\\\\n",
    "    Load and sample from imbalanced image datasets, where some classes have significantly fewer images than others. \\\\\n",
    "    \n",
    "    Parameters:\n",
    "        - Root directory containing subdirectories of images [str]\n",
    "        - Batch size to use for loading images [int]\n",
    "    \n",
    "    Details: \n",
    "        - #1 Calculate class weights for weighted random sampling after building the path;\n",
    "        - #2 Calculate sample weights for weighted random sampling;\n",
    "        - #3 Use WeightedRandomSampler for weighted random sampling;\n",
    "        - #4 Create DataLoader object using ImageFolder dataset and WeightedRandomSampler.\n",
    "\n",
    "    Returns:\n",
    "        PyTorch DataLoader object for the specified dataset.\n",
    "    \"\"\"\n",
    "    my_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    dataset = datasets.ImageFolder(root=root_dir, transform=my_transforms)\n",
    "    subdirectories = dataset.classes\n",
    "    class_weights = []\n",
    "\n",
    "    for subdir in subdirectories:\n",
    "        files = os.listdir(os.path.join(root_dir, subdir))\n",
    "        class_weights.append(1 / len(files))\n",
    "    \n",
    "    sample_weights = [0] * len(dataset)\n",
    "    \n",
    "    for idx, (data, label) in enumerate(dataset):\n",
    "        class_weight = class_weights[label]\n",
    "        sample_weights[idx] = class_weight\n",
    "\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: torch.Size([9, 3, 224, 224]), torch.Size([9])\n",
      "Batch 1: torch.Size([9, 3, 224, 224]), torch.Size([9])\n",
      "Batch 2: torch.Size([9, 3, 224, 224]), torch.Size([9])\n",
      "Batch 3: torch.Size([9, 3, 224, 224]), torch.Size([9])\n",
      "Batch 4: torch.Size([9, 3, 224, 224]), torch.Size([9])\n",
      "Batch 5: torch.Size([6, 3, 224, 224]), torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "desired_bat, tot_samples = 10, len(dset)\n",
    "\n",
    "# Batch size based on num of samples and desired number of batches\n",
    "batch_size_custom = int(np.ceil(tot_samples / desired_bat))\n",
    "# Create the DataLoader with num of batches => total / size \n",
    "loader = imb_get_loader(root_dir=\"dataset/dogs/\", batch_size=9)  #batch_size=batch_size_custom\n",
    "\n",
    "for batch_idx, (data, labels) in enumerate(loader):\n",
    "    print(f\"Batch {batch_idx}: {data.shape}, {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dog breed Golden is:  247\n",
      "Total dog breed Swedish is: 263\n"
     ]
    }
   ],
   "source": [
    "num_goldens, num_swedish = 0, 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    for data, labels in loader:\n",
    "        num_goldens += torch.sum(labels == 0)\n",
    "        num_swedish += torch.sum(labels == 1)\n",
    "\n",
    "print(\"Total dog breed Golden is: \", num_goldens.item())\n",
    "print(\"Total dog breed Swedish is:\", num_swedish.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#BF66F2\"> Recap: </h3>\n",
    "<div style=\"margin-top: -10px;\">\n",
    "\n",
    "**VGG16:** <br>\n",
    "Convolutional neural network architecture [2014] for image classification tasks.    \n",
    "It consists of 16 layers of convolutional and fully connected layers,   \n",
    "and is known for its simplicity and effectiveness in image recognition tasks.     \n",
    "VGG16 is trained on ImageNet with 1000 classes.   \n",
    "\n",
    "\n",
    "<div style=\"line-height:1.5\">\n",
    "Using a pre-trained model can help with generalization since it has been trained on a large dataset like ImageNet. \n",
    "<div style=\"line-height:1.3\">\n",
    "<div>\n",
    "<div>\n",
    "\n",
    "- Freeze the existing parameters with requires_grad=False.    \n",
    "- Replace the final classification layer to have 10 outputs for dogs classes   \n",
    "- Only train the new classification layer, keeping the feature layers frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "num_classes = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1024\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the VGG16 model (initialized with the default weights) load pretrain model & modify it\n",
    "model = torchvision.models.vgg16(weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): Identity()\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set requires_grad = False to do finetuning\n",
    "# Remove these two lines to train entire model, to load only the pretrain weights.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.avgpool = nn.Identity()\n",
    "model.classifier = nn.Sequential(nn.Linear(512, 100), nn.ReLU(), nn.Linear(100, num_classes))\n",
    "# Send to GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#BF66F2 \">  Recap: </h3>\n",
    "<div style=\"margin-top: -19px;\">\n",
    "\n",
    "<div style=\"line-height:1.3\">\n",
    "<div>\n",
    "<div>\n",
    "\n",
    "- The LOSS defines how we measure error/success for our model predictions.\n",
    "- The OPTIMIZER minimizes a specified loss function by updating the model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.22\">\n",
    "<h3 style=\"color:#BF66F2 \">  Loss </h3>\n",
    "<div style=\"line-height:1.3\">\n",
    "<div>\n",
    "<div>\n",
    "\n",
    "- CrossEntropyLoss - Loss for classification. SGD - Basic stochastic gradient descent optimizer.     \n",
    "- BCEWithLogitsLoss - Loss for binary classification. Adam - Adaptive optimizer that works well for non-convex optimization problems.     \n",
    "- MSELoss - Loss for regression problems. RMSprop - Optimizer that divides the gradient by a decaying average of its recent magnitude.     \n",
    "- NLLLoss - Negative log likelihood loss. Adadelta - Adaptive learning rate method based on RMSProp.     \n",
    "- L1Loss - Loss based on L1 norm. Adamax - Adaptive optimizer using the infinity norm.     \n",
    "- KLDivLoss - Kullback–Leibler divergence loss. SparseAdam - Memory efficient version of Adam optimizer.     \n",
    "- HuberLoss - Robust loss function. AdaBound - Adaptive gradient descent optimizer that combines AdaBound and Adam.     \n",
    "- PoissonNLLLoss - Loss for modeling count data. Rprop - Uses the derivative sign for optimization.     \n",
    "- BCELoss - Binary cross entropy loss. ASGD - Asynchronous Stochastic Gradient Descent optimizer.     \n",
    "- CTCLoss - Connectionist Temporal Classification loss. RAdam - Adaptive variant of Adam optimizer.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.3\">\n",
    "<h3 style=\"color:#BF66F2 \">  Optimizers </h3>\n",
    "<div style=\"line-height:1.5\">\n",
    "SGD - Stochastic Gradient Descent. The basic optimizer that performs parameter updates proportionally to the negative of the gradients.\n",
    "<div style=\"line-height:1.5\">\n",
    "<div>\n",
    "<div>\n",
    "<div>\n",
    "\n",
    "- Adam - Adaptive Moment Estimation optimizer. It adaptively rescales the learning rate for each parameter by calculating <br> individual adaptive learning rates for first and second moments of the gradients.   \n",
    "- RMSprop - Uses a moving average of squared gradients to normalize the gradients, dampening the impact of oscillations or large gradients.     \n",
    "- Adadelta - Scales the learning rate by the average of the magnitude of the recent gradients.     \n",
    "- Adagrad - Accumulates the sum of squared gradients over time and divides the learning rate by this accumulated sum.     \n",
    "- AdaBound - Combines the benefits of AdaBound and Adam. Uses an \"AdaBound schedule\" that starts like Adadelta and ends <br> like SGD with momentum.     \n",
    "- Rprop - Uses the sign of the gradient to determine the update direction, and the derivative estimate's magnitude to determine <br> the update magnitude.     \n",
    "- SparseAdam - A memory-efficient implementation of Adam. Only keeps the sufficient statistics of momentum (first moment) <br> and variability (second moment) of gradients.     \n",
    "- ASGD - Asynchronous Stochastic Gradient Descent. Performs numerous SGD updates asynchronously in parallel to reduce <br> locking and improve convergence speed.     \n",
    "- RAdam - An adaptive variant of Adam that decouples the weight dependence of the adaptive learning rate.     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#BF66F2 \">  Recap: Adaptive Mean Squared Gradients</h3>\n",
    "<div style=\"margin-top: -10px;\">\n",
    "\n",
    "<div style=\"line-height:1.5\">\n",
    "Address some issues with the original Adam optimizer, specifically:  <br>\n",
    "    - Large initial learning rates can cause Adam to diverge, even while the step-size is decayed.   <br> \n",
    "    - Adam stores an exponentially decaying average of past squared gradients, which can \"contaminate\" the estimate of the true second moments.  \n",
    "<div style=\"line-height:1.5\">\n",
    "<div>\n",
    "<div>\n",
    "<div>\n",
    "--> Keeps a \"locked\" estimate of the mean squared gradients    <br>\n",
    "--> Uses that locked estimate in the denominator when calculating the adaptive learning rate   <br>\n",
    "--> Tends to stabilize Adam and improve performance   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#BF66F2 \"> <u>  => 1) First trial with CTCLoss and SGD </u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss and optimizer options.\n",
    "    _BCEWithLogitsLoss for binary classification problems\n",
    "    _RMSprop optimize the weights to minimize that loss function\n",
    "\"\"\"\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "#criterion = nn.L1Loss()\n",
    "#criterion = nn.KLDivLoss()\n",
    "#criterion = nn.HuberLoss()\n",
    "#criterion = nn.PoissonNLLLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.CTCLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.8)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)   \n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.SparseAdam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = AdaBound(model.parameters(), lr=learning_rate, weight_decay=0.7, betas=(0.6, 0.9))\n",
    "# optimizer = optim.Rprop(model.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.ASGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.RAdam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:24<00:00,  4.05s/it]\n",
      "100%|██████████| 6/6 [00:22<00:00,  3.79s/it]\n",
      "100%|██████████| 6/6 [00:25<00:00,  4.19s/it]\n",
      "100%|██████████| 6/6 [00:29<00:00,  4.85s/it]\n",
      "100%|██████████| 6/6 [00:25<00:00,  4.31s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Modify the model to have 2 output classes.\n",
    "- Set the right num of is crucial to avoid the RuntimeError of nn.Module linear.py lib.\n",
    "N.B.\n",
    "mat1 and mat2 shapes cannot be multiplied (batch x size (25088 in this case) and Linear_in_features x Linear_out_features).\n",
    "\"\"\"\n",
    "model.classifier = nn.Sequential(nn.Linear(25088, 100), nn.ReLU(), nn.Linear(100, 2))\n",
    "\n",
    "## Adjust loss and optimizer for 2 classes \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "######## Train DataLoader_1\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(loader)):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 0.11726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:28<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 1 is 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:31<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 2 is 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:35<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 3 is 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:40<00:00,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 4 is 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the 2-class dog dataset \n",
    "loader = imb_get_loader(root_dir=\"dataset/dogs/\", batch_size=batch_size_custom)\n",
    "\n",
    "############# Train again loader    \n",
    "for epoch in range(num_epochs):\n",
    "    losses = []    \n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(loader)):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)  \n",
    "        ## Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient descent or Adam step  \n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Accuracy checking.   \n",
    "N.B.\n",
    "predictions = model(x).argmax(dim=1)#, keepdim=True) => Lead to having more num_correct than num_samples!\n",
    "When you include keepdim=True, the output of argmax retains its original shape with a single dimension for the class index, \n",
    "and it's compared element-wise with y, which typically has the shape (batch_size,). \n",
    "\"\"\"\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            predictions = model(x).argmax(dim=1)#, keepdim=True)\n",
    "            # Convert to Python scalar\n",
    "            num_correct += (predictions == y).sum().item()\n",
    "            num_samples += predictions.size(0)\n",
    "        accuracy = (num_correct / num_samples) * 100    \n",
    "        print(f\"Got {num_correct} / {num_samples} with accuracy {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 51 / 51 with accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on the custom loader        \n",
    "check_accuracy(loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#BF66F2 \"> <u>  => 2) Second trial changing criterion and optimizer (HuberLoss + Adam) </u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.HuberLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Train new loader    \n",
    "def train_with_huber_loss(loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)  # Use nn.HuberLoss() here\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 51 / 51 with accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on the custom loader        \n",
    "check_accuracy(loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
