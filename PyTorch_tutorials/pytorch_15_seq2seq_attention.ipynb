{"cells":[{"cell_type":"markdown","metadata":{},"source":["<div style=\"line-height:0.5\">\n","<h1 style=\"color:#BF66F2 \"> Natural Language Processing in PyTorch 2 </h1>\n","<h4> NLP translation (eng to ita) with Bahdanau Attention Mechanism. </h4>\n","<h3 style=\"color:lightblue\"> Keywords: </h3> matplotlib ticker + unicodedata + bottom-margin in markdown + RandomSampler + FixedLocator + set_yticklabels\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#from __future__ import division, print_function, unicode_literals\n","#import print_function\n","#import unicode_literals\n","#import division"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQlLc1lZXNfQ"},"outputs":[],"source":["import re\n","import io\n","import time\n","import math\n","from io import open\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","from google.colab import files\n","\n","import unicodedata\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from matplotlib.ticker import FixedLocator, FixedFormatter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1689600043158,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"WIMxStzCXNjN","outputId":"ca36fa15-a29f-4148-983c-71ef511f3f4e"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":238557,"status":"ok","timestamp":1689600281696,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"b-au89CBXNmh","outputId":"485d0a3f-69cb-4543-9b11-502ded880d2a"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-5afb7c3d-ad7a-4518-8b99-6683e3b898ad\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-5afb7c3d-ad7a-4518-8b99-6683e3b898ad\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving eng-ita.txt to eng-ita.txt\n"]}],"source":["#%%script echo uncomment if not on Colab\n","\n","# Ask the user to upload a file\n","uploaded = files.upload()\n","\n","## Read the contents of the uploaded file\n","file = next(iter(uploaded))\n","file_content = uploaded[file].decode('utf-8')\n","\n","# Load the contents of the file into a pandas dataframe\n","df = pd.read_csv(io.StringIO(file_content), sep='\\t', header=None, names=['English', 'Italian'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1689600281697,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"9agbrzUOXNqN","outputId":"a1a73511-32ec-4281-c263-1d6f84676d8f"},"outputs":[{"data":{"text/html":["\n","\n","  <div id=\"df-0c899bc8-c062-4292-ade0-dbac998ce5b9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>English</th>\n","      <th>Italian</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hi.</td>\n","      <td>Ciao!</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Run!</td>\n","      <td>Corri!</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Corra!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Correte!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Who?</td>\n","      <td>Chi?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c899bc8-c062-4292-ade0-dbac998ce5b9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-81fcfe17-be9d-4cd1-83f0-21b4a4825245\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-81fcfe17-be9d-4cd1-83f0-21b4a4825245')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-81fcfe17-be9d-4cd1-83f0-21b4a4825245 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0c899bc8-c062-4292-ade0-dbac998ce5b9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0c899bc8-c062-4292-ade0-dbac998ce5b9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"],"text/plain":["  English   Italian\n","0     Hi.     Ciao!\n","1    Run!    Corri!\n","2    Run!    Corra!\n","3    Run!  Correte!\n","4    Who?      Chi?"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"tiMloWEwXNtg"},"source":["<h3 style=\"color:#BF66F2 \"> => One-hot encoding </h3>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6907XBgoXNw3"},"outputs":[],"source":["\"\"\" Create a language model for text dataset.\n","N.B.\n","Start of sequence SOS (first input to a neural network model) and End of sequence (EOS) are special tokens,\\\\\n","that are added to the beginning and end of a sentence.\n","\"\"\"\n","SOS_token, EOS_token = 0, 1\n","\n","class Lang:\n","    \"\"\" Custom Language model. \n","    \n","    Attributes:\n","        - name: The name of the language [str]\n","        - word2index: that maps words in the language to unique integer indices [dict]\n","        - word2count: that stores the count of each word in the language [dict]\n","        - index2word: that maps integer indices back to words in the language [dict]\n","        - n_words: that stores the total number of unique words in the language [int]\n","    \n","    Methods:\n","        - addSentence(self, sentence): Add to the Lang object a sentence \n","        - addWord(self, word): Add to the Lang object a word \n","    \"\"\"\n","    def __init__(self, name):\n","        \"\"\" Initializations. \"\"\"\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        \"\"\" Add all the words in the sentence to the Lang object. \"\"\"\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        \"\"\" Add the word to the Lang object if it is not already present.\n","        If the word is already present, it increments the count of the word in the word2count dict.\n","        \"\"\"\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"line-height:0.1\">\n","\n","<h4 style=\"color:#BF66F2 \"><b> Recap: </b> </h4>\n","</div>\n","The 'unicodedata' module can access to the Unicode Character Database (UCD) which defines character properties <br>\n","for all Unicode characters. The data contained in this database is compiled from the UCD version 14.0.0. <br>\n","It supports all of the world’s writing systems and ensures that data can be retrieved or combined using any combination of languages. <br>\n","The module uses the same names and symbols as defined by Unicode Standard Annex #44, “Unicode Character Database”. <br>\n","\n","<h4 style=\"color:#BF66F2; margin-top: 5px;\"> Common functions: </h4>\n","\n","<div style=\"margin-top: -15px;\">\n","\n","- unicodedata.lookup(name) => Look up character by name. \n","- unicodedata.name(chr[, default]) => Returns the name assigned to the character chr as a string.\n","- unicodedata.decimal(chr[, default]) => Returns the decimal value assigned to the character chr as integer.\n","- unicodedata.digit(chr[, default]) => Returns the digit value assigned to the character chr as integer.\n","- unicodedata.numeric(chr[, default]) => Returns the numeric value assigned to the character chr as float.\n","- unicodedata.category(chr) => Returns the general category assigned to the character chr as string.\n","- unicodedata.bidirectional(chr) => Returns the bidirectional class assigned to the character chr as string. \n","- unicodedata.combining(chr) => Returns the canonical combining class assigned to the character chr as integer.\n","- unicodedata.east_asian_width(chr) => Returns the east asian width assigned to the character chr as string.\n","- unicodedata.mirrored(chr) => Returns the mirrored property assigned to the character chr as integer.\n","- unicodedata.decomposition(chr) => Returns the character decomposition mapping assigned to the character chr as string. \n","- unicodedata.normalize(form, unistr) => Return the normal form form for the Unicode string unistr. \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9mUsheKXNz-"},"outputs":[],"source":["def unicodeToAscii(s):\n","    \"\"\" Turn a Unicode string to plain ASCII.\\\\\n","        1. Normalize the Unicode string \"s\" by decomposing any accented characters into the base character and the accent mark.\n","        2. Remove any combining diacritical marks (Mn) from the string.\n","    N.B.\n","    Valid values for 'normalize()' are 'NFC', 'NFKC' 'NFD', and 'NFKD'.\n","    \"\"\"\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","def normalizeString(s):\n","    \"\"\" Normalize given string.\n","    \n","    Details:\n","        - Convert from Unicode to ASCII\n","        - Transform to lowercase and remove trailing whitespace\n","        - Replace periods, exclamation marks, or question marks with a space and the same punctuation mark\n","        - Remove non-letter characters from the string\n","        - Remove trailing whitespace from the string\n","    \"\"\"\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n","    return s.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvelLXBof-Su"},"outputs":[],"source":["def readLangs(lang1, lang2, reverse=False):\n","    \"\"\" Pair from a file and returns two Lang objects and a list of pairs.\n","    \n","    Parameters:\n","        - \"lang1\" and \"lang2\" specify the names of the languages in the file\n","        - \"reverse\" specifies whether to reverse the order of the pairs\n","        \n","    Details:\n","        - Read the file and split into lines\n","        - Split every line into pairs and normalize\n","        - Reverse pairs, make the instances of Lang if \"reverse\" is True\n","        \n","    Returns:\n","        Input and output Lang objects and the list of pairs\n","    \"\"\"\n","    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n","        read().strip().split('\\n')\n","    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        input_lang = Lang(lang2)\n","        output_lang = Lang(lang1)\n","    else:\n","        input_lang = Lang(lang1)\n","        output_lang = Lang(lang2)\n","\n","    return input_lang, output_lang, pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukNKYqyrgAwW"},"outputs":[],"source":["\"\"\" Trim the data set to only relatively short and simple sentences. \"\"\"\n","\n","MAX_LENGTH = 10\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \")\n","\n","def filterPair(p):\n","    \"\"\" Filter a pair of sentences based on their length and prefix.\n","\n","    Parameters:\n","        p: A pair of sentences tuple]\n","    \n","    Returns:\n","        Boolean value => True if both sentences have a length less than MAX_LENGTH\\\\\n","        and the second sentence starts with one of the prefixes in eng_prefixes \\\\\n","        False otherwise.\n","    \"\"\"\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH and \\\n","        p[1].startswith(eng_prefixes)\n","\n","def filterPairs(pairs):\n","    \"\"\" Filters a list of sentence pairs based on their length and prefix.\n","\n","    Parameters:\n","        pairs [list]: List of sentence pairs as tuples\n","    \n","    Returns:\n","        New list containing only the sentence pairs that passed the filter\n","    \"\"\"\n","    return [pair for pair in pairs if filterPair(pair)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvKl36MeiyKX"},"outputs":[],"source":["def prepareData(lang1, lang2, reverse=False):\n","    \"\"\" Read language pairs from a file, filters the pairs, and creates Lang objects for the input and output languages.\n","    \n","    Parameters:\n","        - lang1: The name of the first language in the file [str]\n","        - lang2: The name of the second language in the file [str]\n","        - reverse: Whether to reverse the order of the language pairs [bool]\n","    \n","    Returns:\n","        Input_lang, output_lang, and pairs\n","    \"\"\"\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentence pairs...\" % len(pairs))\n","\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","\n","    print(\"Num of words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","\n","    return input_lang, output_lang, pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10239,"status":"ok","timestamp":1689600292886,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"xlqzv0opjTTE","outputId":"6bc148f2-a9db-43bc-8e9f-7f803101cd63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Read 331799 sentence pairs...\n","Trimmed to 32084 sentence pairs\n","Counting words...\n","Num of words:\n","ita 5400\n","eng 3168\n"]}],"source":["input_lang, output_lang, pairs = prepareData('eng', 'ita', True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11027,"status":"ok","timestamp":1689600303903,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"lGcFJP9KjZD8","outputId":"63174dd9-963b-4198-a2ee-2ad3e2e0657e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Read 331799 sentence pairs...\n","Trimmed to 32084 sentence pairs\n","Counting words...\n","Num of words:\n","ita 5400\n","eng 3168\n","['stai facendo dell ottima roba qui', 'you re doing great stuff here']\n"]}],"source":["input_lang, output_lang, pairs = prepareData('eng', 'ita', True)\n","print(random.choice(pairs))"]},{"cell_type":"markdown","metadata":{"id":"EVA4hPU2j0Re"},"source":["<h3 style=\"color:#BF66F2 \"> => Seq2seq Network: Encoder-Decoder objects </h3>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1rkUPlmj2-6"},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    \"\"\" Encoder component of a sequence-to-sequence model.\n","    \n","    Attributes:\n","        - input_size [int): The size of the input vocabulary.\n","        - hidden_size [int]: The size of the hidden state of the GRU.\n","        - dropout_p [float]: The probability of dropping out a unit in the dropout layer.\n","    \"\"\"\n","    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n","        \"\"\" Initialize the encoder \"\"\"\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, input):\n","        \"\"\" Perform a forward pass of the encoder.\n","        \n","        Parameters:\n","            Input tensor of shape (batch_size, seq_length)\n","        \n","        Returns:\n","            - The output tensor of shape (batch_size, seq_length, hidden_size)\n","            - The hidden state tensor of shape (1, batch_size, hidden_size)\n","        \"\"\"\n","        embedded = self.dropout(self.embedding(input))\n","        output, hidden = self.gru(embedded)\n","        return output, hidden"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsO-8lJPj_NQ"},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    \"\"\" Decoder component of a sequence-to-sequence model.\n","    \n","    Attributes:\n","        - Size of the hidden state of the GRU [int]\n","        - Size of the output vocabulary [int]\n","    \"\"\"\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","        \"\"\" Performs a forward pass of the decoder.\n","        \n","        Parameters:\n","            - The output tensor from the encoder of shape (batch_size, seq_length, hidden_size).\n","            - The hidden state tensor from the encoder of shape (1, batch_size, hidden_size).\n","            - The target tensor of shape (batch_size, seq_length). (default: None)\n","\n","        Details:\n","            - The decoder receives the ground truth target sequence as input at each time step, known as \"Teacher forcing\",\\\\\n","            as the decoder is being \"forced\" to produce the correct output at each time step based on the ground truth targets.\n","            \n","            - The decoder's output at each time step is compared to the corresponding target token using a loss function\\\\\n","            (such as cross-entropy loss).\\\\\n","            The gradients are then backpropagated through time to update the model's parameters.\n","            \n","            - The decoder generates its output tokens based on its own previous output tokens (rather than the ground truth targets),\\\\\n","            which can lead to errors propagating through the sequence.\n","            \n","            - Teacher forcing is not used during inference, and instead the decoder's previous output token\\\\ \n","            is used as input to generate the next token.\\\\\n","            \n","            - The process is repeated until an end-of-sequence token is generated or a maximum sequence length is reached.\n","\n","        Returns:\n","            - Output tensor of shape (batch_size, seq_length, output_size)\n","            - Hidden state tensor of shape (1, batch_size, hidden_size)\n","            - None placeholder value to maintain consistency in the training loop\n","        \"\"\"\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs = []\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n","            decoder_outputs.append(decoder_output)\n","\n","            if target_tensor is not None:\n","                # With teacher forcing: Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1)\n","            else:\n","                # Without teacher forcing: use its own predictions as the next input (detach from history as input)\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach()\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","\n","        return decoder_outputs, decoder_hidden, None\n","\n","    def forward_step(self, input, hidden):\n","        \"\"\" Performs a single step of the forward pass of the decoder.\n","        \n","        Parameters:\n","            - Input tensor of shape (batch_size, 1)\n","            - Hidden state tensor of shape (1, batch_size, hidden_size)\n","        \n","        Returns:\n","            Output tensor of shape (batch_size, 1, output_size)\n","            Hidden state tensor of shape (1, batch_size, hidden_size)\n","        \"\"\"\n","        output = self.embedding(input)\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.out(output)\n","\n","        return output, hidden"]},{"cell_type":"markdown","metadata":{"id":"WjE5lQEymsA-"},"source":["<h3 style=\"color:#BF66F2 \"> Attention Decoder: </h3>\n","<div style=\"margin-top: -20px;\">\n","Bahdanau attention, also known as additive attention, is an attention mechanism in sequence-to-sequence models. <br>\n","It employs a learned alignment model to compute attention scores between the encoder and decoder hidden states.<br>\n","It utilizes a feed-forward neural network to calculate alignment scores.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mov33Gn4m4YW"},"outputs":[],"source":["class BahdanauAttention(nn.Module):\n","    \"\"\" Bahdanau attention mechanism.\n","\n","    Args:\n","        - Size of the hidden state of the decoder [int]\n","\n","    Attributes:\n","        - Wa: Linear layer for the query transformation [nn.Linear]\n","        - Ua: Linear layer for the keys transformation [nn.Linear]\n","        - Va: Linear layer for the attention scoring [nn.Linear]\n","    \n","    Methods:\n","        forward(self, query, keys): Forward pass of the Bahdanau Attention module\n","    \"\"\"\n","    def __init__(self, hidden_size):\n","        super(BahdanauAttention, self).__init__()\n","        self.Wa = nn.Linear(hidden_size, hidden_size)\n","        self.Ua = nn.Linear(hidden_size, hidden_size)\n","        self.Va = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, query, keys):\n","        \"\"\" Performs a forward pass of the attention mechanism.\n","        \n","        Parameters:\n","            - Query tensor of shape (batch_size, 1, hidden_size)\n","            - Keys tensor of shape (batch_size, seq_length, hidden_size)\n","        \n","        Details: \n","            - Calculate attention scores using query and keys\n","            - Squeeze the scores tensor to remove the third dimension and unsqueeze to add a new dimension\n","            - Apply softmax to obtain attention weights\n","            - Calculate the context vector by applying attention weights to keys\n","\n","        Returns:\n","            - Context tensor of shape (batch_size, 1, hidden_size)\n","            - Attention weights tensor of shape (batch_size, 1, seq_length)\n","        \"\"\"\n","        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","\n","        weights = F.softmax(scores, dim=-1)\n","        context = torch.bmm(weights, keys)\n","\n","        return context, weights\n","\n","\n","class AttnDecoderRNN(nn.Module):\n","    \"\"\" Decoder component of a sequence-to-sequence model with attention.\\\\\n","    AttnDecoderRNN network consists of five layers:\n","        - embedding: Embedding layer for the output tokens\n","        - attention: Bahdanau Attention layer\n","        - gru: Gated Recurrent Unit (GRU) layer\n","        - out: Linear layer for output generation\n","        - dropout: Dropout layer for regularization\n","\n","    Args:\n","        - Size of the hidden state of the decoder [int]\n","        - Size of the output vocabulary [int]\n","        - Dropout probability [float, optional (Default is 0.1)]\n","    \n","    Attributes:\n","        - embedding: Embedding layer for the output tokens [nn.Embedding]\n","        - attention: Bahdanau Attention module [BahdanauAttention]\n","        - gru: Gated Recurrent Unit (GRU) layer [nn.GRU]\n","        - out: Linear layer for output generation [nn.Linear]\n","        - dropout: Dropout layer for regularization. [nn.Dropout]\n","    \"\"\"\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.attention = BahdanauAttention(hidden_size)\n","        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","        \"\"\" Performs a forward pass of the decoder.\n","        \n","        Parameters:\n","            - Output tensor from the encoder of shape (batch_size, seq_length, hidden_size).\n","            - Hidden state tensor from the encoder of shape (1, batch_size, hidden_size).\n","            - Target tensor of shape (batch_size, seq_length). (default: None)\n","        \n","        Returns:\n","            - Output tensor of shape (batch_size, seq_length, output_size).\n","            - Hidden state tensor of shape (1, batch_size, hidden_size).\n","            - Attention weights tensor of shape (batch_size, seq_length, 1).\n","        \"\"\"\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs, attentions = [], []\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n","            decoder_outputs.append(decoder_output)\n","            attentions.append(attn_weights)\n","\n","            if target_tensor is not None:\n","                # ...With teacher forcing: Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1)\n","            else:\n","                # ...Without teacher forcing: use its own predictions as the next input\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach()       #detach from history as input!\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","        attentions = torch.cat(attentions, dim=1)\n","\n","        return decoder_outputs, decoder_hidden, attentions\n","\n","    def forward_step(self, input, hidden, encoder_outputs):\n","        \"\"\" Perform a single step of the forward pass during decoding.\n","        \n","        Parameters:\n","            - The input tensor of shape (batch_size, 1)\n","            - The hidden state tensor of shape (1, batch_size, hidden_size)\n","            - The output tensor from the encoder of shape (batch_size, seq_length, hidden_size)\n","        \n","        Details:\n","            - Apply embedding and dropout to the input tensor\n","            - Permute the dimensions of the hidden state tensor to obtain the query tensor\n","            - Pass the query tensor and encoder outputs through the attention mechanism to obtain the context vector\\\\\n","            and attention weights\n","            - Concatenate the embedded input and context vector to obtain the input tensor for the GRU\n","            - Pass the input tensor and hidden state tensor through the GRU to obtain the output tensor\\\\\n","            and updated hidden state tensor\n","            - Pass the output tensor through a linear layer to obtain the output logits tensor\n","        \n","        Returns:\n","            - Output logits tensor of shape (batch_size, 1, output_size)\n","            - Updated hidden state tensor of shape (1, batch_size, hidden_size)\n","            - Attention weights tensor of shape (batch_size, 1, seq_length)\n","        \"\"\"\n","        embedded = self.dropout(self.embedding(input))\n","        query = hidden.permute(1, 0, 2)\n","        context, attn_weights = self.attention(query, encoder_outputs)\n","        input_gru = torch.cat((embedded, context), dim=2)\n","        output, hidden = self.gru(input_gru, hidden)\n","        output = self.out(output)\n","\n","        return output, hidden, attn_weights"]},{"cell_type":"markdown","metadata":{"id":"LSEJHJLQpMNB"},"source":["<h2 style=\"color:#BF66F2 \"> Training </h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4BRbY-9pWeN"},"outputs":[],"source":["\"\"\" Preparing Training Data.\n","N.B.1\n","Need an input tensor (indexes of the words in the input sentence) + \n","target tensor (indexes of the words in the target sentence).\n","N.B.2\n","Append the EOS token to both sequences.\n","\"\"\"\n","def indexesFromSentence(lang, sentence):\n","    \"\"\"Convert a sentence to a list of its corresponding word indices in the given language.\n","    \n","    Parameters:\n","        - Language object representing the language\n","        - Sentence to convert [str]\n","    \n","    Returns:\n","        Word indices [list]\n","    \"\"\"\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","def tensorFromSentence(lang, sentence):\n","    \"\"\" Convert a sentence to a tensor of its corresponding word indices, with an EOS token appended.\n","    \n","    Parameters:\n","        - Language object representing the language\n","        - Sentence to convert [str]\n","    \n","    Returns:\n","        Tensor of word indices, with shape (1, seq_length)\n","    \"\"\"\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n","\n","def tensorsFromPair(pair):\n","    \"\"\" Convert a pair of input and target sentences to corresponding tensors of word indices.\n","    \n","    Parameters:\n","        Pair of input and target sentences. [(Tuple[str, str])]\n","    \n","    Returns:\n","        Input and target tensors of word indices, with shapes (1, input_seq_length) and (1, target_seq_length), respectively.\\\\\n","        [Tuple[Tensor, Tensor]]\n","    \"\"\"\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)\n","\n","def get_dataloader(batch_size):\n","    \"\"\" Get a PyTorch DataLoader for the training data.\n","    \n","    Parameters:\n","        Batch size for the DataLoader [int]\n","    \n","    Details:\n","        - Convert the sentences in the pairs to numpy arrays of word indices\n","        - Create a PyTorch TensorDataset from the numpy arrays of word indices\n","        - Create a PyTorch DataLoader from the TensorDataset\n","    \n","    Returns:\n","        Input, output Language objects + training DataLoader\n","    \"\"\"\n","    input_lang, output_lang, pairs = prepareData('eng', 'ita', True)\n","\n","    ########## Convert the sentences\n","    n = len(pairs)\n","    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","    for idx, (inp, tgt) in enumerate(pairs):\n","        inp_ids = indexesFromSentence(input_lang, inp)\n","        tgt_ids = indexesFromSentence(output_lang, tgt)\n","        inp_ids.append(EOS_token)\n","        tgt_ids.append(EOS_token)\n","        input_ids[idx, :len(inp_ids)] = inp_ids\n","        target_ids[idx, :len(tgt_ids)] = tgt_ids\n","\n","    # TensorDataset\n","    train_data = TensorDataset(torch.LongTensor(input_ids).to(device), torch.LongTensor(target_ids).to(device))\n","\n","    ## DataLoaders\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    return input_lang, output_lang, train_dataloader\n"]},{"cell_type":"markdown","metadata":{"id":"iXb-DCaapWjZ"},"source":["<div style=\"line-height:0.5\">\n","<h2 style=\"color:#BF66F2 \"> Actual Training:</h2>\n","</div>\n","The input sentence pass through the encoder, every output and the latest hidden state are tracked. <br> \n","The decoder take the <SOS> token as first input, and the last hidden state of the encoder as its first hidden state. <br>\n","\n","The outputs of teacher-forced networks read with coherent grammar but wander far from the correct translation since it learned <br> to represent the output grammar and can get the meaning once the teacher tells it the first few words, \n","but it has not properly learned <br> how to create the sentence from the translation in the first place. <br>\n","<div style=\"line-height:0.7\">\n","<h3 style=\"color:#BF66F2 \"> Steps </h3>\n","</div>\n","<div style=\"margin-top: -15px;\">\n","\n","- Start a timer    \n","- Initialize optimizers and criterion   \n","- Create set of training pairs    \n","- Start empty losses array for plotting   \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBQLo6VRpWor"},"outputs":[],"source":["def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n","    \"\"\" Train the neural machine translation model for a single epoch.\n","    \n","    Parameters:\n","        - DataLoader containing the training data\n","        - Encoder object\n","        - Decoder object\n","        - optimizer for the Encoder\n","        - optimizer for the Decoder\n","        - loss function\n","    \n","    Details:\n","        - Zero the gradients for the Encoder and Decoder optimizers;\n","        - Feed the input tensor through the Encoder to obtain the Encoder outputs and hidden state;\n","        - Feed the Encoder outputs, hidden state, and target tensor through the Decoder to obtain the Decoder outputs;\n","        - Compute the loss between the Decoder outputs and the target tensor;\n","        - Backpropagate the loss and compute the gradients for the Encoder and Decoder;\n","        - Update the parameters of the Encoder and Decoder using their respective optimizers;\n","        - Add the loss for this batch to the total loss for the epoch;\n","    \n","    Returns:\n","        The average loss per batch for the epoch [float].\n","    \"\"\"\n","    total_loss = 0\n","    for data in dataloader:\n","        input_tensor, target_tensor = data\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","        ## Feed\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n","\n","        loss = criterion(\n","            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n","            target_tensor.view(-1))\n","        loss.backward()\n","        ## Update\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXLKI2ympWsg"},"outputs":[],"source":["def asMinutes(s):\n","    \"\"\"Convert a time in seconds to a string in the format m minutes s seconds.\n","    \n","    Parameters:\n","        - Time in seconds \n","    \n","    Details:\n","        - Compute the number of minutes\n","        - Subtract the number of minutes from the total time to get the number of seconds\n","    \n","    Returns:\n","        Time in the format \"m minutes s seconds\" [str]\n","    \"\"\"\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    \"\"\" Compute the time elapsed since a given time, as well as the estimated time remaining.\n","    \n","    Parameters:\n","        - Starting time in seconds (since) [float]\n","        - Percentage of the task that has been complete [float]\n","    \n","    Details:\n","        - Get the current time\n","        - Compute the time elapsed since the starting time\n","        - Compute the estimated total time based on the percentage of the task that has been completed\n","        - Compute the estimated time remaining\n","        - Return the elapsed time and estimated time remaining, in the format \"elapsed time (- estimated time remaining)\"\n","    \n","    Returns:\n","        - Elapsed time and the estimated time remaining, in the format \"elapsed time (- estimated time remaining)\" [str]\n","    \"\"\"\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-Z_MioA8FCG"},"outputs":[],"source":["def showPlot(points):\n","    # Change the backend used for rendering plots\n","    plt.switch_backend('agg') \n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # Put ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2) \n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"galEu6IUpWy9"},"outputs":[],"source":["def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n","    \"\"\" Train the neural machine translation model for a specified number of epochs.\n","    \n","    Parameters:\n","        - DataLoader containing the training data.\n","        - Encoder object.\n","        - Decoder object.\n","        - Number of epochs to train for. [int]\n","        - Learning rate for the optimizer. Default 0.001. [float]\n","        - Frequency (in epochs) at which to print the average loss. Default 100 [int] => print_every\n","        - Frequency (in epochs) at which to record the average loss for plotting. Default 100. [int] => plot_every\n","    \n","    Details:\n","        - Record the starting time\n","        - Reset print_every\n","        - Reset plot_every\n","        - Create an optimizer for the Encoder\n","        - Create an optimizer for the Decoder\n","        - Create a loss function\n","        - Loop over the specified number of epochs\n","            - Compute the average loss over the last print_every epochs\n","            - Compute the average loss over the last plot_every epochs\n","    \"\"\"\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0\n","    plot_loss_total = 0\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(1, n_epochs + 1):\n","        # Train the model for one epoch\n","        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)  \n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if epoch % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n","                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n","\n","        if epoch % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0EyS9NbpW5p"},"outputs":[],"source":["def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n","    \"\"\" Evaluate the model on a single sentence.\n","    \n","    Parameters:\n","        - Encoder object\n","        - Decoder object\n","        - Input sentence to be translated\n","        - Lang object for the input language\n","        - Lang object for the output language\n","    \n","    Details:\n","        - Disable gradient computation to save memory\n","        - Convert the input sentence to a tensor\n","        - Feed the input tensor through the Encoder to obtain the Encoder outputs and hidden state\n","        - Feed the Encoder outputs and hidden state through the Decoder to obtain the Decoder outputs and attention weights\n","        - Get the index of the highest-scoring output word for each position in the output sequence\n","        - Remove the extra dimension from the tensor\n","        - Stop decoding when the end-of-sequence token is encountered\n","        - Convert the index to a word and append it to the list of decoded words\n","        - Return the list of decoded words and the attention weights\n","    \n","    Returns:\n","        Decoded words and the attention weights\n","    \"\"\"\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n","\n","        _, topi = decoder_outputs.topk(1)\n","        decoded_ids = topi.squeeze()\n","\n","        decoded_words = []\n","        for idx in decoded_ids:\n","            if idx.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            decoded_words.append(output_lang.index2word[idx.item()])\n","\n","    return decoded_words, decoder_attn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnuL0lWJpW-I"},"outputs":[],"source":["def evaluateRandomly(encoder, decoder, n=10):\n","    \"\"\" Evaluate model on a random selection of sentences from the training set.\n","    \n","    Parameters:\n","        - Encoder object.\n","        - Decoder object.\n","        - Number of sentences to evaluate. Default is 10.\n","    \"\"\"\n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        print('>', pair[0])\n","        print('=', pair[1])\n","        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n","        output_sentence = ' '.join(output_words)\n","        print('<', output_sentence)\n","        print('')"]},{"cell_type":"markdown","metadata":{},"source":["<h2 style=\"color:#BF66F2 \"> Main #1 </h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1732930,"status":"ok","timestamp":1689602036768,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"GF5YFC4kpXBv","outputId":"e3ed8cd8-1e03-45e0-d15c-a52e96bac131"},"outputs":[{"name":"stdout","output_type":"stream","text":["Read 331799 sentence pairs...\n","Trimmed to 32084 sentence pairs\n","Counting words...\n","Num of words:\n","ita 5400\n","eng 3168\n","1m 46s (- 26m 35s) (5 6%) 0.8322\n","3m 33s (- 24m 54s) (10 12%) 0.1661\n","5m 20s (- 23m 9s) (15 18%) 0.0822\n","7m 6s (- 21m 20s) (20 25%) 0.0574\n","8m 53s (- 19m 34s) (25 31%) 0.0465\n","10m 41s (- 17m 49s) (30 37%) 0.0409\n","12m 28s (- 16m 2s) (35 43%) 0.0371\n","14m 16s (- 14m 16s) (40 50%) 0.0349\n","16m 4s (- 12m 30s) (45 56%) 0.0332\n","17m 51s (- 10m 43s) (50 62%) 0.0320\n","19m 39s (- 8m 56s) (55 68%) 0.0310\n","21m 27s (- 7m 9s) (60 75%) 0.0300\n","23m 16s (- 5m 22s) (65 81%) 0.0292\n","25m 5s (- 3m 35s) (70 87%) 0.0292\n","26m 53s (- 1m 47s) (75 93%) 0.0286\n","28m 41s (- 0m 0s) (80 100%) 0.0281\n"]}],"source":["hidden_size = 128\n","batch_size = 32\n","\n","input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n","\n","encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n","\n","train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1689603503960,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"7AOOhEId95s6","outputId":"7df72049-a0a5-46a0-f69c-92c43f2fe724"},"outputs":[{"name":"stdout","output_type":"stream","text":["> siamo matricole\n","= we re freshmen\n","< we re freshmen <EOS>\n","\n","> non sono una celebrita\n","= i m not a celebrity\n","< i m not a celebrity <EOS>\n","\n","> io ho un po paura\n","= i m a little scared\n","< i m a little scared <EOS>\n","\n","> non sono interessato a farlo adesso\n","= i m not interested in doing that now\n","< i m not interested in doing that now <EOS>\n","\n","> io sono ancora interessata a farlo\n","= i m still interested in doing that\n","< i m still interested in doing that <EOS>\n","\n","> lui sta molto bene oggi\n","= he s doing very well today\n","< he s doing very well today <EOS>\n","\n","> sono felice di sentirlo\n","= i m happy to hear it\n","< i m happy to hear you <EOS>\n","\n","> voi siete manipolatori\n","= you re manipulative\n","< you re manipulative <EOS>\n","\n","> tu sei oberata di lavoro\n","= you are overworked\n","< you are overworked <EOS>\n","\n","> sei tutto quel che ho\n","= you re all i ve got\n","< you re all i ve got <EOS>\n","\n"]}],"source":["\"\"\" Set dropout layers to eval mode \"\"\"\n","encoder.eval()\n","decoder.eval()\n","evaluateRandomly(encoder, decoder)"]},{"cell_type":"markdown","metadata":{},"source":["<h3 style=\"color:#BF66F2 \"> => Visualizing Attention </h3>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcE2Kth6N9OY"},"outputs":[],"source":["def showAttention(input_sentence, output_words, attentions):\n","    \"\"\" Plot the attention weights for a single input-output pair\n","    \n","    Parameters:\n","        - Input sentence [str]\n","        - Decoded output words [list]\n","        - Attention weights for the input-output pair [Tensor]\n","    \n","    Details:\n","        - Create a new figure\n","        - Add a new subplot to the figure\\\\\n","        (the argument 111 means that the figure has only one row, one column,\\\\\n","        and the current subplot is the first (and only) subplot)\n","        - Display the attention weights as a matrix\n","        - Add a color bar to the figure\n","\n","        - Set up the x-axis and y-axis labels\n","            - Comment this:\n","                - ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n","                - ax.set_yticklabels([''] + output_words)\n","            To avoid the warning message:\\\\\n","            UserWarning: FixedFormatter should only be used together with FixedLocator ax.set_yticklabels([''] + output_words)\\\\\n","            Is a result of using set_xticklabels with a list of tick labels that does not match the number of ticks on the x-axis\n","\n","        - Split the input sentence into words and create the input_words list\n","        - Set up the x-axis tick locations and labels\n","            - Set the tick locator and formatter\n","        - Set the tick label rotation\n","\n","        - Set up the y-axis tick locations and labels\n","        - Set the tick locator and formatter for the y-ax\n","        - Show a label at every tick\n","    \"\"\"\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111) #equivalent to #ax = add_subplot(1, 1, 1)\n","    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n","    fig.colorbar(cax)\n","\n","    # Split\n","    input_words = [''] + input_sentence.split() + ['<EOS>']\n","    # Set up\n","    x_ticks = range(len(input_words))\n","    x_tick_labels = input_words\n","    # Locator and formatter\n","    ax.xaxis.set_major_locator(FixedLocator(x_ticks))\n","    ax.xaxis.set_major_formatter(FixedFormatter(x_tick_labels))\n","    # Tick label rotation\n","    ax.tick_params(axis='x', labelrotation=90)\n","\n","    # Set up\n","    y_ticks = range(len(output_words))\n","    y_tick_labels = output_words\n","    # Tick locator and formatter\n","    ax.yaxis.set_major_locator(FixedLocator(y_ticks))\n","    ax.yaxis.set_major_formatter(FixedFormatter(y_tick_labels))\n","\n","    #ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n","    #ax.set_yticklabels([''] + output_words)\n","\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","    plt.show()\n","\n","def evaluateAndShowAttention(input_sentence):\n","    \"\"\" Evaluate the neural machine translation model on a single input sentence, get the output words and plot the attention weights.\n","    \n","    Parameters:\n","        Input sentence [str]\n","    \"\"\"\n","    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n","    print('input =', input_sentence)\n","    print('output =', ' '.join(output_words))\n","    # Plot\n","    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2 style=\"color:#BF66F2 \"> Main #2 </h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1689606642069,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"0vK4dScqQZBL","outputId":"7e698c6c-445e-4d2a-9c7f-4d37a37b0c30"},"outputs":[{"name":"stdout","output_type":"stream","text":["input = io sono famelico\n","output = i m famished <EOS>\n","\n","input = sono grata\n","output = i m thankful for that <EOS>\n","input = sei tutto quel che ho\n","output = you re all i ve got <EOS>\n"]}],"source":["#evaluateAndShowAttention('io sono coraggioso')\n","evaluateAndShowAttention('io sono famelico')\n","print()\n","evaluateAndShowAttention('sono grata');\n","evaluateAndShowAttention('sei tutto quel che ho')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMd7Y825FlwWCh720PHvw/K","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
