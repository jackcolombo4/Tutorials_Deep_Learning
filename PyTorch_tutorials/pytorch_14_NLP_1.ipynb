{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:1.2;\">\n",
    "\n",
    "<h1 style=\"color:#BF66F2; margin-bottom: 0.3em;\"> Natural Language Processing in PyTorch 1 </h1>\n",
    "\n",
    "<h4 style=\"margin-top: 0.3em; margin-bottom: 1em;\"> Simple Sentimental analysis. </h4>\n",
    "\n",
    "<div style=\"line-height:1.4; margin-bottom: 0.5em;\">\n",
    "    <h3 style=\"color: lightblue; display: inline; margin-right: 0.5em;\">Keywords:</h3> Counter + maketrans() + nn.Embedding() + 'UNK' token + with warnings.catch_warnings() + squeeze()\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"The Marvel cinematic universe is incredibly well-built\", 1),\n",
    "    (\"Spiderman's character arc in the movies is fascinating\", 1),\n",
    "    (\"The Avengers series brings together amazing superheroes\", 1),\n",
    "    (\"Iron Man's suit technology is intriguing and fun to watch\", 1),\n",
    "    (\"Batman's dark demeanor and backstory are so engaging\", 1),\n",
    "    (\"The Joker in The Dark Knight was portrayed phenomenally\", 1),\n",
    "    (\"Wonder Woman's character is a true symbol of empowerment\", 1),\n",
    "    (\"Black Panther represents both strength and leadership\", 1),\n",
    "    (\"The X-men series did a great job handling multiple characters\", 1),\n",
    "    (\"The Flash's time travel episodes are always interesting\", 1),\n",
    "    (\"Thor’s transformation in the Marvel movies is really admirable\", 1),\n",
    "    (\"The concept of multiverse in Dr. Strange is mind-blowing\", 1),\n",
    "    (\"Ant-Man brought a fun and lighter tone to the Marvel universe\", 1),\n",
    "    (\"The teamwork of the Justice League is wonderful\", 1),\n",
    "    (\"Captain America’s dedication towards his duty is inspirational\", 1),\n",
    "    (\"The action scenes in Aquaman were spectacular\", 1),\n",
    "    (\"Guardians of the Galaxy has a perfect blend of humor and action\", 1),\n",
    "    (\"The alliance of superheroes in Avengers: Endgame was epic\", 1),\n",
    "    (\"Superman's moral compass is something to admire\", 1),\n",
    "    (\"Venom brought an interesting perspective to a villain’s story\", 1),\n",
    "    (\"I love how Marvel subtly connects all its movies and characters\", 1),\n",
    "    (\"The emotional depth in Logan was surprisingly touching\", 1),\n",
    "    (\"The rivalry between Professor X and Magneto is classic\", 1),\n",
    "    (\"The animation and storyline in Spiderman: Into the Spider-Verse were amazing\", 1),\n",
    "    (\"The heroic acts of the superheroes are truly motivational\", 1),\n",
    "    \n",
    "    (\"I did not enjoy the plot twists in the recent Marvel movie\", 0),\n",
    "    (\"The character development in DC movies is often lacking\", 0),\n",
    "    (\"The latest Batman movie did not live up to the hype\", 0),\n",
    "    (\"I am not a fan of how the Flash’s storyline is progressing\", 0),\n",
    "    (\"The reboot of the Spiderman series was totally unnecessary\", 0),\n",
    "    (\"The final battle in the Avengers felt rushed and chaotic\", 0),\n",
    "    (\"I think Superman's character is too overpowered and unrelatable\", 0),\n",
    "    (\"The villains in the DC universe are often underdeveloped\", 0),\n",
    "    (\"I find the humor in Guardians of the Galaxy forced and dry\", 0),\n",
    "    (\"Marvel movies often compromise story depth for visual effects\", 0),\n",
    "    (\"The dark tone of DC movies does not appeal to me\", 0),\n",
    "    (\"I think the Marvel cinematic universe is too commercialized\", 0),\n",
    "    (\"The Justice League movie did not do justice to the characters\", 0),\n",
    "    (\"Wonder Woman’s sequel was not as good as the first movie\", 0),\n",
    "    (\"I get bored with the repetitive story arcs in superhero movies\", 0),\n",
    "    (\"The character of Hulk is not explored well in the movies\", 0),\n",
    "    (\"Batman vs Superman was a letdown considering the expectations\", 0),\n",
    "    (\"The plot of the latest DC movie was too predictable\", 0),\n",
    "    (\"I feel like Thor’s character is often used just for comic relief\", 0),\n",
    "    (\"The last Avengers movie was way too long and tiring\", 0),\n",
    "    (\"I did not like the portrayal of Lex Luthor in Batman vs Superman\", 0),\n",
    "    (\"The continuity errors in the Marvel universe are too evident\", 0),\n",
    "    (\"The death and revival of characters in comics is too repetitive\", 0),\n",
    "    (\"The twist with Mandarin in Iron Man 3 was disappointing\", 0),\n",
    "    (\"DC should stick to animated series instead of movies\", 0),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text.split()\n",
    "\n",
    "word_counts = Counter(tokenize(\" \".join([t[0] for t in train_data])))\n",
    "vocab = set(word_counts.keys())\n",
    "vocab_to_int = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}\n",
    "int_to_vocab = {idx: word for word, idx in vocab_to_int.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Add the '<UNK>' token to your vocab_to_int to handle words of the validation set that are not present in the training set.\n",
    "\"\"\"\n",
    "print('<UNK>' in vocab_to_int)\n",
    "vocab_to_int['<UNK>'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, vocab_to_int, max_len):\n",
    "        \"\"\" Initialize the TextDataset instance.\n",
    "\n",
    "        Args:\n",
    "            - Input data [list of tuples]\n",
    "            - Dictionary that converts words to integers [dict]\n",
    "            - Maximum length to which sequences should be padded or truncated [int]\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.vocab_to_int = vocab_to_int\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Return the size of the data. \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Return the tokenized and numericalized text and label for the given index.\n",
    "\n",
    "        Parameters:\n",
    "            Index for which data is to be returned [int]\n",
    "\n",
    "        Details: \n",
    "            Add the check of words that are not in \"vocab_to_int\" dictionary.\\\\\n",
    "            One common approach is to use a special <UNK> (unknown) token to represent all unknown words. \n",
    "\n",
    "        Returns:\n",
    "            Numericalized text and label [torch.Tensor, torch.Tensor]\n",
    "        \"\"\"\n",
    "        text, label = self.data[idx]\n",
    "        tokenized = tokenize(text)\n",
    "        #numericalized = [self.vocab_to_int[word] for word in tokenized]\n",
    "        numericalized = [self.vocab_to_int.get(word, self.vocab_to_int['<UNK>']) for word in tokenized]\n",
    "        # Add padding\n",
    "        pad_len = self.max_len - len(numericalized)\n",
    "        numericalized += [0] * pad_len  #with 0 is the index for the PAD token\n",
    "        return torch.tensor(numericalized), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_data, vocab_to_int, 50)\n",
    "val_dataset = TextDataset(val_data, vocab_to_int, 50)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n"
     ]
    }
   ],
   "source": [
    "for i, (text, label) in enumerate(train_loader): \n",
    "    print(text.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentModel(\n",
       "  (embedding): Embedding(199, 10)\n",
       "  (rnn): RNN(10, 20, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "model = SentimentModel(len(vocab), 10, 20, 1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6488404273986816\n",
      "Epoch: 2, Loss: 0.6950567960739136\n",
      "Epoch: 3, Loss: 0.6938516497612\n",
      "Epoch: 4, Loss: 0.6952284574508667\n",
      "Epoch: 5, Loss: 0.7652314305305481\n",
      "Epoch: 6, Loss: 0.6931604146957397\n",
      "Epoch: 7, Loss: 0.6941854953765869\n",
      "Epoch: 8, Loss: 0.6795384883880615\n",
      "Epoch: 9, Loss: 0.6945673227310181\n",
      "Epoch: 10, Loss: 0.6962963938713074\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "# Training\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for texts, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts).squeeze()\n",
    "            loss = criterion(outputs.float(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 40.00%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Validation \"\"\"\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in val_loader:\n",
    "        outputs = model(texts).squeeze()\n",
    "        predicted = torch.round(outputs)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        \n",
    "accuracy = total_correct / total_count\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
