{"cells":[{"cell_type":"markdown","metadata":{},"source":["<div style=\"line-height:1.2;\">\n","\n","<h1 style=\"color:#BF66F2; margin-bottom: 0.3em;\"> Embeddings in PyTorch </h1>\n","\n","<h4 style=\"margin-top: 0.3em; margin-bottom: 1em;\"> Text Classification with a Feedforward Network, based on AG_NEWS dataset. </h4>\n","\n","<div style=\"line-height:1.4; margin-bottom: 0.5em;\">\n","    <h3 style=\"color: lightblue; display: inline; margin-right: 0.5em;\">Keywords:</h3> \n","Torchtext + yield + to_map_style_dataset + nn.EmbeddingBag + torch.optim.lr_scheduler.StepLR\n","</div>\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"line-height:0.5\">\n","<h1 style=\"color:#BF66F2 \"> Embeddings in PyTorch </h1>\n","<h4> Text Classification with a Feedforward Network.  </h4>\n","</div>\n","<div style=\"margin-top: -18px;\">\n","<span style=\"display: inline-block;\">\n","    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3>\n","    Torchtext + yield + to_map_style_dataset + nn.EmbeddingBag + torch.optim.lr_scheduler.StepLR\n","</span>\n","</div>"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8876,"status":"ok","timestamp":1689618989207,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"ZvvSi2TRyka_","outputId":"25a40610-3401-4a53-9d9d-3691aa8df759"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.7.0)\n"]}],"source":["#!pip install -U portalocker>=2.0.0\n","!pip install portalocker"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1689618989209,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"3U7h4OgQ-Kb_","outputId":"3055fc13-cf95-4d79-ce88-bab555331f1a"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1689618989213,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"wXQDVEgVws1a"},"outputs":[],"source":["import os\n","import time\n","import torch\n","from torchtext.datasets import AG_NEWS\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","import torch\n","from torch import nn\n","from torchtext.datasets import AG_NEWS\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1689618989215,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"SOTMP6npymFi","outputId":"0692b00f-7518-466b-867c-ccbdb4c77b99"},"outputs":[{"data":{"text/plain":["<generator object ShardingFilterIterDataPipe.__iter__ at 0x7e5e0781f530>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["train_iter = iter(AG_NEWS(split=\"train\"))\n","train_iter"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1689618989217,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"sDB-EjbFwtGW"},"outputs":[],"source":["tokenizer = get_tokenizer(\"basic_english\")\n","# Load the training split of the AG_NEWS dataset\n","train_iter = AG_NEWS(split=\"train\")\n","\n","def yield_tokens(data_iter):\n","    \"\"\" Apply the tokenizer function to each text.\\\\\n","    The batches contain (label, text) tuples, ignoring the label and just extracting the text.\\\\\n","    Yield returns a generator, so this function will lazily tokenize each text and return token lists.\n","    \"\"\"\n","    for _, text in data_iter:\n","        yield tokenizer(text)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":13399,"status":"ok","timestamp":1689619002583,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"Q1dLQW_UwtJp"},"outputs":[],"source":["vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":536,"status":"ok","timestamp":1689619003043,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"4qbUOY-1wtMT","outputId":"61c08e0e-83cc-41cf-cbc7-f9cacca761d2"},"outputs":[{"data":{"text/plain":["[475, 21, 30, 5297]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\" The vocabulary block converts a list of tokens into integers. \"\"\"\n","vocab(['here', 'is', 'an', 'example'])"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1689619003044,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"hGUZdpJ52Nxf"},"outputs":[],"source":["text_pipeline = lambda x: vocab(tokenizer(x))\n","label_pipeline = lambda x: int(x) - 1"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1689619003045,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"_xyBzwd896lz","outputId":"81392d7b-0eac-4580-f4db-765697c6a5d7"},"outputs":[{"data":{"text/plain":["[475, 21, 2, 30, 5297]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["text_pipeline('here is the an example')"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1689619003046,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"X4mzPMEA9730","outputId":"6c3847a5-17a8-4f47-ef57-0fe09c972099"},"outputs":[{"data":{"text/plain":["9"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["label_pipeline('10')"]},{"cell_type":"markdown","metadata":{"id":"LFu9NxQ198o_"},"source":["<h2 style=\"color:#BF66F2 \"> Generate data batch and iterator </h2>"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1689619003047,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"4sFlHpnG9_rR"},"outputs":[],"source":["def collate_batch(batch):\n","    \"\"\" Combine a batch of examples into a single tensor, that can be fed into a neural network\n","    \n","    Parameters:\n","        List of examples, where each example is a tuple containing a label and a text.\n","    \n","    Details:\n","        - Initialize empty lists for the label, text, and offset tensors\n","        - Loop:\n","            - Process the label using the label_pipeline function and append it to the label list\n","            - Process the text using the text_pipeline function and convert the resulting list of tokens to a tensor\n","            - Append the processed text tensor to the text list\n","            - Append the size of the processed text tensor to the offsets list\n","        - Convert the label and offset lists to tensors\n","\n","        - Concatenate the text tensors into a single tensor\n","\n","        - Move the label, text, and offset tensors to the device specified by the `device` variable\n","    \n","    Returns:\n","        Tensors representing the batch, containing the processed labels, processed texts, and offsets\n","    \"\"\"\n","    label_list, text_list, offsets = [], [], [0]\n","\n","    for _label, _text in batch:\n","        label_list.append(label_pipeline(_label))\n","        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","        text_list.append(processed_text)\n","        offsets.append(processed_text.size(0))\n","\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","\n","    text_list = torch.cat(text_list)\n","\n","    return label_list.to(device), text_list.to(device), offsets.to(device)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1689619003048,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"4Eni2rjt-ANu"},"outputs":[],"source":["train_iter = AG_NEWS(split=\"train\")\n","dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"]},{"cell_type":"markdown","metadata":{"id":"RgnmgXe4-ARH"},"source":["<div style=\"line-height:0.5\">\n","<h2 style=\"color:#BF66F2 \"> Define the model </h2>\n","</div>\n","Composed of the nn.EmbeddingBag layer (the default mode of “mean”) + a FFN layer for the classification purpose. <br>\n","Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.<br>\n","Additionally, since nn.EmbeddingBag accumulates the average across the embeddings on the fly, nn.EmbeddingBag can enhance the performance <br> and memory efficiency to process a sequence of tensors.    "]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1689619003049,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"H8eXdDrs-AUI"},"outputs":[],"source":["class TextClassificationModel(nn.Module):\n","    \"\"\" Network model for text classification.\n","    \n","    Attributes:\n","        - Embedding layer that maps input tokens to dense vectors [nn.EmbeddingBag]\n","        - Linear fully-connected layer that maps the embeddings to class scores [nn.Linear]\n","    \n","    Args:\n","        - vocab_size: The size of the vocabulary [int]\n","        - embed_dim: The dimensionality of the embedding vectors [int]\n","        - num_class: The number of classes [int]\n","    \"\"\"\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super(TextClassificationModel, self).__init__()\n","\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        \"\"\" Initialize the weights of the embedding and linear layers. \"\"\"\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","        \"\"\"Compute the forward pass of the model.\n","        \n","        Parameters:\n","            - Input text [torch.Tensors]\n","            - Tensor of offsets that indicate the starting index of each example in the input text.\n","        \n","        Details:\n","            - Embed the input text using the embedding layer.\n","            - Map the embeddings to class scores using the linear layer.\n","        \n","        Returns:\n","            Tensor of class scores.\n","        \"\"\"\n","        embedded = self.embedding(text, offsets)\n","        return self.fc(embedded)"]},{"cell_type":"markdown","metadata":{"id":"9xbBs9np-AW3"},"source":["<h2 style=\"color:#BF66F2 \"> Initiate an instance </h2>"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":2174,"status":"ok","timestamp":1689619005213,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"HJSz6pta-AZr"},"outputs":[],"source":["train_iter = AG_NEWS(split=\"train\")\n","num_class = len(set([label for (label, text) in train_iter]))\n","vocab_size = len(vocab)\n","emsize = 64\n","model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1689619005215,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"QWcQkTLr-Afn"},"outputs":[],"source":["def train(dataloader):\n","    \"\"\" Train the TextClassificationModel on a dataset.\n","    \n","    Parameters:\n","        DataLoader object that provides batches of training data\n","\n","    Details:\n","        - Set the model to training mode\n","\n","        - Initialize variables for tracking accuracy and time\n","\n","        - Loop: Iterate over each batch in the dataloader\n","        - Zero the gradients\n","        - Compute the predicted labels for the batch\n","        - Compute the loss between the predicted and true labels\n","        - Backpropagate the loss and update the model parameters\n","        - Update the accuracy and count variables\n","        - Print the accuracy and time elapsed every `log_interval` batches\n","    \n","    \"\"\"\n","    model.train()\n","\n","    total_acc, total_count = 0, 0\n","    log_interval = 500\n","    start_time = time.time()\n","\n","    for idx, (label, text, offsets) in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        predicted_label = model(text, offsets)\n","        loss = criterion(predicted_label, label)\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","        optimizer.step()\n","\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print(\"| epoch {:3d} | {:5d}/{:5d} batches \"\"| accuracy {:8.3f}\".format(epoch, idx, len(dataloader), total_acc / total_count))\n","            # Reset the accuracy and count variables, and update the start time\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","\n","def evaluate(dataloader):\n","    \"\"\" Evaluate the TextClassificationModel on a dataset.\n","    \n","    Parameters:\n","        DataLoader object that provides batches of evaluation data\n","    \n","    Details:\n","        - Set the model to evaluation mode\n","        - Initialize variables for tracking accuracy\n","        - Loop: Iterate over each batch in the dataloader\n","            - Compute the predicted labels for the batch\n","            - Compute the loss between the predicted and true labels\n","            - Update the accuracy and count variables\n","    \n","    Returns:\n","        Accuracy of the model on the evaluation data [float]\n","    \"\"\"\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text, offsets) in enumerate(dataloader):\n","            predicted_label = model(text, offsets)\n","            loss = criterion(predicted_label, label)\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","\n","    return total_acc / total_count"]},{"cell_type":"markdown","metadata":{"id":"qb14vpv6-AlL"},"source":["<div style=\"line-height:0.5\">\n","<h2 style=\"color:#BF66F2 \"> Main: Split the dataset and run the model </h2>\n","</div>\n","CrossEntropyLoss criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class, when training a classification problem with C classes.<br>\n","SGD implements stochastic gradient descent method as the optimizer."]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1689619005216,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"YllcqfMUFwfj"},"outputs":[],"source":["\"\"\" Hyperparameters \"\"\"\n","EPOCHS = 10  # epoch\n","LR = 5  # learning rate\n","BATCH_SIZE = 64  # batch size for training"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":384151,"status":"ok","timestamp":1689619389350,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"n231u14H-AqH","outputId":"ee30f12a-5b56-455f-e23a-19baac1b4f06"},"outputs":[{"name":"stdout","output_type":"stream","text":["| epoch   1 |   500/ 1782 batches | accuracy    0.682\n","| epoch   1 |  1000/ 1782 batches | accuracy    0.853\n","| epoch   1 |  1500/ 1782 batches | accuracy    0.877\n","-----------------------------------------------------------\n","| end of epoch   1 | time: 41.63s | valid accuracy    0.888 \n","-----------------------------------------------------------\n","| epoch   2 |   500/ 1782 batches | accuracy    0.895\n","| epoch   2 |  1000/ 1782 batches | accuracy    0.902\n","| epoch   2 |  1500/ 1782 batches | accuracy    0.901\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 36.89s | valid accuracy    0.892 \n","-----------------------------------------------------------\n","| epoch   3 |   500/ 1782 batches | accuracy    0.915\n","| epoch   3 |  1000/ 1782 batches | accuracy    0.915\n","| epoch   3 |  1500/ 1782 batches | accuracy    0.911\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 37.82s | valid accuracy    0.902 \n","-----------------------------------------------------------\n","| epoch   4 |   500/ 1782 batches | accuracy    0.922\n","| epoch   4 |  1000/ 1782 batches | accuracy    0.924\n","| epoch   4 |  1500/ 1782 batches | accuracy    0.921\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 37.53s | valid accuracy    0.892 \n","-----------------------------------------------------------\n","| epoch   5 |   500/ 1782 batches | accuracy    0.935\n","| epoch   5 |  1000/ 1782 batches | accuracy    0.939\n","| epoch   5 |  1500/ 1782 batches | accuracy    0.936\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 37.45s | valid accuracy    0.908 \n","-----------------------------------------------------------\n","| epoch   6 |   500/ 1782 batches | accuracy    0.938\n","| epoch   6 |  1000/ 1782 batches | accuracy    0.938\n","| epoch   6 |  1500/ 1782 batches | accuracy    0.939\n","-----------------------------------------------------------\n","| end of epoch   6 | time: 39.52s | valid accuracy    0.907 \n","-----------------------------------------------------------\n","| epoch   7 |   500/ 1782 batches | accuracy    0.941\n","| epoch   7 |  1000/ 1782 batches | accuracy    0.940\n","| epoch   7 |  1500/ 1782 batches | accuracy    0.940\n","-----------------------------------------------------------\n","| end of epoch   7 | time: 38.01s | valid accuracy    0.909 \n","-----------------------------------------------------------\n","| epoch   8 |   500/ 1782 batches | accuracy    0.942\n","| epoch   8 |  1000/ 1782 batches | accuracy    0.942\n","| epoch   8 |  1500/ 1782 batches | accuracy    0.939\n","-----------------------------------------------------------\n","| end of epoch   8 | time: 37.84s | valid accuracy    0.907 \n","-----------------------------------------------------------\n","| epoch   9 |   500/ 1782 batches | accuracy    0.940\n","| epoch   9 |  1000/ 1782 batches | accuracy    0.940\n","| epoch   9 |  1500/ 1782 batches | accuracy    0.942\n","-----------------------------------------------------------\n","| end of epoch   9 | time: 37.62s | valid accuracy    0.908 \n","-----------------------------------------------------------\n","| epoch  10 |   500/ 1782 batches | accuracy    0.940\n","| epoch  10 |  1000/ 1782 batches | accuracy    0.943\n","| epoch  10 |  1500/ 1782 batches | accuracy    0.940\n","-----------------------------------------------------------\n","| end of epoch  10 | time: 37.82s | valid accuracy    0.908 \n","-----------------------------------------------------------\n"]}],"source":["### Loss function + optimizer + learning rate scheduler\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","\n","##### Training and testing sets created from the AG_NEWS dataset\n","total_accu = None\n","train_iter, test_iter = AG_NEWS()\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","num_train = int(len(train_dataset) * 0.95)\n","\n","# Split 2 => Training set into a training subset and a validation subset.\n","split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n","\n","### DataLoader objects for the training, validation, and testing sets.\n","train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n","valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n","\n","#################### Train the model\n","for epoch in range(1, EPOCHS + 1):  #for the specified number of epochs.\n","    # Record the start time of the epoch.\n","    epoch_start_time = time.time()\n","    # Train the model on the training subset.\n","    train(train_dataloader)\n","    # Evaluate the model on the validation subset and record the accuracy.\n","    accu_val = evaluate(valid_dataloader)\n","    # Adjust the learning rate using the scheduler.\n","    if total_accu is not None and total_accu > accu_val:\n","        scheduler.step()\n","    else:\n","        total_accu = accu_val\n","\n","    print(\"-\" * 59)\n","    print(\"| end of epoch {:3d} | time: {:5.2f}s | \" \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val))\n","    print(\"-\" * 59)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1689619389762,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"UCRifAdJ-AtF","outputId":"b84c2e3a-9617-45c1-a0f5-68c72f6db0ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Checking the results of test dataset.\n","test accuracy    0.907\n"]}],"source":["print(\"Checking the results ...\")\n","accu_test = evaluate(test_dataloader)\n","print(\"test accuracy is: {:8.3f}\".format(accu_test))"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1689619389763,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"TYahgYpI-AwC","outputId":"31f3923c-3d11-4bdb-ff11-7332dec7139d"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is a Sci/Tec news\n"]}],"source":["ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n","\n","def predict(text, text_pipeline):\n","    with torch.no_grad():\n","        text = torch.tensor(text_pipeline(text))\n","        output = model(text, torch.tensor([0]))\n","        return output.argmax(1).item() + 1\n","\n","\n","ex_text_str = \"\"\"Apple has unveiled a new iPhone that has a faster processor, improved cameras, and longer battery life.\n","The iPhone 13, which comes in four different models,\n","also features a new cinematic mode that allows users to record professional-looking video with shallow depth of field.\n","The new models start at $699 and are available for pre-order now.\"\"\"\n","\n","model = model.to(\"cpu\")\n","\n","print(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)])"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1689619578558,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"mIiK4323-Ayp","outputId":"194d3d8c-5277-4c1d-de85-a838ed6e1bf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is a Sci/Tec news\n"]}],"source":["ex_text_str2 = \"Scientists have discovered a new species of dinosaur that lived over 90 million years ago. \\\n","The dinosaur, named Hesperornithoides miessleri, was a small, bird-like predator that likely fed on insects and small animals. \\\n","Its fossils were found in Montana, USA, and provide new insights into the evolution of dinosaurs during the Late Cretaceous period.\"\n","\n","print(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)])"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":272,"status":"ok","timestamp":1689620015613,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"qYHW2AS--A0x","outputId":"1d3c6a64-4b93-4768-d9e8-dcc4ccf01e76"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is a Sports news!\n"]}],"source":["ex_text_str3 = \"Ex-Inter Milan striker Diego Milito had a hand in the club's latest signing, as Lautaro Martinez completed a €22 million \\\n","move from Racing Club. The Argentine forward, who scored 13 goals in 21 appearances for Racing Club last season, has signed a five-year contract with Inter Milan. \\\n","Martinez has been compared to Milito, who played for Inter Milan from 2009 to 2014 and helped the club win the treble in his first season. \\\n","Martinez will later on became the captain in 2022.\"\n","\n","print(\"This is a %s news!\" % ag_news_label[predict(ex_text_str3, text_pipeline)])"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMD5NS5yysL9GGVOmDW8pXE","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
