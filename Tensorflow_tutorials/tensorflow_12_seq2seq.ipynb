{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMezAE6PBH7T"
      },
      "source": [
        "<div style=\"line-height:0.5\">\n",
        "<h1 style=\"color:#FF7C00  \">  Seq2Seq in Tensorflow </h1>\n",
        "<h4>  </h4>\n",
        "<h3 style=\"color:lightblue\"> Keywords: </h3>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "opbBBULkBH7i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "zsSSRAzXBH7l"
      },
      "outputs": [],
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "X5q3TSIEBbul"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ZW4BsdReBjjh",
        "outputId": "c7b93119-9172-4f1f-a681-697e130ce6ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-42fdb67f-0117-44b5-a420-cc43a849ab13\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-42fdb67f-0117-44b5-a420-cc43a849ab13\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving eng-ita.txt to eng-ita.txt\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvsiSiIBBH7n"
      },
      "source": [
        "<h2 style=\"color:#FF7C00  \">  <u> Example 1 </u> </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "iVG50fOcBH7o"
      },
      "outputs": [],
      "source": [
        "english_sentences_simple = [\n",
        "    \"hello\", \"goodbye\", \"thank you\", \"please\", \"yes\", \"no\", \"I love you\",\n",
        "    \"how are you\", \"good morning\", \"good night\", \"water\", \"food\", \"sun\",\n",
        "    \"moon\", \"star\", \"book\", \"computer\", \"phone\", \"flower\", \"tree\",\n",
        "    \"house\", \"car\", \"bus\", \"train\", \"sky\", \"cloud\", \"rain\", \"snow\",\n",
        "    \"bird\", \"cat\", \"dog\", \"fish\", \"mountain\", \"valley\", \"ocean\", \"sea\",\n",
        "    \"river\", \"forest\", \"desert\", \"city\", \"village\", \"country\", \"king\",\n",
        "    \"queen\", \"prince\", \"princess\", \"happy\", \"sad\", \"angry\", \"excited\",\n",
        "    \"bored\", \"tired\", \"hungry\", \"thirsty\", \"hot\", \"cold\", \"big\", \"small\",\n",
        "    \"fast\", \"slow\", \"up\", \"down\", \"left\", \"right\", \"day\", \"night\", \"light\",\n",
        "    \"dark\", \"young\", \"old\", \"man\", \"woman\", \"boy\", \"girl\", \"friend\", \"enemy\",\n",
        "    \"song\", \"dance\", \"jump\", \"run\", \"walk\", \"stop\", \"go\", \"come\", \"push\",\n",
        "    \"pull\", \"open\", \"close\", \"hard\", \"soft\", \"short\", \"tall\", \"wide\", \"narrow\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "aCNEkYwjBH7r"
      },
      "outputs": [],
      "source": [
        "italian_sentences_simple = [\n",
        "    \"ciao\", \"addio\", \"grazie\", \"per favore\", \"sì\", \"no\", \"ti amo\",\n",
        "    \"come stai\", \"buongiorno\", \"buonanotte\", \"acqua\", \"cibo\", \"sole\",\n",
        "    \"luna\", \"stella\", \"libro\", \"computer\", \"telefono\", \"fiore\", \"albero\",\n",
        "    \"casa\", \"auto\", \"autobus\", \"treno\", \"cielo\", \"nuvola\", \"pioggia\", \"neve\",\n",
        "    \"uccello\", \"gatto\", \"cane\", \"pesce\", \"montagna\", \"valle\", \"oceano\", \"mare\",\n",
        "    \"fiume\", \"foresta\", \"deserto\", \"città\", \"villaggio\", \"paese\", \"re\",\n",
        "    \"regina\", \"principe\", \"principessa\", \"felice\", \"triste\", \"arrabbiato\", \"eccitato\",\n",
        "    \"annoito\", \"stanco\", \"affamato\", \"assetato\", \"caldo\", \"freddo\", \"grande\", \"piccolo\",\n",
        "    \"veloce\", \"lento\", \"su\", \"giù\", \"sinistra\", \"destra\", \"giorno\", \"notte\", \"luce\",\n",
        "    \"scuro\", \"giovane\", \"vecchio\", \"uomo\", \"donna\", \"ragazzo\", \"ragazza\", \"amico\", \"nemico\",\n",
        "    \"canzone\", \"danza\", \"salta\", \"corri\", \"cammina\", \"ferma\", \"vai\", \"vieni\", \"spingi\",\n",
        "    \"tira\", \"apri\", \"chiudi\", \"duro\", \"morbido\", \"corto\", \"alto\", \"largo\", \"stretto\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq6z5VihBH7t"
      },
      "source": [
        "<h3 style=\"color:#FF7C00  \">  Preprocessing </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "08tgj4uIBH7u"
      },
      "outputs": [],
      "source": [
        "tokenizer_eng = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer_eng.fit_on_texts(english_sentences_simple)\n",
        "english_seq = tokenizer_eng.texts_to_sequences(english_sentences_simple)\n",
        "\n",
        "tokenizer_ita = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer_ita.fit_on_texts(italian_sentences_simple)\n",
        "ita_seq = tokenizer_ita.texts_to_sequences(italian_sentences_simple)\n",
        "\n",
        "max_len_eng = max([len(seq) for seq in english_seq])\n",
        "max_len_frn = max([len(seq) for seq in ita_seq])\n",
        "\n",
        "english_seq = keras.preprocessing.sequence.pad_sequences(english_seq, maxlen=max_len_eng, padding='post')\n",
        "ita_seq = keras.preprocessing.sequence.pad_sequences(ita_seq, maxlen=max_len_frn, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk9lwJ7iBH7v"
      },
      "source": [
        "<h3 style=\"color:#FF7C00  \">  Seq2Seq Model </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "3WKqIl_8BH7w"
      },
      "outputs": [],
      "source": [
        "#### Parameters\n",
        "embedding_dim = 50\n",
        "lstm_units = 128\n",
        "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
        "vocab_size_ita = len(tokenizer_ita.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "6OpCHxZZBH7x"
      },
      "outputs": [],
      "source": [
        "###### Encoder\n",
        "encoder_input = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(vocab_size_eng, embedding_dim)(encoder_input)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "encoder_output, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "####### Decoder\n",
        "decoder_input = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(vocab_size_ita, embedding_dim)(decoder_input)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size_ita, activation='softmax')\n",
        "decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "model = keras.Model([encoder_input, decoder_input], decoder_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "fqZDRGSwBH70"
      },
      "outputs": [],
      "source": [
        "# Encoder inference model\n",
        "encoder_model = keras.Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embedding_inference = Embedding(vocab_size_ita, embedding_dim)(decoder_input)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_inference, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = keras.Model([decoder_input] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-48mmfTBH71",
        "outputId": "b269b13d-3448-4097-8353-dac800228674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "38/38 [==============================] - 6s 73ms/step - loss: 2.3371 - accuracy: 0.5040 - val_loss: 1.6101 - val_accuracy: 0.5737\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 1.7084 - accuracy: 0.5173 - val_loss: 1.4016 - val_accuracy: 0.5789\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 1.6233 - accuracy: 0.5293 - val_loss: 1.3739 - val_accuracy: 0.5895\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 1s 14ms/step - loss: 1.5630 - accuracy: 0.5387 - val_loss: 1.3266 - val_accuracy: 0.5789\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.5168 - accuracy: 0.5467 - val_loss: 1.3012 - val_accuracy: 0.5947\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 1.4904 - accuracy: 0.5547 - val_loss: 1.2887 - val_accuracy: 0.5947\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.4670 - accuracy: 0.5613 - val_loss: 1.2501 - val_accuracy: 0.6000\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 1.4197 - accuracy: 0.5733 - val_loss: 1.2206 - val_accuracy: 0.6000\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.3804 - accuracy: 0.5787 - val_loss: 1.1902 - val_accuracy: 0.6053\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.3512 - accuracy: 0.5840 - val_loss: 1.1704 - val_accuracy: 0.6263\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.3348 - accuracy: 0.5880 - val_loss: 1.1599 - val_accuracy: 0.6526\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.3235 - accuracy: 0.5880 - val_loss: 1.1460 - val_accuracy: 0.6316\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.2876 - accuracy: 0.5933 - val_loss: 1.1455 - val_accuracy: 0.6421\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 1.2749 - accuracy: 0.5973 - val_loss: 1.1253 - val_accuracy: 0.6474\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.2484 - accuracy: 0.6040 - val_loss: 1.1199 - val_accuracy: 0.6368\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.2380 - accuracy: 0.6067 - val_loss: 1.1474 - val_accuracy: 0.6579\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 1.2147 - accuracy: 0.6053 - val_loss: 1.1222 - val_accuracy: 0.6474\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.1812 - accuracy: 0.6240 - val_loss: 1.1095 - val_accuracy: 0.6526\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.1578 - accuracy: 0.6293 - val_loss: 1.1171 - val_accuracy: 0.6579\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.1347 - accuracy: 0.6267 - val_loss: 1.1291 - val_accuracy: 0.6421\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.1112 - accuracy: 0.6307 - val_loss: 1.1287 - val_accuracy: 0.6579\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.0855 - accuracy: 0.6400 - val_loss: 1.1584 - val_accuracy: 0.6474\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.0561 - accuracy: 0.6533 - val_loss: 1.1686 - val_accuracy: 0.6526\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.0234 - accuracy: 0.6667 - val_loss: 1.1968 - val_accuracy: 0.6263\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 1.0095 - accuracy: 0.6587 - val_loss: 1.2182 - val_accuracy: 0.6368\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.9707 - accuracy: 0.6787 - val_loss: 1.2418 - val_accuracy: 0.6053\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.9292 - accuracy: 0.6907 - val_loss: 1.2221 - val_accuracy: 0.6158\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.8755 - accuracy: 0.7053 - val_loss: 1.2180 - val_accuracy: 0.6368\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.8557 - accuracy: 0.7107 - val_loss: 1.2856 - val_accuracy: 0.6105\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.8065 - accuracy: 0.7360 - val_loss: 1.2800 - val_accuracy: 0.6211\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.7584 - accuracy: 0.7627 - val_loss: 1.2725 - val_accuracy: 0.6158\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.7404 - accuracy: 0.7693 - val_loss: 1.3185 - val_accuracy: 0.6053\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.6707 - accuracy: 0.7920 - val_loss: 1.3293 - val_accuracy: 0.6053\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.6121 - accuracy: 0.8160 - val_loss: 1.3598 - val_accuracy: 0.6053\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.5831 - accuracy: 0.8187 - val_loss: 1.3633 - val_accuracy: 0.6211\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.5442 - accuracy: 0.8427 - val_loss: 1.3846 - val_accuracy: 0.6105\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.5013 - accuracy: 0.8653 - val_loss: 1.3987 - val_accuracy: 0.6000\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.4694 - accuracy: 0.8733 - val_loss: 1.4664 - val_accuracy: 0.6105\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.4446 - accuracy: 0.8853 - val_loss: 1.4315 - val_accuracy: 0.6053\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.4218 - accuracy: 0.8907 - val_loss: 1.4730 - val_accuracy: 0.5895\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.3873 - accuracy: 0.8933 - val_loss: 1.5298 - val_accuracy: 0.5947\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.3548 - accuracy: 0.9160 - val_loss: 1.5250 - val_accuracy: 0.6053\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.3349 - accuracy: 0.9173 - val_loss: 1.4902 - val_accuracy: 0.6105\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.3054 - accuracy: 0.9307 - val_loss: 1.5325 - val_accuracy: 0.6158\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.2645 - accuracy: 0.9400 - val_loss: 1.5614 - val_accuracy: 0.6105\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.2453 - accuracy: 0.9387 - val_loss: 1.5882 - val_accuracy: 0.6053\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.2287 - accuracy: 0.9493 - val_loss: 1.6132 - val_accuracy: 0.6000\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.2116 - accuracy: 0.9547 - val_loss: 1.6248 - val_accuracy: 0.6158\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1924 - accuracy: 0.9587 - val_loss: 1.6456 - val_accuracy: 0.6158\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1720 - accuracy: 0.9693 - val_loss: 1.6694 - val_accuracy: 0.6158\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1576 - accuracy: 0.9653 - val_loss: 1.6923 - val_accuracy: 0.6105\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1424 - accuracy: 0.9733 - val_loss: 1.7175 - val_accuracy: 0.6158\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1385 - accuracy: 0.9693 - val_loss: 1.7439 - val_accuracy: 0.6105\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1467 - accuracy: 0.9693 - val_loss: 1.8033 - val_accuracy: 0.5947\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.3321 - accuracy: 0.8973 - val_loss: 1.7359 - val_accuracy: 0.6368\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1886 - accuracy: 0.9573 - val_loss: 1.6866 - val_accuracy: 0.6316\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1346 - accuracy: 0.9787 - val_loss: 1.7648 - val_accuracy: 0.6053\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1077 - accuracy: 0.9800 - val_loss: 1.7622 - val_accuracy: 0.6158\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0956 - accuracy: 0.9827 - val_loss: 1.7773 - val_accuracy: 0.6000\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0878 - accuracy: 0.9813 - val_loss: 1.7850 - val_accuracy: 0.6158\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0849 - accuracy: 0.9813 - val_loss: 1.8201 - val_accuracy: 0.6105\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0782 - accuracy: 0.9853 - val_loss: 1.8436 - val_accuracy: 0.6105\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0740 - accuracy: 0.9853 - val_loss: 1.8580 - val_accuracy: 0.6105\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0709 - accuracy: 0.9840 - val_loss: 1.8705 - val_accuracy: 0.6105\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0741 - accuracy: 0.9867 - val_loss: 1.9007 - val_accuracy: 0.6000\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0599 - accuracy: 0.9893 - val_loss: 1.9076 - val_accuracy: 0.6105\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0563 - accuracy: 0.9920 - val_loss: 1.9386 - val_accuracy: 0.6105\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0537 - accuracy: 0.9893 - val_loss: 1.9330 - val_accuracy: 0.5947\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0494 - accuracy: 0.9920 - val_loss: 1.9638 - val_accuracy: 0.5947\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0486 - accuracy: 0.9907 - val_loss: 1.9661 - val_accuracy: 0.5947\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0479 - accuracy: 0.9947 - val_loss: 1.9712 - val_accuracy: 0.6105\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0473 - accuracy: 0.9920 - val_loss: 1.9748 - val_accuracy: 0.5895\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0479 - accuracy: 0.9933 - val_loss: 2.0005 - val_accuracy: 0.6053\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0436 - accuracy: 0.9907 - val_loss: 2.0494 - val_accuracy: 0.6053\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0382 - accuracy: 0.9947 - val_loss: 2.0487 - val_accuracy: 0.5947\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0338 - accuracy: 0.9960 - val_loss: 2.0781 - val_accuracy: 0.5895\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 0.9973 - val_loss: 2.0542 - val_accuracy: 0.6053\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0309 - accuracy: 0.9947 - val_loss: 2.1244 - val_accuracy: 0.5947\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0291 - accuracy: 0.9973 - val_loss: 2.1123 - val_accuracy: 0.5947\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0281 - accuracy: 0.9947 - val_loss: 2.1333 - val_accuracy: 0.6000\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0273 - accuracy: 0.9973 - val_loss: 2.1331 - val_accuracy: 0.5895\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9973 - val_loss: 2.1603 - val_accuracy: 0.5947\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0211 - accuracy: 0.9987 - val_loss: 2.1620 - val_accuracy: 0.5947\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0222 - accuracy: 0.9987 - val_loss: 2.1714 - val_accuracy: 0.5895\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0199 - accuracy: 0.9973 - val_loss: 2.1895 - val_accuracy: 0.5895\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0194 - accuracy: 0.9987 - val_loss: 2.2011 - val_accuracy: 0.5895\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0176 - accuracy: 0.9973 - val_loss: 2.2146 - val_accuracy: 0.5895\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0167 - accuracy: 0.9987 - val_loss: 2.2230 - val_accuracy: 0.5947\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0157 - accuracy: 0.9987 - val_loss: 2.2286 - val_accuracy: 0.5895\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0161 - accuracy: 0.9987 - val_loss: 2.2464 - val_accuracy: 0.5947\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0150 - accuracy: 0.9973 - val_loss: 2.2546 - val_accuracy: 0.5895\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0141 - accuracy: 0.9973 - val_loss: 2.2568 - val_accuracy: 0.5947\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0142 - accuracy: 0.9973 - val_loss: 2.2539 - val_accuracy: 0.5895\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0146 - accuracy: 0.9973 - val_loss: 2.2901 - val_accuracy: 0.6000\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0130 - accuracy: 0.9987 - val_loss: 2.2900 - val_accuracy: 0.5947\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0126 - accuracy: 0.9987 - val_loss: 2.3011 - val_accuracy: 0.5895\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0128 - accuracy: 0.9973 - val_loss: 2.2919 - val_accuracy: 0.5895\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0122 - accuracy: 0.9973 - val_loss: 2.3105 - val_accuracy: 0.5895\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 2.3231 - val_accuracy: 0.5895\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.9973 - val_loss: 2.3222 - val_accuracy: 0.5895\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7920c45aaf20>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" Training \"\"\"\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Splitting data for training and testing\n",
        "eng_train, eng_val, frn_train, frn_val = train_test_split(english_seq, ita_seq, test_size=0.2)\n",
        "\n",
        "model.fit([eng_train, frn_train[:, :-1]], frn_train[:, 1:],\n",
        "            validation_data=([eng_val, frn_val[:, :-1]], frn_val[:, 1:]),\n",
        "            batch_size=2, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "sx7C-iKkBH72"
      },
      "outputs": [],
      "source": [
        "def translate(input_sentence):\n",
        "    # Tokenize and pad the input sentence\n",
        "    input_seq = tokenizer_eng.texts_to_sequences([input_sentence])\n",
        "    input_seq = keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_len_eng, padding='post')\n",
        "\n",
        "    # Get the encoder states\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Start token for the decoder, using the first word in our dictionary as the start point\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = 1  # Let's use the first word index as a starting point\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Get the token with the highest probability\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Check if the index is in the dictionary\n",
        "        if sampled_token_index in tokenizer_ita.index_word:\n",
        "            sampled_char = tokenizer_ita.index_word[sampled_token_index]\n",
        "            decoded_sentence += ' ' + sampled_char\n",
        "        else:\n",
        "            print(\"does not exist in dictionary!\")\n",
        "            break  # Exit if the index isn't in the dictionary\n",
        "\n",
        "        # Exit loop if max length is reached\n",
        "        if len(decoded_sentence.split()) > max_len_frn:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target_seq and states for the next loop iteration\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "uE0LIIA5BH74",
        "outputId": "6b177c16-6d6d-4cf5-e50f-c08be159e445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 324ms/step\n",
            "1/1 [==============================] - 0s 367ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "does not exist in dictionary!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'è è è ho tom'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"hello\"\n",
        "predicted_translation = translate(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sK6WWi_BH75"
      },
      "source": [
        "<div style=\"line-height:0.5\">\n",
        "<h3 style=\"color:#FF7C00  \"> Note: </h3>\n",
        "</div>\n",
        "Clearly, the prediction is wrong! Seq2Seq models typically require large datasets to produce accurate translations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVhgQJAQBH75"
      },
      "source": [
        "<h2 style=\"color:#FF7C00  \">  <u> Example 2 </u> </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "vtRHSQ4tBH76"
      },
      "outputs": [],
      "source": [
        "english_sentences = []\n",
        "italian_sentences = []\n",
        "\n",
        "with open(\"/content/eng-ita.txt\", \"r\", encoding=\"utf-8\") as f:    #./data/eng-ita if not on Colab\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        eng, ita = line.strip().split(\"\\t\")\n",
        "        english_sentences.append(eng)\n",
        "        italian_sentences.append(ita)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wXrLGZ3PjWO",
        "outputId": "6e526742-ab46-46a6-9193-2dd7490f4e38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['Hi.', 'Run!', 'Run!', 'Run!', 'Who?'],\n",
              " ['Ciao!', 'Corri!', 'Corra!', 'Correte!', 'Chi?'])"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_sentences[:5], italian_sentences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcscbku9BH76",
        "outputId": "ec3f69e0-50d4-4b0f-f596-3e0c64870b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\"\"\" Use only 20% of the data to avoid memory allocation problems. \"\"\"\n",
        "\n",
        "# num_samples = int(0.20 * len(english_sentences))\n",
        "# # Randomly sample indices\n",
        "# sampled_indices = random.sample(range(len(english_sentences)), num_samples)\n",
        "\n",
        "# ## Use indices to sample from lists\n",
        "# english_sentences_sampled = [english_sentences[i] for i in sampled_indices]\n",
        "# italian_sentences_sampled = [italian_sentences[i] for i in sampled_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "DfuV88ceBH77"
      },
      "outputs": [],
      "source": [
        "# Tokenization using TensorFlow's Keras API\n",
        "tokenizer_eng = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer_eng.fit_on_texts(english_sentences)\n",
        "english_sequences = tokenizer_eng.texts_to_sequences(english_sentences)\n",
        "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
        "max_len_eng = max([len(seq) for seq in english_sequences])\n",
        "\n",
        "tokenizer_ita = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer_ita.fit_on_texts(italian_sentences)\n",
        "italian_sequences = tokenizer_ita.texts_to_sequences(italian_sentences)\n",
        "vocab_size_ita = len(tokenizer_ita.word_index) + 1\n",
        "max_len_frn = max([len(seq) for seq in italian_sequences])\n",
        "\n",
        "# Padding sequences\n",
        "english_sequences = tf.keras.preprocessing.sequence.pad_sequences(english_sequences, maxlen=max_len_eng, padding='post')\n",
        "italian_sequences = tf.keras.preprocessing.sequence.pad_sequences(italian_sequences, maxlen=max_len_frn, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "rETTusliPtHp"
      },
      "outputs": [],
      "source": [
        "#### Parameters\n",
        "embedding_dim = 50\n",
        "lstm_units = 128\n",
        "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
        "vocab_size_ita = len(tokenizer_ita.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "R4iFuEKIPtqs"
      },
      "outputs": [],
      "source": [
        "###### Encoder\n",
        "encoder_input = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(vocab_size_eng, embedding_dim)(encoder_input)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "encoder_output, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "####### Decoder\n",
        "decoder_input = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(vocab_size_ita, embedding_dim)(decoder_input)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size_ita, activation='softmax')\n",
        "decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "model = keras.Model([encoder_input, decoder_input], decoder_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "KxM1YrxSPttu"
      },
      "outputs": [],
      "source": [
        "# Encoder inference model\n",
        "encoder_model = keras.Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embedding_inference = Embedding(vocab_size_ita, embedding_dim)(decoder_input)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_inference, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = keras.Model([decoder_input] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "e9UhW-NARI_U"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTeUP3jlBH77",
        "outputId": "370c9d39-31bd-4d60-cdd8-2a57871c7b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4148/4148 [==============================] - 398s 96ms/step - loss: 0.1578 - accuracy: 0.9656 - val_loss: 0.6621 - val_accuracy: 0.9012\n",
            "Epoch 2/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.1320 - accuracy: 0.9698 - val_loss: 0.6496 - val_accuracy: 0.9034\n",
            "Epoch 3/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.1125 - accuracy: 0.9732 - val_loss: 0.6496 - val_accuracy: 0.9053\n",
            "Epoch 4/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0975 - accuracy: 0.9758 - val_loss: 0.6530 - val_accuracy: 0.9061\n",
            "Epoch 5/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0860 - accuracy: 0.9779 - val_loss: 0.6543 - val_accuracy: 0.9069\n",
            "Epoch 6/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0769 - accuracy: 0.9796 - val_loss: 0.6597 - val_accuracy: 0.9076\n",
            "Epoch 7/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0697 - accuracy: 0.9809 - val_loss: 0.6643 - val_accuracy: 0.9082\n",
            "Epoch 8/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0638 - accuracy: 0.9820 - val_loss: 0.6752 - val_accuracy: 0.9085\n",
            "Epoch 9/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0590 - accuracy: 0.9829 - val_loss: 0.6824 - val_accuracy: 0.9085\n",
            "Epoch 10/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0550 - accuracy: 0.9837 - val_loss: 0.6906 - val_accuracy: 0.9090\n",
            "Epoch 11/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0517 - accuracy: 0.9843 - val_loss: 0.6950 - val_accuracy: 0.9090\n",
            "Epoch 12/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0488 - accuracy: 0.9849 - val_loss: 0.7143 - val_accuracy: 0.9091\n",
            "Epoch 13/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0464 - accuracy: 0.9854 - val_loss: 0.7205 - val_accuracy: 0.9090\n",
            "Epoch 14/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0442 - accuracy: 0.9858 - val_loss: 0.7251 - val_accuracy: 0.9092\n",
            "Epoch 15/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0424 - accuracy: 0.9861 - val_loss: 0.7372 - val_accuracy: 0.9087\n",
            "Epoch 16/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0408 - accuracy: 0.9864 - val_loss: 0.7452 - val_accuracy: 0.9090\n",
            "Epoch 17/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0393 - accuracy: 0.9867 - val_loss: 0.7544 - val_accuracy: 0.9090\n",
            "Epoch 18/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0381 - accuracy: 0.9869 - val_loss: 0.7576 - val_accuracy: 0.9088\n",
            "Epoch 19/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0369 - accuracy: 0.9872 - val_loss: 0.7644 - val_accuracy: 0.9089\n",
            "Epoch 20/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0359 - accuracy: 0.9874 - val_loss: 0.7842 - val_accuracy: 0.9089\n",
            "Epoch 21/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0350 - accuracy: 0.9875 - val_loss: 0.7848 - val_accuracy: 0.9088\n",
            "Epoch 22/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0341 - accuracy: 0.9877 - val_loss: 0.7953 - val_accuracy: 0.9087\n",
            "Epoch 23/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0333 - accuracy: 0.9879 - val_loss: 0.8038 - val_accuracy: 0.9086\n",
            "Epoch 24/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0326 - accuracy: 0.9880 - val_loss: 0.8085 - val_accuracy: 0.9086\n",
            "Epoch 25/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0320 - accuracy: 0.9881 - val_loss: 0.8148 - val_accuracy: 0.9082\n",
            "Epoch 26/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0314 - accuracy: 0.9883 - val_loss: 0.8279 - val_accuracy: 0.9082\n",
            "Epoch 27/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0309 - accuracy: 0.9884 - val_loss: 0.8323 - val_accuracy: 0.9083\n",
            "Epoch 28/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0304 - accuracy: 0.9884 - val_loss: 0.8402 - val_accuracy: 0.9078\n",
            "Epoch 29/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0299 - accuracy: 0.9886 - val_loss: 0.8451 - val_accuracy: 0.9077\n",
            "Epoch 30/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0294 - accuracy: 0.9886 - val_loss: 0.8543 - val_accuracy: 0.9079\n",
            "Epoch 31/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0290 - accuracy: 0.9887 - val_loss: 0.8602 - val_accuracy: 0.9078\n",
            "Epoch 32/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0287 - accuracy: 0.9888 - val_loss: 0.8784 - val_accuracy: 0.9078\n",
            "Epoch 33/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0283 - accuracy: 0.9888 - val_loss: 0.8729 - val_accuracy: 0.9076\n",
            "Epoch 34/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0280 - accuracy: 0.9889 - val_loss: 0.8803 - val_accuracy: 0.9075\n",
            "Epoch 35/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0277 - accuracy: 0.9889 - val_loss: 0.8860 - val_accuracy: 0.9074\n",
            "Epoch 36/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0274 - accuracy: 0.9890 - val_loss: 0.8919 - val_accuracy: 0.9072\n",
            "Epoch 37/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0271 - accuracy: 0.9890 - val_loss: 0.9023 - val_accuracy: 0.9076\n",
            "Epoch 38/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0268 - accuracy: 0.9891 - val_loss: 0.9008 - val_accuracy: 0.9070\n",
            "Epoch 39/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0266 - accuracy: 0.9891 - val_loss: 0.9070 - val_accuracy: 0.9073\n",
            "Epoch 40/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0264 - accuracy: 0.9892 - val_loss: 0.9130 - val_accuracy: 0.9072\n",
            "Epoch 41/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0261 - accuracy: 0.9892 - val_loss: 0.9186 - val_accuracy: 0.9070\n",
            "Epoch 42/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0260 - accuracy: 0.9893 - val_loss: 0.9265 - val_accuracy: 0.9069\n",
            "Epoch 43/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0257 - accuracy: 0.9893 - val_loss: 0.9320 - val_accuracy: 0.9070\n",
            "Epoch 44/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0256 - accuracy: 0.9893 - val_loss: 0.9335 - val_accuracy: 0.9068\n",
            "Epoch 45/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0254 - accuracy: 0.9894 - val_loss: 0.9464 - val_accuracy: 0.9069\n",
            "Epoch 46/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0253 - accuracy: 0.9894 - val_loss: 0.9412 - val_accuracy: 0.9065\n",
            "Epoch 47/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0251 - accuracy: 0.9894 - val_loss: 0.9454 - val_accuracy: 0.9064\n",
            "Epoch 48/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0249 - accuracy: 0.9894 - val_loss: 0.9591 - val_accuracy: 0.9068\n",
            "Epoch 49/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0248 - accuracy: 0.9895 - val_loss: 0.9627 - val_accuracy: 0.9066\n",
            "Epoch 50/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0247 - accuracy: 0.9895 - val_loss: 0.9752 - val_accuracy: 0.9067\n",
            "Epoch 51/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0244 - accuracy: 0.9895 - val_loss: 0.9661 - val_accuracy: 0.9066\n",
            "Epoch 52/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0244 - accuracy: 0.9895 - val_loss: 0.9767 - val_accuracy: 0.9065\n",
            "Epoch 53/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0243 - accuracy: 0.9896 - val_loss: 0.9748 - val_accuracy: 0.9066\n",
            "Epoch 54/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0241 - accuracy: 0.9896 - val_loss: 0.9792 - val_accuracy: 0.9065\n",
            "Epoch 55/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0240 - accuracy: 0.9896 - val_loss: 0.9826 - val_accuracy: 0.9063\n",
            "Epoch 56/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0239 - accuracy: 0.9896 - val_loss: 0.9903 - val_accuracy: 0.9061\n",
            "Epoch 57/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0238 - accuracy: 0.9897 - val_loss: 0.9912 - val_accuracy: 0.9064\n",
            "Epoch 58/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0237 - accuracy: 0.9897 - val_loss: 1.0032 - val_accuracy: 0.9062\n",
            "Epoch 59/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0236 - accuracy: 0.9897 - val_loss: 0.9995 - val_accuracy: 0.9064\n",
            "Epoch 60/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0235 - accuracy: 0.9897 - val_loss: 1.0066 - val_accuracy: 0.9065\n",
            "Epoch 61/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0235 - accuracy: 0.9897 - val_loss: 1.0035 - val_accuracy: 0.9064\n",
            "Epoch 62/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0234 - accuracy: 0.9897 - val_loss: 1.0102 - val_accuracy: 0.9062\n",
            "Epoch 63/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0233 - accuracy: 0.9898 - val_loss: 1.0169 - val_accuracy: 0.9064\n",
            "Epoch 64/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0232 - accuracy: 0.9898 - val_loss: 1.0174 - val_accuracy: 0.9061\n",
            "Epoch 65/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0231 - accuracy: 0.9898 - val_loss: 1.0283 - val_accuracy: 0.9063\n",
            "Epoch 66/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0230 - accuracy: 0.9898 - val_loss: 1.0268 - val_accuracy: 0.9059\n",
            "Epoch 67/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0229 - accuracy: 0.9898 - val_loss: 1.0333 - val_accuracy: 0.9061\n",
            "Epoch 68/100\n",
            "4148/4148 [==============================] - 399s 96ms/step - loss: 0.0229 - accuracy: 0.9898 - val_loss: 1.0352 - val_accuracy: 0.9058\n",
            "Epoch 69/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0228 - accuracy: 0.9898 - val_loss: 1.0372 - val_accuracy: 0.9061\n",
            "Epoch 70/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0228 - accuracy: 0.9898 - val_loss: 1.0339 - val_accuracy: 0.9056\n",
            "Epoch 71/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0227 - accuracy: 0.9899 - val_loss: 1.0477 - val_accuracy: 0.9056\n",
            "Epoch 72/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0226 - accuracy: 0.9898 - val_loss: 1.0441 - val_accuracy: 0.9057\n",
            "Epoch 73/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0226 - accuracy: 0.9899 - val_loss: 1.0465 - val_accuracy: 0.9058\n",
            "Epoch 74/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0225 - accuracy: 0.9899 - val_loss: 1.0505 - val_accuracy: 0.9058\n",
            "Epoch 75/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0225 - accuracy: 0.9899 - val_loss: 1.0567 - val_accuracy: 0.9058\n",
            "Epoch 76/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0224 - accuracy: 0.9899 - val_loss: 1.0632 - val_accuracy: 0.9060\n",
            "Epoch 77/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0224 - accuracy: 0.9899 - val_loss: 1.0630 - val_accuracy: 0.9057\n",
            "Epoch 78/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0223 - accuracy: 0.9899 - val_loss: 1.0647 - val_accuracy: 0.9058\n",
            "Epoch 79/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0223 - accuracy: 0.9899 - val_loss: 1.0697 - val_accuracy: 0.9055\n",
            "Epoch 80/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0222 - accuracy: 0.9899 - val_loss: 1.0727 - val_accuracy: 0.9058\n",
            "Epoch 81/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0222 - accuracy: 0.9899 - val_loss: 1.0797 - val_accuracy: 0.9056\n",
            "Epoch 82/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0221 - accuracy: 0.9899 - val_loss: 1.0730 - val_accuracy: 0.9056\n",
            "Epoch 83/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0221 - accuracy: 0.9899 - val_loss: 1.0731 - val_accuracy: 0.9056\n",
            "Epoch 84/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0220 - accuracy: 0.9900 - val_loss: 1.0761 - val_accuracy: 0.9053\n",
            "Epoch 85/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0219 - accuracy: 0.9900 - val_loss: 1.0818 - val_accuracy: 0.9055\n",
            "Epoch 86/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0219 - accuracy: 0.9900 - val_loss: 1.0790 - val_accuracy: 0.9055\n",
            "Epoch 87/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0219 - accuracy: 0.9900 - val_loss: 1.0883 - val_accuracy: 0.9055\n",
            "Epoch 88/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0219 - accuracy: 0.9900 - val_loss: 1.0919 - val_accuracy: 0.9057\n",
            "Epoch 89/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0218 - accuracy: 0.9900 - val_loss: 1.0971 - val_accuracy: 0.9057\n",
            "Epoch 90/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0219 - accuracy: 0.9900 - val_loss: 1.1003 - val_accuracy: 0.9053\n",
            "Epoch 91/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0217 - accuracy: 0.9900 - val_loss: 1.0986 - val_accuracy: 0.9054\n",
            "Epoch 92/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0217 - accuracy: 0.9900 - val_loss: 1.0962 - val_accuracy: 0.9056\n",
            "Epoch 93/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0217 - accuracy: 0.9900 - val_loss: 1.0965 - val_accuracy: 0.9056\n",
            "Epoch 94/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0216 - accuracy: 0.9900 - val_loss: 1.1028 - val_accuracy: 0.9053\n",
            "Epoch 95/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0216 - accuracy: 0.9900 - val_loss: 1.1034 - val_accuracy: 0.9055\n",
            "Epoch 96/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0216 - accuracy: 0.9901 - val_loss: 1.1095 - val_accuracy: 0.9055\n",
            "Epoch 97/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0215 - accuracy: 0.9901 - val_loss: 1.1075 - val_accuracy: 0.9055\n",
            "Epoch 98/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0216 - accuracy: 0.9900 - val_loss: 1.1169 - val_accuracy: 0.9053\n",
            "Epoch 99/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0215 - accuracy: 0.9901 - val_loss: 1.1206 - val_accuracy: 0.9054\n",
            "Epoch 100/100\n",
            "4148/4148 [==============================] - 400s 96ms/step - loss: 0.0215 - accuracy: 0.9900 - val_loss: 1.1180 - val_accuracy: 0.9051\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79201c1378b0>"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.fit([english_sequences, italian_sequences[:,:-1]], keras.utils.to_categorical(italian_sequences[:,1:], num_classes=vocab_size_ita),\n",
        "#                                                                                     batch_size=64,\n",
        "#                                                                                     epochs=5,\n",
        "#                                                                                     validation_split=0.2)\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    [english_sequences, italian_sequences[:,:-1]],  # Inputs\n",
        "    italian_sequences[:,1:],                        # Targets\n",
        "    batch_size=64,\n",
        "    epochs=100,\n",
        "    validation_split=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "wNNiHZQ_QFWj"
      },
      "outputs": [],
      "source": [
        "def translate(input_sentence):\n",
        "    # Tokenize and pad the input sentence\n",
        "    input_seq = tokenizer_eng.texts_to_sequences([input_sentence])\n",
        "    input_seq = keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_len_eng, padding='post')\n",
        "\n",
        "    # Get the encoder states\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Start token for the decoder using the most frequent word in Italian sequences\n",
        "    start_word = tokenizer_ita.index_word[1]\n",
        "    start_index = tokenizer_ita.word_index[start_word]\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    #target_seq[0, 0] = start_index\n",
        "    target_seq[0, 0] = 1  # Let's use the first word index as a starting point\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Get the token with the highest probability\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Check if the index is in the dictionary\n",
        "        if sampled_token_index in tokenizer_ita.index_word:\n",
        "            sampled_char = tokenizer_ita.index_word[sampled_token_index]\n",
        "            decoded_sentence += ' ' + sampled_char\n",
        "        else:\n",
        "            print(\"does not exist in dictionary!\")\n",
        "            break  # Exit if the index isn't in the dictionary\n",
        "\n",
        "        # Exit loop if max length is reached or if the token is a repeat of the start word\n",
        "        if (len(decoded_sentence.split()) > max_len_frn) or (sampled_char == start_word):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target_seq and states for the next loop iteration\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlOdDSPjd9KD",
        "outputId": "254c1c60-aeee-4c06-807c-ec6834a3b4a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "def translate2(input_sentence):\n",
        "    # Tokenize and pad the input sentence\n",
        "    input_seq = tokenizer_eng.texts_to_sequences([input_sentence])\n",
        "    input_seq = keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_len_eng, padding='post')\n",
        "\n",
        "    # Get the encoder states\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Initialize the starting sequence for the decoder with a value of 0\n",
        "    # (this assumes 0 is not a valid token in your dictionary, but just a padding value)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        print(output_tokens[0, -1, :])\n",
        "\n",
        "        # Get the token with the highest probability\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Check if the index is in the dictionary\n",
        "        if sampled_token_index in tokenizer_ita.index_word:\n",
        "            sampled_word = tokenizer_ita.index_word[sampled_token_index]\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "            # Exit loop if max length is reached or if we encounter an end token (this can be adjusted)\n",
        "            if (len(decoded_sentence.split()) > max_len_frn):\n",
        "                stop_condition = True\n",
        "\n",
        "        else:\n",
        "            print(\"does not exist in dictionary!\")\n",
        "            break\n",
        "\n",
        "        # Update the target_seq and states for the next loop iteration\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "baeSDW2PgwlL"
      },
      "outputs": [],
      "source": [
        "def translate3(input_sentence):\n",
        "    input_seq = tokenizer_eng.texts_to_sequences([input_sentence])\n",
        "    input_seq = keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_len_eng, padding='post')\n",
        "\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Change this to use a special <start> token if you added one, else keep as is\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = 1\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Print this for debugging\n",
        "        print(output_tokens)\n",
        "\n",
        "        predicted_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        if predicted_index in tokenizer_ita.index_word:\n",
        "            sampled_char = tokenizer_ita.index_word[predicted_index]\n",
        "            decoded_sentence += ' ' + sampled_char\n",
        "        else:\n",
        "            print(\"Index not in dictionary:\", predicted_index)\n",
        "            break\n",
        "\n",
        "        if (len(decoded_sentence.split()) > max_len_frn) or (sampled_char == tokenizer_ita.index_word[1]):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = predicted_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "RRjPq1S8BH78",
        "outputId": "d0a72954-8c91-4695-c18e-362f24b5da20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[[[7.8913952e-05 9.6949625e-01 7.8210903e-08 ... 7.0891931e-10\n",
            "   7.8226342e-10 7.0940898e-10]]]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tom'"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Ask Tom\"\n",
        "predicted_translation = translate3(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "74AruGIuBH78",
        "outputId": "6d62f5d1-34fc-4b24-f495-da0d3009cb8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "[[[6.9520521e-01 7.1514834e-05 7.0451910e-07 ... 2.1985787e-08\n",
            "   2.1940334e-08 2.1700236e-08]]]\n",
            "Index not in dictionary: 0\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Go slow.\"\n",
        "predicted_translation = translate(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "y1SWKpaDBH78",
        "outputId": "8675668c-c60f-4ea9-b561-011e9d898525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "does not exist in dictionary!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'vada'"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Go slow.\"\n",
        "predicted_translation = translate2(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "iUQh9rsRnSXO",
        "outputId": "9ffc9047-f35d-48f6-c4fd-dac64898b8f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "does not exist in dictionary!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'vada'"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Go slow.\"\n",
        "predicted_translation = translate(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "wiXICYoK_u2V",
        "outputId": "f382f089-d13e-4721-becd-8b97dc0af926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "does not exist in dictionary!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'questa domani oggi oggi'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Call me tomorrow.\"\n",
        "predicted_translation = translate(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Tfvld1yFAHHi",
        "outputId": "4c9e86b9-9c7f-4a3f-e90f-1c266be44f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "does not exist in dictionary!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mi'"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Call me.\"\n",
        "predicted_translation = translate2(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "oTiAQZCkAUm0",
        "outputId": "3c861eb9-df23-4094-85f4-164e6c176ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "does not exist in dictionary!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sono'"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"I got fat.\"\n",
        "predicted_translation = translate(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "xPG_WgpgAUrb",
        "outputId": "f98fb175-7ee4-4063-beaf-c39507496d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "does not exist in dictionary!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lo'"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"know.\"\n",
        "predicted_translation = translate(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "QdnWoZsWAUwT",
        "outputId": "10cef53f-6740-443b-9ddc-9adbc44c6022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "does not exist in dictionary!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Get Away!\"\n",
        "predicted_translation = translate(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "QV9flrXYAU0l",
        "outputId": "fed62f83-20a0-40cc-c723-b3b401d3eb7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "does not exist in dictionary!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'la'"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"I shaved\"\n",
        "predicted_translation = translate(input_sentence)\n",
        "predicted_translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "eMGl4ZrrAU5B",
        "outputId": "c2561bd2-a344-46ea-d4c3-216e39f15955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[[[9.9999070e-01 7.6397136e-18 1.5178664e-18 ... 9.4466676e-13\n",
            "   8.9033479e-13 9.1798411e-13]]]\n",
            "Index not in dictionary: 0\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Let's go!\"\n",
        "predicted_translation = translate3(input_sentence)\n",
        "predicted_translation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
