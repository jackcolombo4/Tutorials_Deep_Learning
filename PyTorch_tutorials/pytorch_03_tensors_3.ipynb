{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:0.5\">\n",
    "<h1 style=\"color:#BF66F2 \">  Tensors in PyTorch 3 </h1>\n",
    "<h4> Indexing, Unsqueezing, and Splitting. </h4> \n",
    "<div style=\"margin-top: 5px;\">\n",
    "<span style=\"display: inline-block;\">\n",
    "    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3>\n",
    "    warnings.catch_warnings + torch.any() + torch.all() + torch.arange() + views + torch.cat()\n",
    "</span>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  \n",
    "    device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 2x3 Tensor (2 rows, 3 columns)\n",
    "my_tensor = torch.tensor(\n",
    "    [[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about tensor: tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True)\n",
      "Type of Tensor {my_tensor.dtype}\n",
      "Device Tensor is on cpu\n",
      "Shape of tensor torch.Size([2, 3])\n",
      "Requires gradient: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Information about tensor: {my_tensor}\")  \n",
    "print(\"Type of Tensor {my_tensor.dtype}\")\n",
    "print(f\"Device Tensor is on {my_tensor.device}\")\n",
    "print(f\"Shape of tensor {my_tensor.shape}\")\n",
    "print(f\"Requires gradient: {my_tensor.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#BF66F2 \">  Create tensors in various way </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> using zeros\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "> using rand\n",
      "tensor([[0.8887, 0.0859, 0.6226],\n",
      "        [0.0683, 0.1083, 0.5266],\n",
      "        [0.3486, 0.7490, 0.9730]])\n",
      "> using ones\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "> using eye\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "> using arange\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "> using linspace\n",
      "tensor([0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000, 0.9000,\n",
      "        1.0000])\n",
      "> using empty\n",
      "tensor([[9.5423e-35, 4.5890e-41, 1.9274e-34],\n",
      "        [0.0000e+00, 4.4842e-44, 0.0000e+00],\n",
      "        [8.9683e-44, 0.0000e+00, 1.9235e-34]])\n",
      "> using empty -normal\n",
      "tensor([[-1.0812, -2.7225,  0.6885, -1.8093, -0.4509]])\n",
      "> using empty -uniform\n",
      "tensor([[0.9151, 0.7595, 0.2144, 0.3425, 0.4196]])\n",
      "> using diag\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"> using zeros\")\n",
    "print(torch.zeros((3, 3)))\n",
    "print(\"> using rand\")\n",
    "print(torch.rand((3, 3)))\n",
    "print(\"> using ones\")\n",
    "print(torch.ones((3, 3)))\n",
    "print(\"> using eye\")\n",
    "print(torch.eye(5, 5))\n",
    "print(\"> using arange\")\n",
    "print(torch.arange(start=0, end=5, step=1))\n",
    "print(\"> using linspace\")\n",
    "print(torch.linspace(start=0.1, end=1, steps=10))\n",
    "print(\"> using empty\")\n",
    "print(torch.empty(size=(3, 3)))\n",
    "print(\"> using empty -normal\")\n",
    "print(torch.empty(size=(1, 5)).normal_(mean=0, std=1))\n",
    "print(\"> using empty -uniform\")\n",
    "print(torch.empty(size=(1, 5)).uniform_(0, 1))\n",
    "print(\"> using diag\")\n",
    "print(torch.diag(torch.ones(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tensor = tensor([0, 1, 2, 3])\n",
      "\n",
      "Converted Boolean: tensor([False,  True,  True,  True])\n",
      "Converted int16 tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "Converted int64 tensor([0, 1, 2, 3])\n",
      "Converted float16 tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "Converted float32 tensor([0., 1., 2., 3.])\n",
      "Converted float64 tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(4)  \n",
    "print(f\"Starting tensor = {tensor}\")\n",
    "print()\n",
    "print(f\"Converted Boolean: {tensor.bool()}\")  \n",
    "print(f\"Converted int16 {tensor.short()}\") \n",
    "print(f\"Converted int64 {tensor.long()}\")  \n",
    "print(f\"Converted float16 {tensor.half()}\")\n",
    "print(f\"Converted float32 {tensor.float()}\")\n",
    "print(f\"Converted float64 {tensor.double()}\") \n",
    "\n",
    "\n",
    "np_array = np.zeros((5, 5))\n",
    "tensor = torch.from_numpy(np_array)\n",
    "np_array_again = (tensor.numpy()) \n",
    "\n",
    "np_array_again"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#BF66F2 \">  => Tensor Math & Comparison: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([13, 62,  4]), tensor([91, 39, 31]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([13, 62, 4])\n",
    "y = torch.tensor([91, 39, 31])\n",
    "\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zadd1 tensor([104., 101.,  35.])\n",
      "zadd2 tensor([104., 101.,  35.])\n",
      "zadd3 tensor([104., 101.,  35.])\n",
      "zsub tensor([-78,  23, -27])\n",
      "zdiv tensor([0.1429, 1.5897, 0.1290])\n",
      "t tensor([ 26., 124.,   8.])\n",
      "zexp1 tensor([ 169, 3844,   16])\n",
      "zexp2 tensor([ 169, 3844,   16])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" All operations followed by _ will mutate the tensor in place. \"\"\"\n",
    "\n",
    "#### Addition\n",
    "zadd1 = torch.empty(3)\n",
    "torch.add(x, y, out=zadd1)  \n",
    "zadd2 = torch.add(x, y)  \n",
    "zadd3 = x + y  \n",
    "# Subtraction\n",
    "zsub = x - y \n",
    "# Division (element-wise division if of equal shape)\n",
    "zdiv = torch.true_divide(x, y)  \n",
    "\n",
    "# Inplace Operations\n",
    "t = torch.zeros(3)\n",
    "\n",
    "t.add_(x)  \n",
    "t += x \n",
    "\n",
    "## Exponentiation \n",
    "zexp1 = x.pow(2)\n",
    "zexp2 = x**2\n",
    "\n",
    "print(f\"zadd1 {zadd1}\")\n",
    "print(f\"zadd2 {zadd1}\")\n",
    "print(f\"zadd3 {zadd1}\")\n",
    "print(f\"zsub {zsub}\")\n",
    "print(f\"zdiv {zdiv}\")\n",
    "print(f\"t {t}\")\n",
    "print(f\"zexp1 {zexp1}\")\n",
    "print(f\"zexp2 {zexp2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([True, True, True]), tensor([False, False, False]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Simple Comparison\n",
    "istru1 = x > 0  \n",
    "istru2 = x < 0  \n",
    "istru1, istru2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9988, 0.2362, 0.8945, 0.2892, 0.1626],\n",
       "         [0.6635, 0.5766, 0.1184, 0.2094, 0.3850]]),\n",
       " tensor([[0.2388, 0.8636, 0.6486],\n",
       "         [0.1661, 0.4776, 0.1657],\n",
       "         [0.4075, 0.8635, 0.0388],\n",
       "         [0.1257, 0.6901, 0.5808],\n",
       "         [0.0402, 0.9301, 0.4934]]),\n",
       " tensor([[0.6852, 2.0986, 0.9699],\n",
       "         [0.3443, 1.4531, 0.8421]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Matrix Multiplication \"\"\"\n",
    "x1 = torch.rand((2, 5))\n",
    "x2 = torch.rand((5, 3))\n",
    "x3 = torch.mm(x1, x2)\n",
    "x3 = x1.mm(x2)  \n",
    "\n",
    "x1,x2,x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.8089, 2.5563, 2.2565, 2.7977, 2.8177],\n",
      "        [5.0540, 3.3086, 2.9690, 3.6719, 3.7327],\n",
      "        [2.9356, 1.9235, 1.7310, 2.0870, 2.1043],\n",
      "        [3.1672, 2.1783, 1.9301, 2.1526, 2.2150],\n",
      "        [3.7691, 2.4321, 2.2943, 2.6194, 2.6318]])\n",
      "Element wise tensor([1183, 2418,  124])\n",
      "\n",
      " Dot product 3725\n"
     ]
    }
   ],
   "source": [
    "# Matrix Exponentiation\n",
    "matrix_exp = torch.rand(5, 5)\n",
    "print(matrix_exp.matrix_power(3))  \n",
    "\n",
    "# Element-wise Multiplication\n",
    "z = x * y  \n",
    "print(f\"Elementwise {z}\")\n",
    "\n",
    "# Dot product\n",
    "dot = torch.dot(x, y)\n",
    "print(f\"\\n Dot product {dot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[8.4261e-01, 3.0092e-01, 7.8176e-01, 3.1163e-01, 1.4427e-01,\n",
       "          8.6063e-01, 8.2137e-02, 1.9298e-01, 2.5813e-01, 3.3769e-01,\n",
       "          5.7559e-01, 4.0910e-01, 1.1051e-03, 3.7224e-01, 1.2842e-01,\n",
       "          7.3466e-01, 3.4883e-01, 4.9214e-02, 1.4183e-01, 2.4460e-01],\n",
       "         [9.9859e-01, 3.9346e-01, 5.7881e-01, 1.3190e-01, 2.1938e-01,\n",
       "          4.5551e-01, 3.1521e-01, 9.6823e-01, 7.2722e-02, 1.9922e-01,\n",
       "          1.5762e-01, 1.3863e-01, 5.7842e-02, 8.7356e-01, 1.5956e-01,\n",
       "          4.9559e-01, 6.3387e-02, 2.3631e-01, 7.9616e-01, 6.0165e-01],\n",
       "         [2.7927e-01, 3.8414e-01, 9.5781e-01, 5.7902e-02, 9.0150e-01,\n",
       "          1.5924e-01, 7.2419e-01, 5.3276e-01, 9.0321e-01, 7.0979e-01,\n",
       "          7.9326e-02, 8.1351e-01, 9.2070e-02, 4.8113e-01, 3.4554e-01,\n",
       "          8.4606e-01, 6.1683e-01, 4.8480e-01, 8.4794e-02, 6.3852e-01],\n",
       "         [6.0609e-01, 7.2384e-02, 8.4133e-01, 8.2729e-01, 8.5949e-01,\n",
       "          7.1178e-01, 6.7081e-01, 1.1893e-01, 3.8066e-01, 1.2461e-01,\n",
       "          9.7887e-01, 6.8822e-01, 8.6399e-01, 8.2572e-01, 9.2623e-02,\n",
       "          5.2561e-01, 7.0493e-01, 2.0004e-01, 6.0126e-01, 9.1173e-01],\n",
       "         [4.6852e-01, 8.4071e-01, 1.3624e-01, 2.9176e-01, 6.9649e-01,\n",
       "          3.6588e-01, 8.5637e-01, 1.8911e-01, 5.8209e-02, 8.3880e-01,\n",
       "          3.9023e-01, 6.5437e-01, 6.6466e-01, 7.1189e-02, 8.4758e-01,\n",
       "          8.6246e-01, 5.3558e-02, 1.6942e-01, 3.5152e-01, 8.5153e-01],\n",
       "         [5.4050e-01, 8.3473e-01, 7.8465e-01, 7.4800e-01, 1.9125e-01,\n",
       "          4.2240e-01, 3.7131e-02, 5.3550e-03, 3.7048e-01, 9.3652e-01,\n",
       "          4.0460e-01, 9.5421e-01, 1.8759e-01, 2.8847e-02, 1.8625e-01,\n",
       "          6.3899e-01, 2.2605e-01, 1.4847e-01, 6.2965e-01, 7.1238e-01],\n",
       "         [1.2085e-01, 7.2590e-01, 2.9433e-01, 8.6680e-02, 8.6505e-01,\n",
       "          1.8166e-01, 3.9143e-02, 4.4658e-01, 6.6502e-01, 2.6793e-01,\n",
       "          5.4773e-01, 9.8223e-01, 9.1855e-01, 1.3922e-01, 3.8243e-01,\n",
       "          9.4641e-02, 5.6261e-01, 4.8232e-01, 7.5240e-01, 9.1007e-01],\n",
       "         [2.1378e-02, 9.6130e-03, 8.7933e-01, 1.1856e-01, 6.2533e-02,\n",
       "          4.1833e-01, 7.6653e-02, 1.4489e-02, 2.7146e-02, 7.4681e-01,\n",
       "          3.5424e-01, 2.7724e-01, 5.1080e-01, 6.1710e-01, 6.4931e-01,\n",
       "          6.2403e-02, 3.8388e-02, 7.8712e-01, 2.5930e-01, 4.3866e-01],\n",
       "         [7.0860e-01, 1.2768e-01, 8.4540e-01, 2.7242e-02, 1.4838e-01,\n",
       "          7.3839e-03, 5.7792e-01, 7.2968e-01, 5.1789e-01, 2.8049e-01,\n",
       "          9.5052e-01, 1.4590e-01, 2.1251e-01, 9.8302e-01, 7.6977e-01,\n",
       "          8.4431e-01, 1.4648e-01, 9.8338e-01, 2.0828e-01, 9.6445e-01],\n",
       "         [4.2671e-01, 9.5239e-01, 4.1316e-01, 4.3670e-02, 4.0015e-01,\n",
       "          1.7856e-01, 7.2195e-01, 4.5508e-01, 6.2403e-01, 8.7605e-01,\n",
       "          9.4031e-01, 6.0006e-01, 8.7312e-01, 4.4905e-01, 1.3636e-01,\n",
       "          6.1792e-01, 8.2176e-01, 6.9653e-02, 1.0387e-01, 3.9008e-01]],\n",
       "\n",
       "        [[8.9952e-01, 3.0170e-01, 7.5148e-01, 9.3860e-01, 1.4710e-01,\n",
       "          2.9566e-01, 3.7665e-01, 8.4130e-01, 6.9528e-03, 5.0128e-01,\n",
       "          7.1231e-02, 8.0801e-01, 4.2375e-01, 2.8985e-01, 3.2623e-02,\n",
       "          6.0109e-01, 1.6290e-01, 6.5412e-01, 5.3306e-01, 3.4008e-01],\n",
       "         [2.5937e-01, 7.2070e-02, 9.4695e-01, 2.5656e-01, 6.0398e-01,\n",
       "          1.6724e-01, 6.8288e-01, 2.5127e-01, 1.6830e-01, 6.1123e-01,\n",
       "          7.6625e-01, 7.5485e-01, 3.7987e-01, 9.6781e-01, 1.8314e-02,\n",
       "          9.5129e-04, 6.0312e-01, 6.6405e-01, 9.8813e-01, 9.4324e-02],\n",
       "         [9.5483e-01, 7.1551e-01, 1.1479e-01, 2.0426e-01, 1.5363e-01,\n",
       "          4.9447e-01, 1.6485e-01, 5.3680e-01, 4.1981e-01, 3.7021e-01,\n",
       "          6.3081e-01, 7.5202e-01, 8.5163e-01, 8.1651e-01, 2.0642e-01,\n",
       "          6.8782e-01, 7.4140e-02, 9.8061e-01, 9.2939e-01, 1.6641e-01],\n",
       "         [8.8598e-01, 1.4416e-01, 5.4074e-03, 1.2813e-01, 2.3649e-01,\n",
       "          9.8125e-01, 3.4946e-01, 4.2981e-01, 5.0331e-01, 8.7759e-01,\n",
       "          3.3935e-01, 3.5122e-01, 1.6095e-01, 5.0283e-01, 7.1376e-01,\n",
       "          4.7569e-01, 1.2126e-02, 8.7780e-01, 4.7162e-01, 2.0409e-01],\n",
       "         [9.5501e-01, 5.3693e-01, 6.2849e-01, 9.3234e-01, 2.0174e-01,\n",
       "          6.9967e-01, 1.1561e-01, 3.1969e-01, 4.7088e-01, 8.8846e-01,\n",
       "          9.2398e-01, 8.2463e-01, 3.2421e-01, 6.7987e-01, 2.8090e-01,\n",
       "          2.7732e-01, 9.6575e-01, 3.5750e-01, 7.7099e-01, 9.1259e-01],\n",
       "         [6.3689e-01, 8.5229e-01, 2.9081e-01, 5.5157e-01, 3.8453e-01,\n",
       "          5.3089e-02, 8.1013e-01, 8.7045e-01, 6.9473e-01, 3.9669e-01,\n",
       "          5.7604e-02, 9.5242e-01, 3.4995e-01, 9.9621e-01, 2.8922e-01,\n",
       "          3.2797e-01, 2.0432e-01, 9.3330e-01, 4.4288e-01, 6.0353e-01],\n",
       "         [4.1265e-01, 4.0836e-01, 1.7948e-01, 7.7343e-01, 6.3877e-01,\n",
       "          7.2132e-02, 8.0099e-01, 4.1460e-02, 2.1588e-01, 1.4664e-01,\n",
       "          1.5222e-01, 4.0491e-01, 6.3900e-01, 3.3631e-01, 1.0655e-01,\n",
       "          4.2115e-01, 8.5530e-01, 5.4978e-01, 5.0170e-01, 5.1275e-01],\n",
       "         [5.7772e-01, 2.9648e-02, 2.8209e-01, 7.9148e-01, 5.8922e-01,\n",
       "          9.1160e-01, 2.0626e-01, 8.5821e-02, 6.8711e-01, 7.5499e-01,\n",
       "          5.3360e-02, 9.2902e-01, 4.0201e-01, 4.0896e-01, 1.1707e-01,\n",
       "          6.2483e-01, 6.7158e-01, 2.1811e-01, 8.9608e-01, 6.3157e-01],\n",
       "         [6.5992e-01, 4.0613e-01, 7.1488e-01, 9.1991e-02, 7.9633e-01,\n",
       "          1.0363e-02, 4.1305e-02, 4.0870e-01, 7.1386e-01, 4.6928e-01,\n",
       "          7.7551e-01, 3.9020e-01, 7.9811e-01, 3.9877e-02, 4.8247e-01,\n",
       "          6.6079e-01, 4.3011e-01, 7.5289e-02, 1.7825e-01, 9.9908e-01],\n",
       "         [9.1653e-01, 7.4320e-02, 7.2001e-02, 3.1285e-01, 7.3317e-01,\n",
       "          3.6459e-01, 1.2280e-01, 7.6536e-01, 2.8583e-01, 1.6855e-01,\n",
       "          4.1290e-01, 9.6970e-01, 8.4516e-02, 5.8430e-01, 7.9399e-01,\n",
       "          3.7369e-01, 2.1476e-01, 7.0958e-01, 6.2546e-02, 2.7034e-02]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Batch Matrix Multiplication => between the last two dimensions of each tensor. \"\"\"\n",
    "batch = 32\n",
    "n = 10\n",
    "m = 20\n",
    "p = 30\n",
    "tensor1 = torch.rand((batch, n, m))\n",
    "tensor2 = torch.rand((batch, m, p))\n",
    "out_bmm = torch.bmm(tensor1, tensor2)\n",
    "\n",
    "print(tensor1.shape)\n",
    "tensor1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3.4172, 3.8610, 3.4128, 4.0864, 4.3969, 3.2427, 2.4913, 5.0018,\n",
      "          4.0682, 3.9401, 3.6040, 4.2782, 3.8998, 3.8991, 3.5839, 3.6537,\n",
      "          4.2483, 2.9701, 3.1290, 4.5038, 3.4652, 3.3697, 2.9554, 3.7689,\n",
      "          3.4784, 2.9475, 3.5563, 2.7185, 3.1320, 3.1868],\n",
      "         [2.8471, 4.6272, 3.6242, 4.8018, 4.5335, 3.2338, 3.8616, 4.8141,\n",
      "          3.8052, 4.2151, 4.9653, 5.0533, 4.2333, 4.2066, 3.7224, 4.5597,\n",
      "          4.1895, 3.0118, 3.3971, 4.6309, 4.1493, 2.9031, 2.6158, 4.5307,\n",
      "          4.1657, 3.4376, 4.7756, 3.0895, 3.2068, 3.5410],\n",
      "         [4.1296, 6.0509, 5.8640, 5.4673, 5.2855, 5.6392, 3.7974, 6.7183,\n",
      "          6.2356, 5.0796, 5.1835, 6.2738, 5.1677, 5.9051, 5.3783, 5.3138,\n",
      "          5.4212, 4.4006, 4.5818, 5.8626, 3.9073, 4.2488, 4.6746, 6.3730,\n",
      "          5.5211, 5.4732, 5.3882, 4.4101, 4.1686, 4.4311],\n",
      "         [5.7404, 6.6648, 6.0872, 6.6425, 7.3778, 5.2485, 4.3733, 8.0382,\n",
      "          5.8612, 6.1575, 5.8222, 6.4290, 6.2461, 6.6716, 5.8081, 5.8750,\n",
      "          5.8788, 4.7651, 5.3134, 6.7028, 5.3989, 5.4599, 4.8853, 6.3637,\n",
      "          5.9350, 5.3523, 5.7444, 5.0124, 5.5457, 4.7985],\n",
      "         [4.5094, 5.6798, 5.4602, 5.3686, 5.1331, 5.0043, 3.0728, 5.6650,\n",
      "          6.0005, 5.2114, 6.3957, 5.9298, 4.7802, 5.1986, 4.9832, 4.8118,\n",
      "          5.3692, 4.4041, 3.9058, 6.3717, 3.3549, 4.9972, 4.4718, 6.1979,\n",
      "          5.0574, 5.0985, 5.0512, 4.4407, 4.8525, 4.2284],\n",
      "         [3.9364, 4.7713, 5.0160, 4.6430, 5.2683, 5.2628, 2.8987, 5.6019,\n",
      "          5.2388, 5.0797, 5.3581, 5.1353, 4.3229, 4.2071, 4.5867, 4.4863,\n",
      "          4.9045, 3.6626, 3.7000, 5.9800, 3.8782, 4.2098, 4.2624, 5.3803,\n",
      "          4.7165, 4.5743, 4.2134, 3.7435, 3.8127, 3.8309],\n",
      "         [3.8033, 5.5415, 4.8654, 4.7767, 4.8621, 5.4998, 3.6582, 6.2387,\n",
      "          5.2738, 4.1163, 4.9998, 5.3384, 4.3494, 4.8217, 5.1766, 4.7793,\n",
      "          4.6375, 4.2836, 4.4485, 5.2120, 4.6781, 4.4420, 4.8846, 5.8362,\n",
      "          5.3995, 5.4590, 4.7534, 3.9104, 4.3274, 4.0547],\n",
      "         [2.2713, 3.8322, 3.4042, 3.0307, 3.1921, 3.3326, 2.1227, 4.0402,\n",
      "          2.6354, 3.3995, 3.4181, 4.2895, 3.7973, 3.4546, 3.6907, 4.1433,\n",
      "          3.1354, 2.8789, 3.0325, 3.6947, 3.5154, 3.1719, 2.4382, 3.8401,\n",
      "          3.7007, 3.2960, 3.1188, 2.5650, 3.5648, 2.4907],\n",
      "         [3.7801, 6.0668, 5.0857, 5.8138, 5.9042, 4.5614, 3.9905, 6.6861,\n",
      "          4.3591, 5.8018, 5.8241, 6.0378, 5.5810, 5.6598, 5.3532, 6.0827,\n",
      "          5.6341, 4.6829, 4.6073, 6.1390, 4.8475, 4.2864, 3.2590, 5.9503,\n",
      "          6.0596, 5.1076, 5.8095, 4.1348, 4.7260, 4.7537],\n",
      "         [4.7483, 5.1912, 4.9392, 5.1518, 5.3402, 4.7839, 2.9579, 7.0984,\n",
      "          6.1245, 5.5191, 5.2613, 5.6859, 4.7619, 5.6836, 5.9562, 4.7798,\n",
      "          5.5574, 4.3872, 4.7681, 5.5579, 4.2360, 5.1888, 4.7435, 5.7103,\n",
      "          5.0725, 6.0066, 4.5555, 4.7710, 5.6776, 4.2277]],\n",
      "\n",
      "        [[4.4557, 3.5196, 3.9820, 4.6200, 4.0820, 5.4896, 5.0177, 3.4754,\n",
      "          3.1302, 5.0410, 4.5174, 5.2229, 4.3288, 3.7244, 4.9496, 5.1203,\n",
      "          4.8086, 4.8369, 3.7067, 3.9928, 5.1182, 4.9158, 4.8373, 3.9364,\n",
      "          4.9024, 3.6774, 4.1518, 4.4802, 4.4678, 4.6540],\n",
      "         [4.9824, 4.2680, 4.8621, 4.2942, 4.3172, 4.9841, 5.6951, 3.6327,\n",
      "          4.6482, 5.2288, 4.3806, 5.4462, 4.6049, 3.9990, 4.1757, 5.5218,\n",
      "          4.4844, 5.0650, 3.9175, 4.1153, 4.9309, 4.4554, 4.3252, 4.1971,\n",
      "          4.9305, 3.7724, 3.7029, 4.1180, 4.3601, 4.8406],\n",
      "         [5.3036, 4.3595, 5.5812, 5.3828, 4.6052, 5.7706, 6.3841, 4.0289,\n",
      "          4.5588, 5.6886, 4.4498, 6.0121, 5.5988, 5.0502, 4.9384, 5.7820,\n",
      "          5.7448, 5.7005, 4.7725, 5.5026, 4.8233, 5.2439, 4.9004, 4.6670,\n",
      "          4.5034, 4.7569, 4.7677, 4.9086, 5.0608, 5.2243],\n",
      "         [4.0091, 4.1668, 5.2972, 4.6219, 3.6024, 4.9234, 4.7670, 3.6747,\n",
      "          4.2094, 4.6736, 3.8875, 4.4694, 4.3484, 4.1460, 4.3227, 4.8266,\n",
      "          5.1607, 4.8599, 3.7695, 4.3920, 3.9906, 4.4301, 3.9242, 4.0608,\n",
      "          3.2809, 4.4647, 4.1412, 3.9121, 4.5254, 4.9650],\n",
      "         [6.1741, 4.9913, 6.6120, 5.9521, 4.8086, 6.7741, 7.2121, 4.3121,\n",
      "          4.8706, 6.7386, 5.8460, 6.7840, 6.2590, 5.5041, 5.5175, 6.9887,\n",
      "          6.9100, 6.3921, 5.5617, 5.9828, 6.0074, 6.2351, 6.7245, 5.7099,\n",
      "          6.2530, 5.5625, 5.4205, 6.5748, 6.1360, 6.3796],\n",
      "         [5.3479, 4.3615, 5.6112, 5.1838, 4.8543, 5.2733, 5.8743, 4.2753,\n",
      "          4.7811, 5.9166, 4.6297, 6.2836, 4.7145, 4.7044, 5.4537, 5.8580,\n",
      "          5.7020, 5.7667, 4.9460, 5.1644, 5.7176, 5.7992, 5.3105, 5.1270,\n",
      "          5.2828, 4.6504, 5.4023, 4.8517, 5.1456, 5.5279],\n",
      "         [3.5287, 3.2182, 4.2270, 3.8896, 3.8786, 3.8956, 4.7425, 2.8477,\n",
      "          4.0307, 4.7226, 3.6800, 4.6953, 4.0432, 3.5978, 4.1077, 4.8843,\n",
      "          3.5292, 4.3766, 4.0916, 4.3800, 4.1368, 4.3600, 4.5249, 3.7223,\n",
      "          3.8526, 3.0735, 3.3458, 3.9882, 4.0035, 4.6337],\n",
      "         [4.2743, 3.7596, 4.9298, 4.9548, 4.5163, 5.3250, 6.1299, 3.9619,\n",
      "          4.6281, 5.8638, 4.9847, 5.2255, 4.5268, 4.2435, 5.0573, 6.3357,\n",
      "          5.3957, 5.2821, 4.9087, 5.1935, 5.1978, 4.9632, 4.6918, 4.6733,\n",
      "          4.4535, 4.5025, 4.0592, 4.6187, 4.9010, 5.8537],\n",
      "         [4.3207, 3.9630, 4.6938, 3.9851, 4.3217, 4.6214, 4.8186, 2.9810,\n",
      "          4.1461, 4.4852, 4.6450, 5.7120, 5.2308, 3.8812, 4.8766, 4.9413,\n",
      "          5.2394, 4.1432, 4.2085, 4.5755, 4.2197, 5.2307, 4.9261, 4.5192,\n",
      "          4.5426, 4.3157, 3.1704, 4.9140, 4.8222, 5.2548],\n",
      "         [3.4203, 3.5980, 4.4275, 4.4692, 3.7577, 4.6904, 4.0654, 2.8614,\n",
      "          3.4902, 4.1852, 3.5592, 4.0815, 3.4278, 3.3669, 4.6045, 3.8853,\n",
      "          5.2867, 3.8854, 2.4549, 3.4139, 4.3790, 4.3969, 4.1343, 3.4253,\n",
      "          3.6880, 3.8974, 3.4819, 3.2991, 3.5847, 4.5455]]])\n"
     ]
    }
   ],
   "source": [
    "print(out_bmm[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([5, 5]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Broadcasting \"\"\"\n",
    "x1 = torch.rand((5, 5))\n",
    "x2 = torch.ones((1, 5))\n",
    "bd = (x1 - x2)  \n",
    "# Element-wise exponentiation\n",
    "be = (x1**x2)  \n",
    "\n",
    "bd.shape, be.shape "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#BF66F2 \">  => Tensor operations: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 62,  4])\n",
      "tensor(79)\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "tensor([[ 6,  8],\n",
      "        [10, 12]])\n"
     ]
    }
   ],
   "source": [
    "sum_x = torch.sum(x, dim=0)\n",
    "\n",
    "tx = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "sum_tx = torch.sum(tx, dim=0)\n",
    "\n",
    "print(x)\n",
    "print(sum_x)\n",
    "print(tx)\n",
    "print(sum_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor(62),\n",
       "indices=tensor(1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max(dim=0)\n",
    "#or ...\n",
    "#values, indices = torch.max(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor(4),\n",
       "indices=tensor(2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.min(dim=0) \n",
    "#or ...\n",
    "#values, indices = torch.min(x, dim=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_x = torch.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the max / min value\n",
    "in1 = torch.argmax(x, dim=0)  \n",
    "in2 = torch.argmin(x, dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = torch.mean(x.float(), dim=0)  #x must be float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise comparison\n",
    "comp = torch.eq(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True,  True,  True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor([1, 0, 1, 1, 1], dtype=torch.bool)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([31, 39, 91])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_y, indices = torch.sort(y, dim=0, descending=False)\n",
    "sorted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cla = torch.clamp(tt, min=0)\n",
    "cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Any and All \"\"\"\n",
    "anyx = torch.any(x)  # any element is True or non-zero?\n",
    "allx = torch.all(x)  # all elements are True or non-zero?\n",
    "anyx, allx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#BF66F2 \">  => Tensor Indexing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5357, 0.2601, 0.3347, 0.5389, 0.9592, 0.0840, 0.5363, 0.5458, 0.7332,\n",
       "         0.5033, 0.4220, 0.7546, 0.8533, 0.8763, 0.2569, 0.8281, 0.9374, 0.5556,\n",
       "         0.6928, 0.6930, 0.7050, 0.3199, 0.1857, 0.3942, 0.0534],\n",
       "        [0.0603, 0.8431, 0.8331, 0.7032, 0.1361, 0.1627, 0.0261, 0.3157, 0.4291,\n",
       "         0.6073, 0.2645, 0.4724, 0.8195, 0.9654, 0.8667, 0.0721, 0.8472, 0.9209,\n",
       "         0.7602, 0.8365, 0.5832, 0.5591, 0.7055, 0.7231, 0.6286],\n",
       "        [0.1058, 0.6518, 0.6896, 0.3820, 0.8234, 0.9171, 0.4418, 0.5996, 0.6889,\n",
       "         0.0093, 0.3729, 0.5428, 0.0521, 0.3721, 0.4343, 0.7559, 0.4039, 0.6153,\n",
       "         0.3644, 0.6034, 0.7974, 0.6528, 0.6158, 0.6581, 0.6424],\n",
       "        [0.5645, 0.1624, 0.6306, 0.5633, 0.9614, 0.1309, 0.0600, 0.8707, 0.1207,\n",
       "         0.3257, 0.6645, 0.6538, 0.6423, 0.7665, 0.1972, 0.1321, 0.6533, 0.1512,\n",
       "         0.4793, 0.3954, 0.4870, 0.9543, 0.8805, 0.5194, 0.9795],\n",
       "        [0.2722, 0.3399, 0.0431, 0.1068, 0.3334, 0.8692, 0.5377, 0.4068, 0.6149,\n",
       "         0.9955, 0.6011, 0.6407, 0.4562, 0.1224, 0.8378, 0.8545, 0.9803, 0.5874,\n",
       "         0.7448, 0.0679, 0.7714, 0.9435, 0.5003, 0.0547, 0.0263],\n",
       "        [0.4157, 0.2844, 0.3297, 0.7720, 0.7204, 0.1153, 0.6088, 0.4528, 0.2875,\n",
       "         0.1070, 0.1557, 0.8575, 0.5091, 0.5574, 0.6917, 0.1833, 0.0722, 0.1307,\n",
       "         0.6723, 0.0816, 0.4677, 0.9324, 0.6160, 0.6436, 0.8885],\n",
       "        [0.4700, 0.4953, 0.5387, 0.4149, 0.5511, 0.4035, 0.7898, 0.2746, 0.9900,\n",
       "         0.2658, 0.6429, 0.0145, 0.3158, 0.7934, 0.2855, 0.7879, 0.6717, 0.5861,\n",
       "         0.5331, 0.0941, 0.1360, 0.8212, 0.7996, 0.2075, 0.1148],\n",
       "        [0.9748, 0.9341, 0.2206, 0.7548, 0.5034, 0.3034, 0.7972, 0.0331, 0.3962,\n",
       "         0.7049, 0.1009, 0.8638, 0.0168, 0.9032, 0.2640, 0.5277, 0.1478, 0.4015,\n",
       "         0.7338, 0.1564, 0.0054, 0.4417, 0.5826, 0.7772, 0.5526],\n",
       "        [0.4911, 0.3420, 0.8487, 0.0211, 0.6119, 0.4494, 0.5465, 0.8109, 0.2551,\n",
       "         0.6646, 0.4507, 0.3019, 0.2704, 0.6076, 0.5202, 0.5565, 0.9504, 0.2550,\n",
       "         0.7851, 0.2245, 0.4813, 0.7061, 0.9643, 0.8887, 0.7212],\n",
       "        [0.3704, 0.8940, 0.4738, 0.8435, 0.8508, 0.6248, 0.0383, 0.1369, 0.3367,\n",
       "         0.6231, 0.2026, 0.3862, 0.0111, 0.2861, 0.5007, 0.1480, 0.4397, 0.6630,\n",
       "         0.6639, 0.2823, 0.1018, 0.9016, 0.1679, 0.8007, 0.4841]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "features = 25\n",
    "te = torch.rand((batch_size, features))\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(te[0].shape)\n",
    "print(te[:, 0].shape)\n",
    "print(te[2, 0:10].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "ttt = torch.arange(10)\n",
    "indices = [2, 5, 8]\n",
    "print(ttt[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1364, 0.5146, 0.8312, 0.9020, 0.8082],\n",
       "         [0.5645, 0.9798, 0.1588, 0.0601, 0.6598],\n",
       "         [0.9875, 0.0044, 0.0702, 0.1203, 0.3766]]),\n",
       " tensor([1, 0]),\n",
       " tensor([4, 0]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((3, 5))\n",
    "rows = torch.tensor([1, 0])\n",
    "cols = torch.tensor([4, 0])\n",
    "x, rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6598, 0.1364])\n"
     ]
    }
   ],
   "source": [
    "print(x[rows, cols])    # second row/fifth column elem + first row/first column elem => row 1 and 0 + column 4 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 9])\n",
      "tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "print(x)\n",
    "print(x[(x < 2) | (x > 8)]) \n",
    "print(x[x.remainder(2) == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  4,  6,  8, 10,  6,  7,  8,  9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(x > 5, x, x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of dimensions of tor is  1\n",
      "num of element of tor is  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tor = torch.tensor([0, 0, 1, 2, 2, 3, 4]).unique()\n",
    "print(\"num of dimensions of tor is \", tor.ndimension())\n",
    "print(\"num of element of tor is \", tor.numel()) \n",
    "tor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#BF66F2 \">  => Tensor Changing shape </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]]),\n",
       " tensor([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Reshape: => View and reshape do the same action ...\n",
    "But the latter act on contiguous tensors => meaning that if the tensor it is not stored contiguously in memory \n",
    "it makes a copy of the tensor to make it contiguously stored => leading to performance loss in general in general !\n",
    "\"\"\"\n",
    "x = torch.arange(9)\n",
    "x_3x3_v = x.view(3, 3)\n",
    "x_3x3_r = x.reshape(3, 3)\n",
    "x_3x3_v, x_3x3_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#BF66F2\"> Common errors: </h3>\n",
    "<div style=\"margin-top: -17px;\">\n",
    "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). <br>\n",
    ".reshape(...) should be used instead. <br>\n",
    "\n",
    "PyTorch is unable to create a new tensor with the desired shape because the memory layout of y is not contiguous. <br>\n",
    "Now the tensor is stored in memory as it was stored [0, 1, 2, ... 8], whereas now it's [0, 3, 6, 1, 4, 7, 2, 5, 8] <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#BF66F2\"> Recap: </h3>\n",
    "<div style=\"margin-top: -22px;\">\n",
    "A tensor is said to be contiguous if its data is stored in a contiguous block of memory,  <br>\n",
    "which means that the elements of the tensor are stored in a linear sequence without any gaps or padding. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is contiguous?\n",
      "False\n",
      "tensor([0, 3, 6, 1, 4, 7, 2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check continuity. \n",
    "After y.t(), PyTorch creates a new tensor y that is a transposed version of x_3x3_v. \n",
    "However, the transpose operation changes the memory layout of the tensor, which means that the elements \n",
    "of y are no longer stored in a contiguous block of memory.\n",
    "\"\"\"\n",
    "y = x_3x3_v.t()\n",
    "print(\"is contiguous?\")\n",
    "print(y.is_contiguous())\n",
    "#y.view(9) Error! But calling .contiguous() before view and it works\n",
    "print(y.contiguous().view(9)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2886, 0.2156, 0.0916, 0.1131, 0.9295],\n",
      "        [0.0850, 0.1922, 0.7210, 0.6702, 0.7830]])\n",
      "tensor([[0.4782, 0.6378, 0.9850, 0.7037, 0.0858],\n",
      "        [0.6427, 0.9730, 0.6960, 0.3944, 0.1554]])\n",
      "\n",
      "tensor([[0.2886, 0.2156, 0.0916, 0.1131, 0.9295],\n",
      "        [0.0850, 0.1922, 0.7210, 0.6702, 0.7830],\n",
      "        [0.4782, 0.6378, 0.9850, 0.7037, 0.0858],\n",
      "        [0.6427, 0.9730, 0.6960, 0.3944, 0.1554]])\n",
      "\n",
      "tensor([[0.2886, 0.2156, 0.0916, 0.1131, 0.9295, 0.4782, 0.6378, 0.9850, 0.7037,\n",
      "         0.0858],\n",
      "        [0.0850, 0.1922, 0.7210, 0.6702, 0.7830, 0.6427, 0.9730, 0.6960, 0.3944,\n",
      "         0.1554]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(2, 5)\n",
    "x2 = torch.rand(2, 5)\n",
    "\n",
    "cat1 = torch.cat((x1, x2), dim=0)\n",
    "cat2 = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print()\n",
    "print(cat1)\n",
    "print()\n",
    "print(cat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2886, 0.2156, 0.0916, 0.1131, 0.9295, 0.0850, 0.1922, 0.7210, 0.6702,\n",
       "        0.7830])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unroll x1 into one long vector with 10 elements\n",
    "unro1 = x1.view(-1)\n",
    "unro1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2106, 0.9901, 0.5215, 0.7674, 0.7645],\n",
       "          [0.1227, 0.5335, 0.0043, 0.4914, 0.0064]],\n",
       " \n",
       "         [[0.1640, 0.6757, 0.2341, 0.1470, 0.9348],\n",
       "          [0.7159, 0.4676, 0.4251, 0.5388, 0.2570]]]),\n",
       " tensor([[0.2106, 0.9901, 0.5215, 0.7674, 0.7645, 0.1227, 0.5335, 0.0043, 0.4914,\n",
       "          0.0064],\n",
       "         [0.1640, 0.6757, 0.2341, 0.1470, 0.9348, 0.7159, 0.4676, 0.4251, 0.5388,\n",
       "          0.2570]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 64\n",
    "x = torch.rand((batch, 2, 5))\n",
    "unro2 = x.view(batch, -1)\n",
    "\n",
    "x[:2], unro2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2106, 0.1227],\n",
       "         [0.9901, 0.5335],\n",
       "         [0.5215, 0.0043],\n",
       "         [0.7674, 0.4914],\n",
       "         [0.7645, 0.0064]],\n",
       "\n",
       "        [[0.1640, 0.7159],\n",
       "         [0.6757, 0.4676],\n",
       "         [0.2341, 0.4251],\n",
       "         [0.1470, 0.5388],\n",
       "         [0.9348, 0.2570]],\n",
       "\n",
       "        [[0.7083, 0.6712],\n",
       "         [0.2961, 0.4143],\n",
       "         [0.3482, 0.4635],\n",
       "         [0.7602, 0.1377],\n",
       "         [0.6553, 0.8822]],\n",
       "\n",
       "        [[0.5195, 0.9047],\n",
       "         [0.9322, 0.2550],\n",
       "         [0.8232, 0.9111],\n",
       "         [0.6901, 0.3973],\n",
       "         [0.3389, 0.2164]],\n",
       "\n",
       "        [[0.7395, 0.3150],\n",
       "         [0.0132, 0.8828],\n",
       "         [0.5130, 0.3999],\n",
       "         [0.0814, 0.1520],\n",
       "         [0.3744, 0.1290]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Permute\n",
    "perm = x.permute(0, 2, 1)\n",
    "perm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 5])\n",
      "torch.Size([64, 1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1227, 0.5335, 0.0043, 0.4914, 0.0064]],\n",
       " \n",
       "         [[0.7159, 0.4676, 0.4251, 0.5388, 0.2570]],\n",
       " \n",
       "         [[0.6712, 0.4143, 0.4635, 0.1377, 0.8822]],\n",
       " \n",
       "         [[0.9047, 0.2550, 0.9111, 0.3973, 0.2164]],\n",
       " \n",
       "         [[0.3150, 0.8828, 0.3999, 0.1520, 0.1290]],\n",
       " \n",
       "         [[0.7984, 0.0282, 0.2919, 0.1347, 0.6902]],\n",
       " \n",
       "         [[0.4025, 0.3768, 0.5137, 0.3990, 0.3248]],\n",
       " \n",
       "         [[0.9057, 0.0207, 0.1701, 0.5858, 0.1975]],\n",
       " \n",
       "         [[0.4497, 0.0796, 0.7945, 0.2010, 0.3226]],\n",
       " \n",
       "         [[0.3809, 0.0394, 0.8266, 0.1538, 0.5597]],\n",
       " \n",
       "         [[0.4836, 0.8849, 0.2649, 0.9121, 0.0803]],\n",
       " \n",
       "         [[0.3814, 0.3743, 0.3280, 0.2800, 0.1398]],\n",
       " \n",
       "         [[0.6580, 0.0643, 0.6374, 0.4317, 0.0315]],\n",
       " \n",
       "         [[0.2898, 0.1485, 0.9488, 0.1526, 0.0411]],\n",
       " \n",
       "         [[0.4615, 0.2853, 0.0089, 0.9115, 0.6696]],\n",
       " \n",
       "         [[0.4493, 0.2971, 0.9602, 0.8934, 0.2660]],\n",
       " \n",
       "         [[0.6981, 0.3945, 0.1486, 0.4506, 0.5218]],\n",
       " \n",
       "         [[0.2006, 0.7963, 0.6755, 0.4820, 0.8380]],\n",
       " \n",
       "         [[0.2643, 0.6081, 0.9020, 0.5221, 0.9563]],\n",
       " \n",
       "         [[0.1451, 0.6639, 0.8022, 0.2711, 0.7108]],\n",
       " \n",
       "         [[0.7315, 0.3185, 0.2069, 0.6467, 0.5069]],\n",
       " \n",
       "         [[0.2982, 0.0415, 0.6892, 0.0577, 0.6295]],\n",
       " \n",
       "         [[0.2529, 0.8579, 0.0296, 0.2049, 0.8020]],\n",
       " \n",
       "         [[0.0874, 0.2771, 0.2740, 0.0654, 0.9001]],\n",
       " \n",
       "         [[0.8914, 0.8988, 0.0230, 0.6392, 0.7349]],\n",
       " \n",
       "         [[0.8237, 0.8078, 0.9791, 0.5436, 0.9748]],\n",
       " \n",
       "         [[0.1610, 0.4923, 0.4397, 0.5139, 0.0508]],\n",
       " \n",
       "         [[0.6115, 0.8653, 0.5280, 0.4849, 0.2252]],\n",
       " \n",
       "         [[0.6267, 0.9387, 0.0782, 0.2357, 0.1857]],\n",
       " \n",
       "         [[0.9875, 0.9225, 0.3970, 0.6560, 0.5165]],\n",
       " \n",
       "         [[0.3807, 0.1815, 0.8503, 0.4877, 0.4470]],\n",
       " \n",
       "         [[0.7708, 0.4182, 0.1740, 0.2277, 0.2477]],\n",
       " \n",
       "         [[0.6702, 0.6935, 0.1514, 0.7925, 0.4349]],\n",
       " \n",
       "         [[0.0774, 0.6029, 0.6430, 0.1047, 0.6379]],\n",
       " \n",
       "         [[0.7107, 0.7968, 0.4259, 0.0162, 0.7680]],\n",
       " \n",
       "         [[0.1836, 0.6136, 0.5735, 0.5052, 0.3291]],\n",
       " \n",
       "         [[0.1788, 0.2367, 0.7576, 0.6522, 0.5265]],\n",
       " \n",
       "         [[0.0300, 0.5163, 0.1292, 0.2921, 0.1337]],\n",
       " \n",
       "         [[0.1867, 0.7491, 0.4474, 0.3856, 0.1845]],\n",
       " \n",
       "         [[0.3587, 0.3194, 0.3773, 0.6063, 0.5511]],\n",
       " \n",
       "         [[0.9749, 0.0668, 0.3171, 0.2220, 0.4282]],\n",
       " \n",
       "         [[0.7302, 0.5984, 0.6674, 0.4088, 0.2869]],\n",
       " \n",
       "         [[0.5356, 0.8781, 0.1671, 0.7126, 0.9108]],\n",
       " \n",
       "         [[0.2247, 0.1425, 0.8085, 0.6238, 0.7740]],\n",
       " \n",
       "         [[0.9305, 0.7708, 0.2113, 0.3246, 0.3727]],\n",
       " \n",
       "         [[0.4280, 0.4489, 0.2043, 0.6600, 0.4152]],\n",
       " \n",
       "         [[0.3367, 0.5365, 0.5259, 0.7687, 0.2421]],\n",
       " \n",
       "         [[0.5488, 0.1075, 0.1603, 0.2419, 0.7201]],\n",
       " \n",
       "         [[0.6254, 0.7557, 0.0794, 0.9757, 0.4448]],\n",
       " \n",
       "         [[0.8729, 0.2779, 0.2923, 0.1731, 0.5254]],\n",
       " \n",
       "         [[0.3808, 0.8090, 0.3760, 0.1161, 0.3928]],\n",
       " \n",
       "         [[0.5884, 0.0994, 0.3524, 0.7399, 0.6863]],\n",
       " \n",
       "         [[0.7154, 0.6194, 0.2625, 0.7525, 0.7977]],\n",
       " \n",
       "         [[0.4410, 0.7182, 0.3486, 0.7439, 0.3132]],\n",
       " \n",
       "         [[0.6018, 0.2225, 0.9160, 0.7459, 0.0441]],\n",
       " \n",
       "         [[0.5602, 0.8124, 0.4259, 0.3663, 0.9297]],\n",
       " \n",
       "         [[0.0449, 0.7230, 0.6758, 0.7877, 0.2111]],\n",
       " \n",
       "         [[0.8574, 0.1791, 0.1246, 0.0847, 0.4666]],\n",
       " \n",
       "         [[0.7475, 0.3915, 0.7175, 0.6014, 0.9572]],\n",
       " \n",
       "         [[0.0042, 0.1689, 0.1004, 0.8365, 0.8741]],\n",
       " \n",
       "         [[0.5392, 0.8107, 0.4842, 0.6263, 0.4366]],\n",
       " \n",
       "         [[0.1431, 0.4428, 0.2970, 0.4860, 0.2703]],\n",
       " \n",
       "         [[0.1295, 0.3736, 0.8255, 0.2934, 0.9003]],\n",
       " \n",
       "         [[0.7997, 0.5852, 0.2551, 0.6473, 0.0412]]]),)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Split x last dimension into chunks of 2 the last dimension \"\"\" \n",
    "chu = torch.chunk(x, chunks=2, dim=1)\n",
    "print(chu[0].shape)\n",
    "print(chu[1].shape)\n",
    "\n",
    "chu[1:2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#BF66F2\"> Recap: Unsqueeze</h3>\n",
    "<div style=\"margin-top: -22px;\">\n",
    "Adding a new dimension to a tensor (inverse of squeeze)! \n",
    "\n",
    "- It adds a dimension of size 1 at the position given by the dim argument. <br>\n",
    "- The number of dimensions of the tensor increases by 1 after applying unsqueeze. <br>\n",
    "- The new dimension is inserted at the location specified by dim. All subsequent dimensions are shifted. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([10, 1])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7],\n",
       "         [8],\n",
       "         [9]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Unsqueeze \"\"\" \n",
    "x = torch.arange(10)\n",
    "print(x.unsqueeze(0).shape)  \n",
    "print(x.unsqueeze(1).shape)  \n",
    "print()\n",
    "\n",
    "x, x.unsqueeze(0), x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]]),\n",
       " tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torx = torch.arange(10).unsqueeze(0).unsqueeze(1)\n",
    "sqsq = torx.squeeze(1)\n",
    "torx, sqsq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
