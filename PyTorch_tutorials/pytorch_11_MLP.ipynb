{"cells":[{"cell_type":"markdown","metadata":{},"source":["<div style=\"line-height:0.5\">\n","<h1 style=\"color:#BF66F2 \">  Multi-layer Perceptron in PyTorch </h1>\n","<h4> 3 examples with various MLP implementations. </h4>\n","<span style=\"display: inline-block;\">\n","    <h3 style=\"color: lightblue; display: inline;\">Keywords:</h3>\n","    Dataset creation + kaiming_uniform_ + xavier_uniform_ + nn.Dropout(dropout_rate)\n","</span>\n","</div>"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1685988951613,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"yV3HoLr0imUs"},"outputs":[],"source":["import warnings \n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision.datasets import MNIST\n","import torchvision.transforms as transforms\n","from torch.nn.init import xavier_uniform_, kaiming_uniform_\n","from torch.utils.data import Dataset, DataLoader, random_split"]},{"cell_type":"markdown","metadata":{},"source":["<h3 style=\"color:#BF66F2 \">  Recap: </h3>\n","<div style=\"margin-top: -25px;\">\n","Traditional Multi-Layer Perceptrons (MLPs) are composed primarily of torch Linear (fully connected or dense) layers.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<h2 style=\"color:#BF66F2 \">  <u> Example #1 </u> </h2>\n","Based on the 'ionosphere' dataset"]},{"cell_type":"markdown","metadata":{},"source":["<h3 style=\"color:#BF66F2 \">  Note: </h3>\n","<div style=\"margin-top: -25px;\">\n","Creating a custom CSVDataset class (as a subclass of torch.utils.data.Dataset) is beneficial for:\n","\n","- Encapsulating all pre-processing preliminar steps like normalization, encoding categorical variables, etc.\n","- Loading efficienctly and process data in batches to avoid loading the entire dataset into memory. \n","- Controlling the data loading and transformation process, in a flexibile manner such as implementing on-the-fly data augmentation, dealing with missing data, or handling imbalanced datasets.\n","- Ensuring compatibility with PyTorch's data loading utilities, such as DataLoader, which handles batching, shuffling, and multi-process data loading."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E47vFtcwx2_d"},"outputs":[],"source":["class CSVDataset(Dataset):\n","    \"\"\" Custom PyTorch dataset class for working with CSV data.\n","\n","    Args:\n","        Path to the CSV file [str]\n","\n","    Attributes:\n","        - X : Features data [numpy.ndarray]\n","        - y : Labels data [numpy.ndarray]\n","\n","    Methods:\n","        - __len__(self): Get the number of samples in the dataset.\n","        - __getitem__(self, idx): Get a data sample (features and label) at a specified index.\n","        - get_splits(self, n_test=0.33): Split the dataset into training and testing sets.\n","\n","    \"\"\"    \n","    def __init__(self, path):\n","        \"\"\" Initialize the dataset by loading and preprocessing data from a CSV file.\n","\n","        Args:\n","            Path to the CSV file [str]\n","        \"\"\"\n","        # Read data from a CSV file into a DataFrame\n","        df = pd.read_csv(path, header=None)\n","        \n","        ##### Get features (X) and labels (y) from the DataFrame\n","        # Extract all columns except the last one as features\n","        self.X = df.values[:, :-1]  \n","        # Extract the last column as labels\n","        self.y = df.values[:, -1]    \n","        \n","        # Convert feature data type to float32\n","        self.X = self.X.astype('float32')\n","        # Encode labels using LabelEncoder to convert them into numerical values\n","        self.y = LabelEncoder().fit_transform(self.y)\n","        # Convert label data type to float32\n","        self.y = self.y.astype('float32')\n","        \n","        # Reshape labels to be a column vector\n","        self.y = self.y.reshape((len(self.y), 1))\n","\n","    def __len__(self):\n","        \"\"\" Get the number of samples in the dataset. \"\"\"\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        \"\"\" Get a data sample (features and label) at a specified index.\n","\n","        Args:\n","            Index of the sample to retrieve [int]\n","\n","        Returns:\n","            Features and the corresponding label [list]\n","        \"\"\"        \n","        return [self.X[idx], self.y[idx]]\n","\n","    def get_splits(self, n_test=0.33):\n","        \"\"\" Split the dataset (without using sklearn) into training and testing sets, according to the size received. \n","\n","        Args:\n","            Proportion of the dataset to include in the test split [float]\n","\n","        Returns:\n","            Training and testing datasets [tuple]\n","        \"\"\"        \n","        # Determine sizes\n","        test_size = round(n_test * len(self.X))\n","        train_size = len(self.X) - test_size\n","        # Split\n","        return random_split(self, [train_size, test_size])                       "]},{"cell_type":"markdown","metadata":{},"source":["<h3 style=\"color:#BF66F2 \">  Recap: </h3>\n","<div style=\"margin-top: -25px;\">\n","\n","The 'kaiming_uniform_' and the 'xavier_uniform_' are 2 weight initialization methods. <br>\n","The first is suitable for layers that use the Rectified Linear Unit (ReLU) activation function. <br>\n","The latter is used for layers that use sigmoid or hyperbolic tangent (tanh) activation functions. <br>\n","\n","Setting initial values for the weights: <br>\n","- Favouring training convergence \n","- Avoiding vanishing or exploding gradients\n","\n","1. Kaiming uniform (He) initialization <br> \n","\n","$std = \\sqrt{2 / (fan\\_in + a)}$\n","\n","2. Xavier uniform (Glorot) initialization <br>\n","\n","$std = \\sqrt{2 / (fan\\_in + fan\\_out)}$\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFIJNb7oydFm"},"outputs":[],"source":["class MLP(nn.Module):\n","    \"\"\" Multi-Layer Perceptron (MLP) Neural Network model.\n","\n","    Args:\n","        Number of input features [int]\n","\n","    Attributes:\n","        - First hidden layer with 10 units [nn.Linear]\n","        - ReLU activation function for the first hidden layer [nn.ReLU]\n","        - Second hidden layer with 8 units [nn.Linear]\n","        - ReLU activation function for the second hidden layer [nn.ReLU]\n","        - Output layer with 1 unit [nn.Linear]\n","        - Sigmoid activation function for the output layer [nn.Sigmoid]\n","    \"\"\"    \n","    def __init__(self, n_inputs):\n","        super(MLP, self).__init__()\n","        #### First hidden layer with 10 units\n","        self.hidden1 = nn.Linear(n_inputs, 10)\n","        # Initialize the weights of the first hidden layer using Kaiming (He) initialization\n","        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n","        # Define the ReLU activation function for the first hidden layer\n","        self.act1 = nn.ReLU()\n","        \n","        #### Second hidden layer\n","        self.hidden2 = nn.Linear(10, 8)\n","        # Initialize the weights of the second hidden layer using Kaiming (He) initialization\n","        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n","        self.act2 = nn.ReLU()\n","        \n","        #### Third hidden layer and output\n","        self.hidden3 = nn.Linear(8, 1)\n","        xavier_uniform_(self.hidden3.weight)\n","        # Initialize the weights of the output layer using Xavier (Glorot) initialization\n","        self.act3 = nn.Sigmoid()\n","\n","    def forward(self, X):\n","        \"\"\" Forward pass to propagate outputs through the MLP.\n","\n","        Args:\n","            Input tensor with shape (batch_size, n_inputs)\n","\n","        Details: \n","            - Apply the first hidden layer\n","            - Apply ReLU activation to the first hidden layer output\n","            - Apply the second hidden layer\n","            - Apply ReLU activation to the second hidden layer output\n","            - Apply the output layer\n","            - Apply Sigmoid activation to the output layer output\n","        \n","        Returns:\n","            Output tensor with shape (batch_size, 1)\\\\\n","            It represents the network's predictions for each input sample.\n","        \"\"\"\n","        X = self.hidden1(X)  \n","        X = self.act1(X)     \n","        X = self.hidden2(X)  \n","        X = self.act2(X)     \n","        X = self.hidden3(X)  \n","        X = self.act3(X)     \n","        return X"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1041,"status":"ok","timestamp":1685989008177,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"64cVqIYzlIPb","outputId":"0759c8f2-9dc8-4c33-e8bd-62a3111bae86"},"outputs":[],"source":["def prepare_data(path):\n","    \"\"\" Prepare the dataset. \"\"\" \n","    # Create a dataset object from a CSV file\n","    dataset = CSVDataset(path)\n","    # Split the dataset into training and testing sets\n","    train, test = dataset.get_splits()\n","    ## Prepare data loaders for training and testing sets\n","    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n","    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n","    \n","    return train_dl, test_dl\n","\n","def train_model(train_dl, model):\n","    \"\"\" Perform training for a given train dataloader on the received model. \"\"\"\n","    # Define the loss function (Binary Cross Entropy)\n","    criterion = nn.BCELoss()\n","    # Define the optimizer (Stochastic Gradient Descent)\n","    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","    # For all epochs and all mini-batches\n","    for epoch in range(100):\n","        for i, (inputs, targets) in enumerate(train_dl):\n","            # Clear the gradients\n","            optimizer.zero_grad()\n","            # Compute the model output\n","            yhat = model(inputs)\n","            # Calculate loss\n","            loss = criterion(yhat, targets)\n","            # Assign credit \n","            loss.backward()\n","            # Ipdate model weights\n","            optimizer.step()\n","\n","def evaluate_model(test_dl, model):\n","    \"\"\" Calculate accuracy of the model, given some unseen samples. \n","    \n","    Details: \n","        - detach() creates a tensor (from the computation graph) that shares storage with 'yhat'\\\\\n","        It will not have its gradients computed during backpropagation.\\\\\n","        Detaching the tensor is crucial when dealing with non-leaf tensors\\\\\n","        (those that are not directly resulting from .forward() computations but are intermediary nodes in the computational graph)\\\\ \n","        to avoid PyTorch throwing an error related to trying to access a non-leaf tensor's values! \n","    \"\"\"\n","    predictions, actuals = list(), list()\n","    for i, (inputs, targets) in enumerate(test_dl):\n","        # Evaluate the model on the test set\n","        yhat = model(inputs)\n","        ## Retrieve numpy arrays\n","        yhat = yhat.detach().numpy()\n","        actual = targets.numpy()\n","        # Reshape\n","        actual = actual.reshape((len(actual), 1))\n","        # Round to class values\n","        yhat = yhat.round()\n","        ## Store\n","        predictions.append(yhat)\n","        actuals.append(actual)\n","    predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n","    # Calculate accuracy\n","    acc = accuracy_score(actuals, predictions)\n","    \n","    return acc\n","\n","def predict(row, model):\n","    \"\"\" Predict class for one sample. \"\"\"\n","    # Convert row to data\n","    row = torch.Tensor([row])\n","    # Make prediction\n","    yhat = model(row)\n","    # Retrieve numpy array\n","    yhat = yhat.detach().numpy()\n","    \n","    return yhat"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1685989555942,"user":{"displayName":"Ben Goshi","userId":"05767231154074524663"},"user_tz":-120},"id":"J7WE8iXTtwkw","outputId":"d0125946-2504-4bf2-c176-3ad6876f6d73"},"outputs":[{"name":"stdout","output_type":"stream","text":["235 116\n","<class 'torch.utils.data.dataset.Subset'>\n","\n","[array([[ 1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n","         0.0000e+00,  6.6667e-01,  1.1111e-01,  1.0000e+00, -1.1111e-01,\n","         8.8889e-01, -1.1111e-01,  1.0000e+00, -2.2222e-01,  7.7778e-01,\n","         0.0000e+00,  7.7778e-01,  0.0000e+00,  1.0000e+00, -1.1111e-01,\n","         7.7778e-01, -1.1111e-01,  6.6667e-01, -1.1111e-01,  6.6667e-01,\n","         0.0000e+00,  9.0347e-01, -5.3520e-02,  1.0000e+00,  1.1111e-01,\n","         8.8889e-01, -1.1111e-01,  1.0000e+00,  0.0000e+00],\n","       [ 1.0000e+00,  0.0000e+00,  3.6876e-01, -1.0000e+00, -1.0000e+00,\n","        -1.0000e+00, -7.6610e-02,  1.0000e+00,  1.0000e+00,  9.5041e-01,\n","         7.4597e-01, -3.8710e-01, -1.0000e+00, -7.9313e-01, -9.6770e-02,\n","         1.0000e+00,  4.8684e-01,  4.6502e-01,  3.1755e-01, -2.7461e-01,\n","        -1.4343e-01, -2.0188e-01, -1.1976e-01,  6.8950e-02,  3.0210e-02,\n","         6.6390e-02,  3.4430e-02, -1.1860e-02, -4.0300e-03, -1.6720e-02,\n","        -7.6100e-03,  1.0800e-03,  1.5000e-04,  3.2500e-03],\n","       [ 1.0000e+00,  0.0000e+00,  1.0000e+00,  4.0780e-02,  1.0000e+00,\n","         1.1982e-01,  1.0000e+00,  1.6159e-01,  1.0000e+00,  2.7921e-01,\n","         9.8703e-01,  3.0889e-01,  9.2745e-01,  3.7639e-01,  9.1118e-01,\n","         3.9749e-01,  8.1939e-01,  4.6059e-01,  7.8619e-01,  4.6994e-01,\n","         7.9400e-01,  5.6282e-01,  7.0331e-01,  5.8129e-01,  6.7077e-01,\n","         5.9723e-01,  5.8903e-01,  6.0990e-01,  5.3952e-01,  6.0932e-01,\n","         4.5312e-01,  6.3636e-01,  4.0442e-01,  6.2658e-01]],\n","      dtype=float32), array([[1.],\n","       [0.],\n","       [1.]], dtype=float32)]\n","\n","[array([ 1.     ,  0.     ,  1.     ,  0.     ,  1.     ,  0.     ,\n","        0.66667,  0.11111,  1.     , -0.11111,  0.88889, -0.11111,\n","        1.     , -0.22222,  0.77778,  0.     ,  0.77778,  0.     ,\n","        1.     , -0.11111,  0.77778, -0.11111,  0.66667, -0.11111,\n","        0.66667,  0.     ,  0.90347, -0.05352,  1.     ,  0.11111,\n","        0.88889, -0.11111,  1.     ,  0.     ], dtype=float32), array([1.], dtype=float32)]\n","[array([ 1.0000e+00,  0.0000e+00,  3.6876e-01, -1.0000e+00, -1.0000e+00,\n","       -1.0000e+00, -7.6610e-02,  1.0000e+00,  1.0000e+00,  9.5041e-01,\n","        7.4597e-01, -3.8710e-01, -1.0000e+00, -7.9313e-01, -9.6770e-02,\n","        1.0000e+00,  4.8684e-01,  4.6502e-01,  3.1755e-01, -2.7461e-01,\n","       -1.4343e-01, -2.0188e-01, -1.1976e-01,  6.8950e-02,  3.0210e-02,\n","        6.6390e-02,  3.4430e-02, -1.1860e-02, -4.0300e-03, -1.6720e-02,\n","       -7.6100e-03,  1.0800e-03,  1.5000e-04,  3.2500e-03], dtype=float32), array([0.], dtype=float32)]\n","[array([1.     , 0.     , 1.     , 0.04078, 1.     , 0.11982, 1.     ,\n","       0.16159, 1.     , 0.27921, 0.98703, 0.30889, 0.92745, 0.37639,\n","       0.91118, 0.39749, 0.81939, 0.46059, 0.78619, 0.46994, 0.794  ,\n","       0.56282, 0.70331, 0.58129, 0.67077, 0.59723, 0.58903, 0.6099 ,\n","       0.53952, 0.60932, 0.45312, 0.63636, 0.40442, 0.62658],\n","      dtype=float32), array([1.], dtype=float32)]\n","[array([ 1.   ,  0.   ,  1.   , -1.   ,  1.   ,  1.   ,  1.   ,  1.   ,\n","        1.   , -0.5  ,  1.   ,  1.   ,  1.   ,  1.   ,  1.   ,  1.   ,\n","        0.   ,  0.   ,  1.   ,  1.   ,  1.   ,  1.   ,  1.   , -1.   ,\n","        1.   ,  1.   ,  1.   ,  0.625,  1.   , -0.75 , -0.75 ,  1.   ,\n","        1.   ,  1.   ], dtype=float32), array([0.], dtype=float32)]\n","[array([ 1.     ,  0.     ,  1.     , -0.06604,  1.     ,  0.62937,\n","        1.     ,  0.09557,  1.     ,  0.2028 ,  1.     , -1.     ,\n","        1.     , -0.40559,  1.     , -0.15851,  1.     ,  0.04895,\n","        1.     , -0.61538,  1.     , -0.26573,  1.     , -1.     ,\n","        1.     , -0.58042,  1.     , -0.81372,  1.     , -1.     ,\n","        1.     , -0.78555,  1.     , -0.48252], dtype=float32), array([1.], dtype=float32)]\n","[array([ 1.     ,  0.     ,  0.66161, -1.     ,  1.     ,  1.     ,\n","        1.     , -0.67321,  0.80893, -0.40446,  1.     , -1.     ,\n","        1.     , -0.89375,  1.     ,  0.73393,  0.17589,  0.70982,\n","        1.     ,  0.78036,  1.     ,  0.85268,  1.     , -1.     ,\n","        1.     ,  0.85357,  1.     , -0.08571,  0.95982, -0.3625 ,\n","        1.     ,  0.65268,  1.     ,  0.34732], dtype=float32), array([0.], dtype=float32)]\n","[array([ 1.     ,  0.     ,  0.66667, -0.01366,  0.97404,  0.06831,\n","        0.4959 ,  0.50137,  0.75683, -0.00273,  0.65164, -0.14071,\n","        0.40164, -0.48907,  0.39208,  0.58743,  0.76776,  0.31831,\n","        0.78552,  0.11339,  0.47541, -0.44945,  1.     ,  0.00683,\n","        0.60656,  0.06967,  0.68656,  0.17088,  0.87568,  0.07787,\n","        0.55328,  0.2459 ,  0.13934,  0.48087], dtype=float32), array([1.], dtype=float32)]\n","[array([1.     , 0.     , 0.7479 , 0.0084 , 0.83312, 0.01659, 0.82638,\n","       0.02469, 0.86555, 0.01681, 0.60504, 0.05882, 0.79093, 0.04731,\n","       0.77441, 0.05407, 0.64706, 0.19328, 0.84034, 0.04202, 0.71285,\n","       0.07122, 0.68895, 0.07577, 0.66387, 0.08403, 0.63728, 0.08296,\n","       0.61345, 0.01681, 0.58187, 0.08757, 0.5533 , 0.08891],\n","      dtype=float32), array([1.], dtype=float32)]\n"]}],"source":["\"\"\" Prepare data \"\"\"\n","path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n","train_dl, test_dl = prepare_data(path)\n","example = train_dl.dataset[:3]\n","print(len(train_dl.dataset), len(test_dl.dataset))\n","print(type(train_dl.dataset))\n","example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhxFAralzzE0"},"outputs":[],"source":["for i in range(len(train_dl.dataset)//29):   #integer division (//) operator to ensure that the result is an integer\n","    example = train_dl.dataset[i]\n","    print(example)"]},{"cell_type":"markdown","metadata":{},"source":["<h2 style=\"color:#BF66F2 \">  <u> Example #2 </u> </h2>\n","Based on custom dataset"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["class MyMLP2(nn.Module):\n","    \"\"\"  A Multi-Layer Perceptron (MLP) model with three linear layers (which apply a linear transformation to incoming data) and a binary output. \n","    This class is utilized \n","    for binary classification tasks. It uses ReLU activation functions for hidden layers and a Sigmoid \n","    activation function for the output layer.\n","\n","    Attributes:\n","        - hidden1: First hidden layer, containing 15 neurons [torch.nn.Module]\n","            - Weight initialization: Kaiming (He) initialization for ReLU activation\n","        - act1: ReLU Activation function applied after the first hidden layer [torch.nn.Module]\n","        - hidden2: Second hidden layer, containing 12 neurons [torch.nn.Module]\n","            - Weight initialization: Kaiming (He) initialization for ReLU activation\n","        - act2: ReLU Activation function  [torch.nn.Module]\n","        - hidden3: Third hidden layer, containing 10 neurons[torch.nn.Module]\n","            - Weight initialization: Kaiming (He) initialization for ReLU activation.\n","        - act3: ReLU Activation function [torch.nn.Module]\n","        - output: Output layer, containing 1 neuron [torch.nn.Module]\n","        - act4 : Activation function (Sigmoid) applied after the output layer, squashing outputs to the [0, 1] range [torch.nn.Module]\n","    \"\"\"    \n","\n","    def __init__(self, n_inputs):\n","        super(MyMLP2, self).__init__()\n","        self.hidden1 = nn.Linear(n_inputs, 15)\n","        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n","        self.act1 = nn.ReLU()\n","        \n","        self.hidden2 = nn.Linear(15, 12)\n","        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n","        self.act2 = nn.ReLU()\n","        \n","        self.hidden3 = nn.Linear(12, 10)\n","        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n","        self.act3 = nn.ReLU()\n","        \n","        self.output = nn.Linear(10, 1)\n","        xavier_uniform_(self.output.weight)\n","        self.act4 = nn.Sigmoid()\n","\n","    def forward(self, X):\n","        \"\"\" Forward pass through the network.\n","        \n","        Parameters:\n","            Input tensor containing features [torch.Tensor, shape (batch_size, n_inputs)]\n","            \n","        Returns:\n","            Output tensor after passing through the network [torch.Tensor, shape is (batch_size, 1)]\n","        \"\"\"        \n","        X = self.hidden1(X)  \n","        X = self.act1(X)     \n","        X = self.hidden2(X)  \n","        X = self.act2(X)     \n","        X = self.hidden3(X)  \n","        X = self.act3(X)\n","        X = self.output(X)  \n","        X = self.act4(X)     \n","        return X\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["class CSVDataset(Dataset):\n","    \"\"\"\n","    CSVDataset: A PyTorch Dataset for handling CSV files.\n","    \n","    Args:\n","        - Path to the CSV file [str]\n","        - Column names (or indices) used as input features [list]\n","        - Column name (or index) used as target/label [str or int]\n","        - Optional transform to be applied on a sample\n","\n","    Attributes:\n","        - The entire dataset loaded as a Pandas DataFrame [pd.DataFrame]\n","        - Column names (or indices) used as input features [list]\n","        - Column name (or index) used as target/label [str or int]\n","        - Optional transform to be applied on a sample \n","\n","    \"\"\"    \n","    def __init__(self, path):\n","        \"\"\" Initialize the CSVDataset instance.\n","\n","        Notes:\n","            - '.values' converts the last column selected by iloc (a Pandas Series) to a NumPy array.\n","            - reshape(-1, 1) changes the array to have one column and as many rows as necessary to preserve the number of elements.\n","        \"\"\"\n","        df = pd.read_csv(path)\n","        self.X = df.iloc[:, :-1].values.astype(np.float32)\n","        self.y = df.iloc[:, -1].values.astype(np.float32).reshape(-1, 1)\n","\n","    def __len__(self):\n","        \"\"\" Get the total number of samples in the dataset. \"\"\"\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        \"\"\" Retrieve a sample at the given index. \"\"\"\n","        return [self.X[idx], self.y[idx]]\n","\n","    def get_splits(self, test_size=0.33):\n","        # Split the data\n","        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=test_size, random_state=42, stratify=self.y)\n","        train = [X_train, y_train]\n","        test = [X_test, y_test]\n","        return CSVSplits(train), CSVSplits(test)\n","\n","class CSVSplits(Dataset):\n","    def __init__(self, xy):\n","        self.X, self.y = xy[0], xy[1]\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return [self.X[idx], self.y[idx]]\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["\"\"\" Generate a synthetic dataset, convert it to a DataFrame, and store it as CSV file. \"\"\"\n","X, y = make_classification(\n","    n_samples=1000,\n","    n_features=20,\n","    n_informative=15,\n","    n_classes=2,\n","    random_state=42\n",")\n","\n","data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n","data['label'] = y\n","data.to_csv('./data/synthetic_data_mlp11.csv', index=False)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["(__main__.CSVSplits,\n"," 670,\n"," [array([[-3.3660216e+00,  4.6903701e+00, -2.9365644e-01, -6.2550503e-01,\n","           1.9373842e+00,  5.3089440e-01, -7.8419065e+00, -9.6214857e+00,\n","           1.5070822e+00, -3.5070446e+00,  6.5736854e-01, -1.9820571e+00,\n","           3.5781031e+00, -8.1942484e-02, -2.2932630e+00,  7.3670134e-02,\n","          -5.7234077e+00, -6.2933826e-01,  5.5940595e+00,  7.3103750e-01],\n","         [ 3.9878318e+00, -1.4504327e+00, -7.4221528e-01, -4.4320866e-02,\n","           8.1662476e-01,  2.2038660e+00, -3.2856840e-01, -1.6053659e+00,\n","           6.5204924e-01, -5.5203280e+00,  3.0548160e+00, -3.5256939e+00,\n","           1.1853160e+00, -1.1811910e+00, -1.1965318e+00, -8.5692817e-01,\n","          -1.5044899e+00,  4.9679227e+00,  1.1657474e+01, -6.3480270e-01],\n","         [-9.5817977e-01,  4.1054419e-01,  1.4526561e+00, -3.4971721e+00,\n","           3.0058694e-01,  6.7987144e-01,  2.9035997e-01, -3.8141093e+00,\n","          -1.1357025e+00, -3.0209696e+00,  9.3515891e-01, -2.5258760e+00,\n","          -9.2425704e-01,  1.8914552e+00, -6.8371081e+00, -1.2351856e+00,\n","           6.8190400e-03,  7.8225523e-01,  9.2923880e+00, -5.1606017e-01]],\n","        dtype=float32),\n","  array([[1.],\n","         [1.],\n","         [1.]], dtype=float32)])"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["path_2 = './data/synthetic_data_mlp11.csv'\n","train_dl, test_dl = prepare_data(path_2)\n","example = train_dl.dataset[:3]\n","\n","type(train_dl.dataset), len(train_dl.dataset), train_dl.dataset[:3]"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["def train_model_2(train_dl, model):\n","    \"\"\" Perform training for a given train dataloader on the received model. \"\"\"\n","    ###### Some loss functions\n","    criterion0 = nn.BCELoss()\n","    criterion1 = nn.BCEWithLogitsLoss()\n","    criterion2 = nn.MSELoss()\n","    criterion3 = nn.MarginRankingLoss()\n","    criterion4 = nn.HingeEmbeddingLoss()\n","    criterion5 = nn.SoftMarginLoss()\n","    \n","    ###### Some optimizers\n","    optimizer0 = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","    optimizer1 = optim.LBFGS(model.parameters(), lr=1e-2)\n","    optimizer2 = optim.Rprop(model.parameters(), lr=1e-2)\n","    optimizer3 = optim.ASGD(model.parameters(), lr=1e-2)\n","    optimizer4 = optim.AdamW(model.parameters(), lr=1e-2)\n","    optimizer5 = optim.Adamax(model.parameters(), lr=1e-2)\n","\n","    # For all epochs and all mini-batches...\n","    for epoch in range(100):\n","        for i, (inputs, targets) in enumerate(train_dl):\n","            def closure():\n","                \"\"\" Now i need to define a clousure function that is necessary\n","                for the LBFGS optimizer to RE-evaluate the model and RE-compute the gradients, to minimize the objective function, due to its quasi-Newton nature.\n","                \"\"\"\n","                # Clear the gradients\n","                optimizer1.zero_grad()\n","                # Compute the model output\n","                yhat = model(inputs)\n","                # Calculate loss\n","                loss = criterion1(yhat, targets)\n","                # Assign credit \n","                loss.backward()\n","                return loss\n","\n","            # Update model weights using the closure function\n","            optimizer1.step(closure)\n","\n","def predict_2(test_dl, model):\n","    \"\"\" Calculate metrics.\\\\ \n","    Check evaluate_model method above. \n","    \"\"\"\n","    predictions, actuals = list(), list()\n","    for i, (inputs, targets) in enumerate(test_dl):\n","        yhat = model(inputs)\n","        yhat = yhat.detach().numpy()\n","        actual = targets.numpy()\n","        actual = actual.reshape((len(actual), 1))\n","        yhat = yhat.round()\n","        predictions.append(yhat)\n","        actuals.append(actual)\n","    predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n","    \n","    return predictions, actuals"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["MyMLP2(\n","  (hidden1): Linear(in_features=20, out_features=15, bias=True)\n","  (act1): ReLU()\n","  (hidden2): Linear(in_features=15, out_features=12, bias=True)\n","  (act2): ReLU()\n","  (hidden3): Linear(in_features=12, out_features=10, bias=True)\n","  (act3): ReLU()\n","  (output): Linear(in_features=10, out_features=1, bias=True)\n","  (act4): Sigmoid()\n",")"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["n_inputs = train_dl.dataset[0][0].shape[0]\n","model_2 = MyMLP2(n_inputs)\n","model_2"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# Training\n","with warnings.catch_warnings():\n","    warnings.filterwarnings(\"ignore\", category=UserWarning)\n","    train_model_2(train_dl, model_2)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.9523599]]\n","Accuracy: 0.918\n"]}],"source":["test_arr = [\n","    -8.3762154e-01,  9.4032541e-01, -2.9565234e-01, -5.7823402e-01,\n","    1.2473841e+00,  4.9238140e-01, -7.1245905e-01, -9.3821475e-01,\n","    1.1074532e+00, -3.1046456e-01,  6.2431584e-01, -1.7843570e-01,\n","    3.4785401e-01, -7.2924824e-02, -2.1325610e-01,  7.3725134e-02,\n","    -5.5123057e-01, -6.1438266e-01,  5.7940594e-01,  8.2131500e-01\n","    ]\n","\n","# Make predictions \n","y_pred = predict(test_arr, model_2)\n","# Evaluate\n","acc = evaluate_model(test_dl, model_2)\n","\n","print(y_pred)\n","print('Accuracy: %.3f' % acc)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.89      0.94      0.92       156\n","         1.0       0.95      0.90      0.92       174\n","\n","    accuracy                           0.92       330\n","   macro avg       0.92      0.92      0.92       330\n","weighted avg       0.92      0.92      0.92       330\n","\n"]}],"source":["act, predictions = predict_2(test_dl, model_2)\n","report = classification_report(act, predictions, zero_division=1)\n","print(\"Classification Report:\")\n","print(report)"]},{"cell_type":"markdown","metadata":{},"source":["<h4 style=\"color:#BF66F2 \">  => Change loss and optimizer #1 </h4>"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["def train_model_2_1(train_dl, model):\n","    \"\"\" Perform training for a given train dataloader on the received model.\\\\\n","        Check 'train_model_2()' for further info. \"\"\"\n","    ###### Some loss functions\n","    criterion0 = nn.BCELoss()\n","    criterion1 = nn.BCEWithLogitsLoss()\n","    criterion2 = nn.MSELoss()\n","    criterion3 = nn.MarginRankingLoss()\n","    criterion4 = nn.HingeEmbeddingLoss()\n","    criterion5 = nn.SoftMarginLoss()\n","    \n","    ###### Some optimizers\n","    optimizer0 = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","    optimizer1 = optim.LBFGS(model.parameters(), lr=1e-2)\n","    optimizer2 = optim.Rprop(model.parameters(), lr=1e-2)\n","    optimizer3 = optim.ASGD(model.parameters(), lr=1e-2)\n","    optimizer4 = optim.AdamW(model.parameters(), lr=1e-2)\n","    optimizer5 = optim.Adamax(model.parameters(), lr=1e-2)\n","\n","    for epoch in range(100):\n","        for i, (inputs, targets) in enumerate(train_dl):\n","            def closure():\n","                optimizer1.zero_grad()\n","                yhat = model(inputs)\n","                loss = criterion1(yhat, targets)\n","                loss.backward()\n","                return loss\n","\n","            optimizer1.step(closure)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"text/plain":["MyMLP2(\n","  (hidden1): Linear(in_features=20, out_features=15, bias=True)\n","  (act1): ReLU()\n","  (hidden2): Linear(in_features=15, out_features=12, bias=True)\n","  (act2): ReLU()\n","  (hidden3): Linear(in_features=12, out_features=10, bias=True)\n","  (act3): ReLU()\n","  (output): Linear(in_features=10, out_features=1, bias=True)\n","  (act4): Sigmoid()\n",")"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["n_inputs = train_dl.dataset[0][0].shape[0]\n","model_2 = MyMLP2(n_inputs)\n","model_2"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["# Training\n","with warnings.catch_warnings():\n","    warnings.filterwarnings(\"ignore\", category=UserWarning)\n","    train_model_2_1(train_dl, model_2)"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.]]\n","Accuracy: 0.718\n"]}],"source":["test_arr = [\n","    -8.3762154e-01,  9.4032541e-01, -2.9565234e-01, -5.7823402e-01,\n","    1.2473841e+00,  4.9238140e-01, -7.1245905e-01, -9.3821475e-01,\n","    1.1074532e+00, -3.1046456e-01,  6.2431584e-01, -1.7843570e-01,\n","    3.4785401e-01, -7.2924824e-02, -2.1325610e-01,  7.3725134e-02,\n","    -5.5123057e-01, -6.1438266e-01,  5.7940594e-01,  8.2131500e-01\n","    ]\n","\n","# Make predictions \n","y_pred = predict(test_arr, model_2)\n","# Evaluate\n","acc = evaluate_model(test_dl, model_2)\n","\n","print(y_pred)\n","print('Accuracy: %.3f' % acc)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.51      0.88      0.64        96\n","         1.0       0.93      0.65      0.77       234\n","\n","    accuracy                           0.72       330\n","   macro avg       0.72      0.76      0.71       330\n","weighted avg       0.81      0.72      0.73       330\n","\n"]}],"source":["act, predictions = predict_2(test_dl, model_2)\n","report = classification_report(act, predictions, zero_division=1)\n","print(\"Classification Report:\")\n","print(report)"]},{"cell_type":"markdown","metadata":{},"source":["<h4 style=\"color:#BF66F2 \">  => Change loss and optimizer #2 </h4>"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["def train_model_2_2(train_dl, model):\n","    \"\"\" Perform training for a given train dataloader on the received model.\\\\\n","        Check 'train_model_2()' for further info. \"\"\"\n","    margin = 0.5  \n","    \n","    ###### Some loss functions\n","    criterion0 = nn.BCELoss()\n","    criterion1 = nn.BCEWithLogitsLoss()\n","    criterion2 = nn.MSELoss()\n","    criterion3 = nn.MarginRankingLoss(margin)\n","    criterion4 = nn.HingeEmbeddingLoss()\n","    criterion5 = nn.SoftMarginLoss()\n","    \n","    ###### Some optimizers\n","    optimizer0 = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","    optimizer1 = optim.LBFGS(model.parameters(), lr=1e-2)\n","    optimizer2 = optim.Rprop(model.parameters(), lr=1e-2)\n","    optimizer3 = optim.ASGD(model.parameters(), lr=1e-2)\n","    optimizer4 = optim.AdamW(model.parameters(), lr=1e-2)\n","    optimizer5 = optim.Adamax(model.parameters(), lr=1e-2)\n","\n","    for epoch in range(100):\n","        for i, (inputs, targets) in enumerate(train_dl):\n","            def closure():\n","                optimizer4.zero_grad()\n","                yhat = model(inputs)\n","                # Create x since loss need tensors as input\n","                baseline = torch.zeros_like(yhat) \n","                loss = criterion3(yhat, baseline, targets)\n","                loss.backward()\n","                return loss\n","\n","            optimizer4.step(closure)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"data":{"text/plain":["MyMLP2(\n","  (hidden1): Linear(in_features=20, out_features=15, bias=True)\n","  (act1): ReLU()\n","  (hidden2): Linear(in_features=15, out_features=12, bias=True)\n","  (act2): ReLU()\n","  (hidden3): Linear(in_features=12, out_features=10, bias=True)\n","  (act3): ReLU()\n","  (output): Linear(in_features=10, out_features=1, bias=True)\n","  (act4): Sigmoid()\n",")"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["n_inputs = train_dl.dataset[0][0].shape[0]\n","model_2 = MyMLP2(n_inputs)\n","model_2"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["# Training\n","with warnings.catch_warnings():\n","    warnings.filterwarnings(\"ignore\", category=UserWarning)\n","    train_model_2_2(train_dl, model_2)"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.7745221]]\n","Accuracy: 0.500\n"]}],"source":["test_arr = [\n","    -8.3762154e-01,  9.4032541e-01, -2.9565234e-01, -5.7823402e-01,\n","    1.2473841e+00,  4.9238140e-01, -7.1245905e-01, -9.3821475e-01,\n","    1.1074532e+00, -3.1046456e-01,  6.2431584e-01, -1.7843570e-01,\n","    3.4785401e-01, -7.2924824e-02, -2.1325610e-01,  7.3725134e-02,\n","    -5.5123057e-01, -6.1438266e-01,  5.7940594e-01,  8.2131500e-01\n","    ]\n","\n","# Make predictions \n","y_pred = predict(test_arr, model_2)\n","# Evaluate\n","acc = evaluate_model(test_dl, model_2)\n","\n","print(y_pred)\n","print('Accuracy: %.3f' % acc)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.00      1.00      0.00         0\n","         1.0       1.00      0.50      0.67       330\n","\n","    accuracy                           0.50       330\n","   macro avg       0.50      0.75      0.33       330\n","weighted avg       1.00      0.50      0.67       330\n","\n"]}],"source":["act, predictions = predict_2(test_dl, model_2)\n","report = classification_report(act, predictions, zero_division=1)\n","print(\"Classification Report:\")\n","print(report)"]},{"cell_type":"markdown","metadata":{},"source":["<h4 style=\"color:#BF66F2 \">  => Change loss and optimizer #3 </h4>"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["def train_model_2_2(train_dl, model):\n","    \"\"\" Perform training for a given train dataloader on the received model.\\\\\n","        Check 'train_model_2()' for further info. \"\"\"\n","    criterion4 = nn.HingeEmbeddingLoss()\n","    optimizer2 = optim.Rprop(model.parameters(), lr=1e-2)\n","\n","    for epoch in range(100):\n","        for i, (inputs, targets) in enumerate(train_dl):\n","            optimizer2.zero_grad()  \n","            yhat = model(inputs)    \n","            loss = criterion4(yhat, targets) \n","            loss.backward()  \n","            optimizer2.step() \n"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/plain":["MyMLP2(\n","  (hidden1): Linear(in_features=20, out_features=15, bias=True)\n","  (act1): ReLU()\n","  (hidden2): Linear(in_features=15, out_features=12, bias=True)\n","  (act2): ReLU()\n","  (hidden3): Linear(in_features=12, out_features=10, bias=True)\n","  (act3): ReLU()\n","  (output): Linear(in_features=10, out_features=1, bias=True)\n","  (act4): Sigmoid()\n",")"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["n_inputs = train_dl.dataset[0][0].shape[0]\n","model_2 = MyMLP2(n_inputs)\n","model_2"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["# Training\n","with warnings.catch_warnings():\n","    warnings.filterwarnings(\"ignore\", category=UserWarning)\n","    train_model_2_2(train_dl, model_2)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.]]\n","Accuracy: 0.500\n"]}],"source":["test_arr = [\n","    -8.3762154e-01,  9.4032541e-01, -2.9565234e-01, -5.7823402e-01,\n","    1.2473841e+00,  4.9238140e-01, -7.1245905e-01, -9.3821475e-01,\n","    1.1074532e+00, -3.1046456e-01,  6.2431584e-01, -1.7843570e-01,\n","    3.4785401e-01, -7.2924824e-02, -2.1325610e-01,  7.3725134e-02,\n","    -5.5123057e-01, -6.1438266e-01,  5.7940594e-01,  8.2131500e-01\n","    ]\n","\n","# Make predictions \n","y_pred = predict(test_arr, model_2)\n","# Evaluate\n","acc = evaluate_model(test_dl, model_2)\n","\n","print(y_pred)\n","print('Accuracy: %.3f' % acc)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       1.00      0.50      0.67       330\n","         1.0       0.00      1.00      0.00         0\n","\n","    accuracy                           0.50       330\n","   macro avg       0.50      0.75      0.33       330\n","weighted avg       1.00      0.50      0.67       330\n","\n"]}],"source":["act, predictions = predict_2(test_dl, model_2)\n","report = classification_report(act, predictions, zero_division=1)\n","print(\"Classification Report:\")\n","print(report)"]},{"cell_type":"markdown","metadata":{},"source":["<h2 style=\"color:#BF66F2 \">  <u> Example #3 </u> </h2>\n","Based on the MNIST dataset"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["class MyMLP3(nn.Module):\n","    \"\"\"A PyTorch Multi-Layer Perceptron model for multiclass classification.\n","    \n","    Args:\n","        - Number of input features [int]\n","        - Number of classes (output units) [int]\n","        - Dropout rate for dropout layer [float, optional, default is 0.2]\n","\n","    Attributes:\n","        - First hidden layer with 64 units [nn.Linear]\n","        - Second hidden layer with 32 units [nn.Linear]\n","        - Third hidden layer with 16 units[nn.Linear]\n","        - Dropout layer to reduce overfitting [nn.Dropout]\n","        - Output layer with units equal to the number of classes [nn.Linear]\n","    \"\"\"\n","\n","    def __init__(self, input_size, output_size, dropout_rate=0.2):\n","        super(MyMLP3, self).__init__()\n","        self.layer1 = nn.Linear(input_size, 64)\n","        self.layer2 = nn.Linear(64, 32)\n","        self.layer3 = nn.Linear(32, 16)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.output = nn.Linear(16, output_size)\n","\n","    def forward(self, x):\n","        \"\"\" Forward pass through the network. \"\"\"\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.layer3(x))\n","        x = self.output(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["<h3 style=\"color:#BF66F2 \">  Recap: </h3>\n","<div style=\"margin-top: -25px;\">\n","\n","- nn.ReLU() creates an nn.Module which can be added to an nn.Sequential model\n","- nn.functional.relu is just the functional API call to the relu function that can be added in a forward method"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 11631440.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00<00:00, 31710914.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 11770007.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00<00:00, 17721422.11it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","\n","#### Download and load the training and test data\n","train_data = MNIST(root='./data', train=True, download=True, transform=transform)\n","test_data = MNIST(root='./data', train=False, download=True, transform=transform)\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Loss: 0.4742\n","Epoch [2/10], Loss: 0.3746\n","Epoch [3/10], Loss: 0.5508\n","Epoch [4/10], Loss: 0.4877\n","Epoch [5/10], Loss: 0.0695\n","Epoch [6/10], Loss: 0.1630\n","Epoch [7/10], Loss: 0.1675\n","Epoch [8/10], Loss: 0.0565\n","Epoch [9/10], Loss: 0.1157\n","Epoch [10/10], Loss: 0.1757\n"]}],"source":["### Initialize the model, criterion, and optimizer\n","model = MyMLP3(input_size=784, output_size=10, dropout_rate=0.2)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# Training\n","num_epochs = 10 \n","for epoch in range(num_epochs):\n","    for inputs, labels in train_loader:\n","        # Reshape the input\n","        inputs = inputs.view(-1, 28*28)  \n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","    \n","    if (epoch+1) % 1 == 0:  \n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the model on the test set: 95.95%\n"]}],"source":["# Evaluate the model\n","model.eval()\n","total, correct = 0, 0\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        # Reshape the input\n","        inputs = inputs.view(-1, 28*28)  \n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the model on the test set: {100 * correct / total}%')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNQtE8A9MmT8DOy4gyV11s+","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
